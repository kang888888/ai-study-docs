{
  "title": "è§£ç ç­–ç•¥ï¼ˆDecoding Strategiesï¼‰",
  "subtitle": "å¤§è¯­è¨€æ¨¡å‹ä¸­æ–‡æœ¬ç”Ÿæˆçš„å…³é”®æŠ€æœ¯ï¼Œç›´æ¥å½±å“ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€å¤šæ ·æ€§å’Œå¯æ§æ€§ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "è§£ç ç­–ç•¥å¯¹æ¯”",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "è§£ç ç­–ç•¥å¯¹æ¯”",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "è§£ç ç­–ç•¥æ•ˆæœ",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "è§£ç ç­–ç•¥æ•ˆæœ",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ä¸åŒè§£ç ç­–ç•¥ç”Ÿæˆæ–‡æœ¬",
          "language": "python",
          "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_name = \"microsoft/DialoGPT-medium\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\nprompt = \"Hello, how are you?\"\n\n# è´ªå¿ƒæœç´¢\noutputs_greedy = model.generate(\n    tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"],\n    max_length=100,\n    do_sample=False,\n    num_beams=1\n)\n\n# æŸæœç´¢\noutputs_beam = model.generate(\n    tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"],\n    max_length=100,\n    do_sample=False,\n    num_beams=4\n)\n\n# æ ¸é‡‡æ ·\noutputs_nucleus = model.generate(\n    tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"],\n    max_length=100,\n    do_sample=True,\n    top_p=0.9,\n    temperature=0.7\n)"
        }
      ]
    }
  ]
}