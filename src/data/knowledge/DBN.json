{
  "title": "DBN (Deep Belief Network) æ·±åº¦ä¿¡å¿µç½‘ç»œ",
  "subtitle": "æ·±åº¦å­¦ä¹ æ—©æœŸçš„é‡è¦æ¶æ„",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "ç”±å¤šä¸ªå—é™ç»å°”å…¹æ›¼æœºï¼ˆRBMï¼‰å †å è€Œæˆçš„æ·±åº¦ç”Ÿæˆæ¨¡å‹ã€‚é€šè¿‡é€å±‚é¢„è®­ç»ƒ+å¾®è°ƒçš„æ–¹å¼è®­ç»ƒï¼Œæ˜¯æ·±åº¦å­¦ä¹ æ—©æœŸï¼ˆ2006å¹´ï¼‰çš„é‡è¦æ¶æ„ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "é€å±‚é¢„è®­ç»ƒï¼šå…ˆæ— ç›‘ç£é¢„è®­ç»ƒæ¯å±‚RBMï¼Œå†æœ‰ç›‘ç£å¾®è°ƒ",
            "ç”Ÿæˆæ¨¡å‹ï¼šå¯ä»¥ç”Ÿæˆæ•°æ®ï¼Œä¹Ÿå¯ä»¥ç”¨äºåˆ†ç±»",
            "æ— ç›‘ç£å­¦ä¹ ï¼šä»æ— æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ ç‰¹å¾",
            "å†å²æ„ä¹‰ï¼š2006å¹´æ·±åº¦å­¦ä¹ å¤å…´çš„å…³é”®æŠ€æœ¯",
            "ç°å·²è¾ƒå°‘ä½¿ç”¨ï¼šè¢«Transformerç­‰æ¶æ„å–ä»£"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "å—é™ç»å°”å…¹æ›¼æœºï¼ˆRBMï¼‰ã€å¯¹æ¯”æ•£åº¦ç®—æ³•ï¼ˆContrastive Divergenceï¼‰ã€é€å±‚é¢„è®­ç»ƒ"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "å›¾åƒè¯†åˆ«ã€ç‰¹å¾æå–ã€é™ç»´ã€ååŒè¿‡æ»¤ï¼ˆæ—©æœŸï¼‰"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "DBNDiagram",
              "caption": "DBNæ¶æ„",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "DBNæ¶æ„",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "DBNDiagram",
              "caption": "RBMç»“æ„",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "RBMç»“æ„",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "å—é™ç»å°”å…¹æ›¼æœºï¼ˆRBMï¼‰èƒ½é‡å‡½æ•°",
          "formulas": [
            {
              "text": "RBMçš„èƒ½é‡å‡½æ•°ï¼š"
            },
            {
              "display": "E(v, h) = -\\sum_{i} a_i v_i - \\sum_{j} b_j h_j - \\sum_{i,j} v_i W_{ij} h_j"
            },
            {
              "text": "å…¶ä¸­ï¼š"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "æ¦‚ç‡åˆ†å¸ƒ",
          "formulas": [
            {
              "text": "åŸºäºèƒ½é‡å‡½æ•°çš„æ¦‚ç‡åˆ†å¸ƒï¼š"
            },
            {
              "display": "P(v, h) = \\frac{1}{Z} e^{-E(v, h)}"
            },
            {
              "text": "å…¶ä¸­ $Z = \\sum_{v,h} e^{-E(v, h)}$ æ˜¯é…åˆ†å‡½æ•°",
              "inline": "Z = \\sum_{v,h} e^{-E(v, h)}"
            },
            {
              "text": "æ¡ä»¶æ¦‚ç‡ï¼š"
            },
            {
              "display": "P(h_j=1 | v) = \\sigma(b_j + \\sum_{i} W_{ij} v_i)"
            },
            {
              "display": "P(v_i=1 | h) = \\sigma(a_i + \\sum_{j} W_{ij} h_j)"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "å¯¹æ¯”æ•£åº¦ï¼ˆCDï¼‰ç®—æ³•",
          "formulas": [
            {
              "text": "æƒé‡æ›´æ–°è§„åˆ™ï¼š"
            },
            {
              "display": "\\Delta W_{ij} = \\epsilon (\\langle v_i h_j \\rangle_{data} - \\langle v_i h_j \\rangle_{recon})"
            },
            {
              "text": "å…¶ä¸­ $\\langle \\cdot \\rangle_{data}$ æ˜¯æ•°æ®åˆ†å¸ƒçš„æœŸæœ›ï¼Œ$\\langle \\cdot \\rangle_{recon}$ æ˜¯é‡æ„åˆ†å¸ƒçš„æœŸæœ›",
              "inline": "\\langle \\cdot \\rangle_{data}"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ PyTorch å®ç°ç®€å•çš„ RBM",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass RBM(nn.Module):\n    \"\"\"å—é™ç»å°”å…¹æ›¼æœº\"\"\"\n    def __init__(self, n_visible, n_hidden):\n        super(RBM, self).__init__()\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        \n        # æƒé‡å’Œåç½®\n        self.W = nn.Parameter(torch.randn(n_visible, n_hidden) * 0.1)\n        self.v_bias = nn.Parameter(torch.zeros(n_visible))\n        self.h_bias = nn.Parameter(torch.zeros(n_hidden))\n    \n    def sample_h(self, v):\n        \"\"\"ç»™å®šå¯è§å±‚ï¼Œé‡‡æ ·éšè—å±‚\"\"\"\n        p_h = torch.sigmoid(torch.matmul(v, self.W) + self.h_bias)\n        return p_h, torch.bernoulli(p_h)\n    \n    def sample_v(self, h):\n        \"\"\"ç»™å®šéšè—å±‚ï¼Œé‡‡æ ·å¯è§å±‚\"\"\"\n        p_v = torch.sigmoid(torch.matmul(h, self.W.t()) + self.v_bias)\n        return p_v, torch.bernoulli(p_v)\n    \n    def contrastive_divergence(self, v0, k=1):\n        \"\"\"å¯¹æ¯”æ•£åº¦ç®—æ³•\"\"\"\n        # æ­£ç›¸\n        p_h0, h0 = self.sample_h(v0)\n        \n        # è´Ÿç›¸ï¼ˆGibbsé‡‡æ ·ï¼‰\n        v_k = v0\n        for _ in range(k):\n            p_h_k, h_k = self.sample_h(v_k)\n            p_v_k, v_k = self.sample_v(h_k)\n        \n        # è®¡ç®—æ¢¯åº¦\n        positive_grad = torch.matmul(v0.t(), p_h0)\n        negative_grad = torch.matmul(v_k.t(), p_h_k)\n        \n        return positive_grad - negative_grad\n    \n    def forward(self, v):\n        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n        p_h, h = self.sample_h(v)\n        return p_h\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    # åˆ›å»ºRBM\n    rbm = RBM(n_visible=784, n_hidden=500)\n    \n    # æ¨¡æ‹Ÿè¾“å…¥ï¼ˆäºŒå€¼åŒ–å›¾åƒï¼‰\n    v0 = torch.rand(32, 784)\n    v0 = (v0 > 0.5).float()\n    \n    # å‰å‘ä¼ æ’­\n    h = rbm(v0)\n    print(f\"éšè—å±‚å½¢çŠ¶: {h.shape}\")  # [32, 500]\n    \n    # å¯¹æ¯”æ•£åº¦ï¼ˆç”¨äºè®­ç»ƒï¼‰\n    grad = rbm.contrastive_divergence(v0, k=1)\n    print(f\"æ¢¯åº¦å½¢çŠ¶: {grad.shape}\")  # [784, 500]"
        },
        {
          "type": "code-box",
          "title": "DBN é€å±‚é¢„è®­ç»ƒ",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nfrom torch.optim import Adam\n\nclass DBN(nn.Module):\n    \"\"\"æ·±åº¦ä¿¡å¿µç½‘ç»œ\"\"\"\n    def __init__(self, layers):\n        super(DBN, self).__init__()\n        self.layers = nn.ModuleList([RBM(layers[i], layers[i+1]) \n                                     for i in range(len(layers)-1)])\n    \n    def pretrain_layer(self, layer_idx, data, epochs=10):\n        \"\"\"é¢„è®­ç»ƒå•å±‚RBM\"\"\"\n        rbm = self.layers[layer_idx]\n        optimizer = Adam(rbm.parameters(), lr=0.01)\n        \n        for epoch in range(epochs):\n            # è·å–å½“å‰å±‚çš„è¾“å…¥\n            if layer_idx == 0:\n                input_data = data\n            else:\n                with torch.no_grad():\n                    input_data = self.layers[layer_idx-1](data)\n            \n            # å¯¹æ¯”æ•£åº¦\n            grad = rbm.contrastive_divergence(input_data)\n            \n            # æ›´æ–°æƒé‡ï¼ˆç®€åŒ–ç‰ˆï¼‰\n            optimizer.zero_grad()\n            loss = -torch.sum(grad * rbm.W)\n            loss.backward()\n            optimizer.step()\n            \n            if (epoch + 1) % 5 == 0:\n                print(f\"Layer {layer_idx}, Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    # åˆ›å»ºDBN: 784 -> 500 -> 250 -> 100\n    dbn = DBN([784, 500, 250, 100])\n    \n    # æ¨¡æ‹Ÿæ•°æ®\n    data = torch.rand(100, 784)\n    data = (data > 0.5).float()\n    \n    # é€å±‚é¢„è®­ç»ƒ\n    for i in range(len(dbn.layers)):\n        print(f\"é¢„è®­ç»ƒç¬¬ {i+1} å±‚...\")\n        dbn.pretrain_layer(i, data, epochs=10)"
        }
      ]
    }
  ]
}