{
  "title": "SAM (Sharpness-Aware Minimization) é”åº¦æ„ŸçŸ¥æœ€å°åŒ–",
  "subtitle": "é€šè¿‡åŒæ—¶ä¼˜åŒ–æŸå¤±å€¼ä¸æŸå¤±æ™¯è§‚çš„å¹³å¦åº¦æå‡æ³›åŒ–èƒ½åŠ›",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "SAMæ˜¯ä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼Œä¸ä»…æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼Œè¿˜åŒæ—¶æœ€å°åŒ–æŸå¤±æ™¯è§‚çš„é”åº¦ï¼ˆsharpnessï¼‰ã€‚é€šè¿‡åœ¨æŸå¤±æ™¯è§‚çš„å¹³å¦åŒºåŸŸå¯»æ‰¾æœ€ä¼˜è§£ï¼ŒSAMèƒ½å¤Ÿæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡å°‘è¿‡æ‹Ÿåˆã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "æ³›åŒ–æå‡ï¼šé€šè¿‡ä¼˜åŒ–æŸå¤±æ™¯è§‚å¹³å¦åº¦ï¼Œæ˜¾è‘—æå‡æ³›åŒ–èƒ½åŠ›",
            "åŒé‡ä¼˜åŒ–ï¼šåŒæ—¶ä¼˜åŒ–æŸå¤±å€¼å’ŒæŸå¤±æ™¯è§‚",
            "å¹³å¦åŒºåŸŸï¼šåœ¨æŸå¤±æ™¯è§‚çš„å¹³å¦åŒºåŸŸå¯»æ‰¾æœ€ä¼˜è§£",
            "è®¡ç®—å¼€é”€ï¼šç›¸æ¯”æ ‡å‡†SGDï¼Œè®¡ç®—é‡å¢åŠ çº¦2å€",
            "æ•ˆæœæ˜¾è‘—ï¼šåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "é”åº¦åº¦é‡ã€åŒé‡ä¼˜åŒ–ã€æŸå¤±æ™¯è§‚åˆ†æã€æ³›åŒ–ç†è®º"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "å›¾åƒåˆ†ç±»ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€å°æ•°æ®é›†è®­ç»ƒã€éœ€è¦å¼ºæ³›åŒ–èƒ½åŠ›çš„åœºæ™¯"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "SAMä¼˜åŒ–åŸç†",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "SAMä¼˜åŒ–åŸç†"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "SAMä¼˜åŒ–ç›®æ ‡",
          "formulas": [
            {
              "text": "SAMä¼˜åŒ–ç›®æ ‡ï¼š"
            },
            {
              "display": "\\min_w \\max_{\\|\\epsilon\\| \\leq \\rho} L(w + \\epsilon)"
            },
            {
              "text": "å…¶ä¸­ $L$ æ˜¯æŸå¤±å‡½æ•°ï¼Œ$\\rho$ æ˜¯æ‰°åŠ¨åŠå¾„ï¼Œ$\\epsilon$ æ˜¯å‚æ•°æ‰°åŠ¨"
            },
            {
              "text": "é€šè¿‡æœ€å¤§åŒ–æ‰°åŠ¨åçš„æŸå¤±ï¼Œæ‰¾åˆ°å¹³å¦çš„æœ€ä¼˜è§£"
            },
            {
              "text": "æ‰°åŠ¨æ–¹å‘è¿‘ä¼¼ä¸ºï¼š"
            },
            {
              "display": "\\epsilon^*(w) \\approx \\rho \\frac{\\nabla_w L(w)}{\\|\\nabla_w L(w)\\|}"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "SAMä¼˜åŒ–å™¨å®ç°",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# 1. ä½¿ç”¨SAMåº“ï¼ˆéœ€è¦å®‰è£…ï¼špip install sam-pytorchï¼‰\n# from sam import SAM\n\n# 2. æ‰‹åŠ¨å®ç°SAMä¼˜åŒ–å™¨\nclass SAM(torch.optim.Optimizer):\n    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n        \n        defaults = dict(rho=rho, **kwargs)\n        super(SAM, self).__init__(params, defaults)\n        \n        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n        self.param_groups = self.base_optimizer.param_groups\n    \n    @torch.no_grad()\n    def first_step(self, zero_grad=False):\n        \"\"\"ç¬¬ä¸€æ­¥ï¼šè®¡ç®—æ‰°åŠ¨å¹¶æ›´æ–°å‚æ•°\"\"\"\n        grad_norm = self._grad_norm()\n        for group in self.param_groups:\n            scale = group[\"rho\"] / (grad_norm + 1e-12)\n            \n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                e_w = p.grad * scale\n                p.add_(e_w)  # æ‰°åŠ¨å‚æ•°\n                self.state[p][\"e_w\"] = e_w\n        \n        if zero_grad:\n            self.zero_grad()\n    \n    @torch.no_grad()\n    def second_step(self, zero_grad=False):\n        \"\"\"ç¬¬äºŒæ­¥ï¼šæ¢å¤å‚æ•°å¹¶æ›´æ–°\"\"\"\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                p.sub_(self.state[p][\"e_w\"])  # æ¢å¤å‚æ•°\n        \n        self.base_optimizer.step()  # ä½¿ç”¨åŸºç¡€ä¼˜åŒ–å™¨æ›´æ–°\n        \n        if zero_grad:\n            self.zero_grad()\n    \n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"SAMä¸éœ€è¦æ ‡å‡†çš„stepæ–¹æ³•\"\"\"\n        raise NotImplementedError(\"SAM doesn't support step(), use first_step() and second_step()\")\n    \n    def _grad_norm(self):\n        \"\"\"è®¡ç®—æ¢¯åº¦èŒƒæ•°\"\"\"\n        shared_device = self.param_groups[0][\"params\"][0].device\n        norm = torch.norm(\n            torch.stack([\n                p.grad.norm(p=2).to(shared_device)\n                for group in self.param_groups for p in group[\"params\"]\n                if p.grad is not None\n            ]),\n            p=2\n        )\n        return norm\n    \n    def load_state_dict(self, state_dict):\n        super().load_state_dict(state_dict)\n        self.base_optimizer.param_groups = self.param_groups\n\n# 3. ä½¿ç”¨SAMè®­ç»ƒæ¨¡å‹\nmodel = nn.Sequential(\n    nn.Linear(784, 128),\n    nn.ReLU(),\n    nn.Linear(128, 10)\n)\n\n# åˆ›å»ºSAMä¼˜åŒ–å™¨ï¼ˆåŸºäºSGDï¼‰\nbase_optimizer = optim.SGD\noptimizer = SAM(model.parameters(), base_optimizer, lr=0.01, momentum=0.9, rho=0.05)\ncriterion = nn.CrossEntropyLoss()\n\n# è®­ç»ƒå¾ªç¯\nfor epoch in range(10):\n    # æ¨¡æ‹Ÿæ•°æ®\n    x = torch.randn(32, 784)\n    y = torch.randint(0, 10, (32,))\n    \n    # ç¬¬ä¸€æ­¥ï¼šå‰å‘ä¼ æ’­å¹¶è®¡ç®—æ‰°åŠ¨åçš„æŸå¤±\n    def closure():\n        optimizer.zero_grad()\n        output = model(x)\n        loss = criterion(output, y)\n        loss.backward()\n        return loss\n    \n    # è®¡ç®—æ¢¯åº¦\n    loss = closure()\n    \n    # SAMç¬¬ä¸€æ­¥ï¼šæ‰°åŠ¨å‚æ•°\n    optimizer.first_step(zero_grad=True)\n    \n    # åœ¨æ‰°åŠ¨åçš„å‚æ•°ä¸Šè®¡ç®—æŸå¤±\n    output = model(x)\n    loss = criterion(output, y)\n    loss.backward()\n    \n    # SAMç¬¬äºŒæ­¥ï¼šæ¢å¤å‚æ•°å¹¶æ›´æ–°\n    optimizer.second_step(zero_grad=True)\n    \n    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n\n# 4. SAM vs æ ‡å‡†SGDå¯¹æ¯”\nprint(\"\\n=== SAM vs SGD å¯¹æ¯” ===\")\nprint(\"SAMé€šè¿‡ä¼˜åŒ–æŸå¤±æ™¯è§‚çš„å¹³å¦åº¦ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›\")\nprint(\"è®¡ç®—å¼€é”€çº¦ä¸ºæ ‡å‡†SGDçš„2å€ï¼ˆéœ€è¦ä¸¤æ¬¡å‰å‘å’Œåå‘ä¼ æ’­ï¼‰\")\nprint(\"é€‚åˆéœ€è¦å¼ºæ³›åŒ–èƒ½åŠ›çš„åœºæ™¯ï¼Œå¦‚å°æ•°æ®é›†è®­ç»ƒ\")"
        }
      ]
    }
  ]
}
