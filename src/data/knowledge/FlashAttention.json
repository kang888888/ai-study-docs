{
  "title": "FlashAttentionï¼šIO æ„ŸçŸ¥çš„æ³¨æ„åŠ›è®¡ç®—",
  "subtitle": "é€šè¿‡å—çŠ¶ tilingã€å¯„å­˜å™¨å¤ç”¨å’Œèåˆ softmaxï¼Œå°†æ³¨æ„åŠ›å¤æ‚åº¦é™ä½ä¸º IO æœ€ä¼˜ï¼Œå®ç°æ›´å¿«çš„é•¿åºåˆ—æ¨ç†ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“Š å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æµç¨‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "flow",
                "title": "æµç¨‹"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "Tiling",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Tiling"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "Flash Decoding",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Flash Decoding"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "åœ¨çº¿ Softmax",
          "formulas": [
            {
              "display": "m_i = \\max(m_{i-1}, x_i), \\quad l_i = l_{i-1}\\, e^{m_{i-1}-m_i} + e^{x_i - m_i}"
            },
            {
              "display": "\\text{softmax}(x)_i = \\frac{e^{x_i - m_n}}{l_n}"
            },
            {
              "text": "æ— éœ€å­˜å‚¨å…¨éƒ¨ logitsã€‚"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "IO æœ€ä¼˜",
          "formulas": [
            {
              "text": "FlashAttention å°† IO å¤æ‚åº¦é™è‡³ï¼š"
            },
            {
              "display": "O\\Big(\\frac{n^2}{B} + n d\\Big)"
            },
            {
              "text": "$B$ ä¸ºå—å¤§å°ï¼Œç†è®ºä¸Šå·²è¾¾ IO ä¸‹ç•Œã€‚",
              "inline": "B"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "PyTorch 2.x å¯ç”¨ FlashAttention",
          "language": "python",
          "code": "import torch\nfrom torch.nn.functional import scaled_dot_product_attention\n\ndef flash_attention(q, k, v, is_causal=True):\n    return scaled_dot_product_attention(\n        q, k, v,\n        attn_mask=None,\n        dropout_p=0.0,\n        is_causal=is_causal\n    )\n\n# åœ¨æ¨ç†æ¨¡å‹ä¸­æ›¿æ¢åŸå§‹ Attention\nwith torch.backends.cuda.sdp_kernel(enable_flash=True, enable_mem_efficient=True, enable_math=True):\n    y = flash_attention(q, k, v)"
        }
      ]
    }
  ]
}