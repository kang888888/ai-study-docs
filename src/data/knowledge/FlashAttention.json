{
  "title": "FlashAttentionï¼šIO æ„ŸçŸ¥çš„æ³¨æ„åŠ›è®¡ç®—",
  "subtitle": "é€šè¿‡å—çŠ¶ tilingã€å¯„å­˜å™¨å¤ç”¨å’Œèåˆ softmaxï¼Œå°†æ³¨æ„åŠ›å¤æ‚åº¦é™ä½ä¸º IO æœ€ä¼˜ï¼Œå®ç°æ›´å¿«çš„é•¿åºåˆ—æ¨ç†ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "FlashAttentionæ˜¯ä¼˜åŒ–çš„æ³¨æ„åŠ›è®¡ç®—ç®—æ³•ï¼Œé€šè¿‡åˆ†å—è®¡ç®—å’Œåœ¨çº¿softmaxï¼Œåœ¨ä¿æŒæ•°å€¼ç²¾åº¦çš„åŒæ—¶å¤§å¹…å‡å°‘å†…å­˜å ç”¨å’Œè®¡ç®—æ—¶é—´ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "å†…å­˜ä¼˜åŒ–ï¼šåˆ†å—è®¡ç®—å‡å°‘å†…å­˜å ç”¨",
            "æ•°å€¼ç¨³å®šï¼šåœ¨çº¿softmaxä¿è¯ç²¾åº¦",
            "è®¡ç®—é«˜æ•ˆï¼šå‡å°‘å†…å­˜è®¿é—®æ¬¡æ•°",
            "é•¿åºåˆ—æ”¯æŒï¼šå¯å¤„ç†æ›´é•¿åºåˆ—",
            "æ€§èƒ½æå‡ï¼šè®­ç»ƒå’Œæ¨ç†é€Ÿåº¦æå‡2-4å€"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "åˆ†å—è®¡ç®—ã€åœ¨çº¿softmaxã€å†…å­˜ä¼˜åŒ–ã€è®¡ç®—ä¼˜åŒ–ã€é•¿åºåˆ—æ”¯æŒ"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "é•¿åºåˆ—è®­ç»ƒã€å¤§æ¨¡å‹è®­ç»ƒã€æ³¨æ„åŠ›ä¼˜åŒ–ã€å†…å­˜å—é™è®­ç»ƒã€é«˜æ€§èƒ½æ¨ç†"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æµç¨‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "flow",
                "title": "æµç¨‹"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "Tiling",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Tiling"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "Flash Decoding",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Flash Decoding"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "åœ¨çº¿ Softmax",
          "formulas": [
            {
              "display": "m_i = \\max(m_{i-1}, x_i), \\quad l_i = l_{i-1}\\, e^{m_{i-1}-m_i} + e^{x_i - m_i}"
            },
            {
              "display": "\\text{softmax}(x)_i = \\frac{e^{x_i - m_n}}{l_n}"
            },
            {
              "text": "æ— éœ€å­˜å‚¨å…¨éƒ¨ logitsã€‚"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "IO æœ€ä¼˜",
          "formulas": [
            {
              "text": "FlashAttention å°† IO å¤æ‚åº¦é™è‡³ï¼š"
            },
            {
              "display": "O\\Big(\\frac{n^2}{B} + n d\\Big)"
            },
            {
              "text": "$B$ ä¸ºå—å¤§å°ï¼Œç†è®ºä¸Šå·²è¾¾ IO ä¸‹ç•Œã€‚",
              "inline": "B"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "PyTorch 2.x å¯ç”¨ FlashAttention",
          "language": "python",
          "code": "import torch\nfrom torch.nn.functional import scaled_dot_product_attention\n\ndef flash_attention(q, k, v, is_causal=True):\n    return scaled_dot_product_attention(\n        q, k, v,\n        attn_mask=None,\n        dropout_p=0.0,\n        is_causal=is_causal\n    )\n\n# åœ¨æ¨ç†æ¨¡å‹ä¸­æ›¿æ¢åŸå§‹ Attention\nwith torch.backends.cuda.sdp_kernel(enable_flash=True, enable_mem_efficient=True, enable_math=True):\n    y = flash_attention(q, k, v)"
        }
      ]
    }
  ]
}