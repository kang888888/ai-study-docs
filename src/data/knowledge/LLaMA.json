{
  "title": "LLaMA (Large Language Model Meta AI)",
  "subtitle": "Metaå¼€æºçš„å¤§è¯­è¨€æ¨¡å‹ç³»åˆ—",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "Metaå¼€æºçš„å¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼ŒåŒ…å«7Båˆ°70Bå¤šä¸ªè§„æ¨¡ã€‚é‡‡ç”¨RMSNormã€SwiGLUã€RoPEç­‰ç°ä»£ä¼˜åŒ–æŠ€æœ¯ï¼Œæ€§èƒ½ä¼˜å¼‚ä¸”å®Œå…¨å¼€æºå¯å•†ç”¨ï¼Œæˆä¸ºå¼€æºç¤¾åŒºçš„åŸºåº§æ¨¡å‹ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "å®Œå…¨å¼€æºï¼šå¯å•†ç”¨ï¼Œè¡ç”Ÿå‡ºå¤§é‡å¾®è°ƒç‰ˆæœ¬ï¼ˆAlpacaã€Vicunaç­‰ï¼‰",
            "RMSNormï¼šæ›¿ä»£LayerNormï¼Œè®¡ç®—æ›´é«˜æ•ˆ",
            "SwiGLUæ¿€æ´»ï¼šæ›¿ä»£ReLUï¼Œæ€§èƒ½æ›´å¥½",
            "RoPEä½ç½®ç¼–ç ï¼šæ—‹è½¬ä½ç½®ç¼–ç ï¼Œæ”¯æŒé•¿ä¸Šä¸‹æ–‡æ‰©å±•",
            "GQAä¼˜åŒ–ï¼šLLaMA-2/3ä½¿ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼Œé™ä½KV Cache"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "RMSNormã€SwiGLUã€RoPEã€Grouped-Query Attentionï¼ˆGQAï¼‰"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "é€šç”¨å¯¹è¯ã€ä»£ç ç”Ÿæˆã€æŒ‡ä»¤éµå¾ªã€ä½œä¸ºåŸºåº§æ¨¡å‹å¾®è°ƒ"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "LLaMAæ•´ä½“æ¶æ„å›¾",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "LLaMAæ•´ä½“æ¶æ„å›¾"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "RMSNormã€SwiGLUã€RoPEç­‰å…³é”®ç»„ä»¶",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "RMSNormã€SwiGLUã€RoPEç­‰å…³é”®ç»„ä»¶"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "LLaMAä¸å…¶ä»–å¼€æºæ¨¡å‹å¯¹æ¯”",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "LLaMAä¸å…¶ä»–å¼€æºæ¨¡å‹å¯¹æ¯”"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "RMSNormï¼ˆRoot Mean Square Layer Normalizationï¼‰",
          "formulas": [
            {
              "text": "RMSNorm å…¬å¼ï¼š"
            },
            {
              "display": "\\text{RMSNorm}(x) = \\frac{x}{\\text{RMS}(x)} \\odot g"
            },
            {
              "display": "\\text{RMS}(x) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} x_i^2}"
            },
            {
              "text": "ç›¸æ¯” LayerNormï¼ŒRMSNorm ä¸éœ€è¦è®¡ç®—å‡å€¼ï¼Œè®¡ç®—æ›´é«˜æ•ˆ"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "SwiGLU æ¿€æ´»å‡½æ•°",
          "formulas": [
            {
              "text": "SwiGLU å…¬å¼ï¼š"
            },
            {
              "display": "\\text{SwiGLU}(x) = \\text{Swish}(xW + b) \\odot (xV + c)"
            },
            {
              "display": "\\text{Swish}(x) = x \\cdot \\sigma(x)"
            },
            {
              "text": "å…¶ä¸­ $\\sigma$ æ˜¯ sigmoid å‡½æ•°ï¼Œ$\\odot$ æ˜¯é€å…ƒç´ ç›¸ä¹˜",
              "inline": "\\sigma"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰",
          "formulas": [
            {
              "text": "æ—‹è½¬ä½ç½®ç¼–ç ï¼š"
            },
            {
              "display": "R_{\\Theta, m}^d = \\begin{pmatrix}\n                        \\cos m\\theta_1 &amp; -\\sin m\\theta_1 &amp; 0 &amp; 0 &amp; \\cdots \\\\\n                        \\sin m\\theta_1 &amp; \\cos m\\theta_1 &amp; 0 &amp; 0 &amp; \\cdots \\\\\n                        0 &amp; 0 &amp; \\cos m\\theta_2 &amp; -\\sin m\\theta_2 &amp; \\cdots \\\\\n                        0 &amp; 0 &amp; \\sin m\\theta_2 &amp; \\cos m\\theta_2 &amp; \\cdots \\\\\n                        \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots\n                        \\end{pmatrix}"
            },
            {
              "text": "å…¶ä¸­ $\\theta_i = 10000^{-2(i-1)/d}$ï¼Œ$m$ æ˜¯ä½ç½®ç´¢å¼•",
              "inline": "\\theta_i = 10000^{-2(i-1)/d}"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ Transformers åº“åŠ è½½ LLaMA",
          "language": "python",
          "code": "from transformers import LlamaForCausalLM, LlamaTokenizer\nimport torch\n\n# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\nmodel_name = \"meta-llama/Llama-2-7b-hf\"  # éœ€è¦HuggingFaceè®¿é—®æƒé™\ntokenizer = LlamaTokenizer.from_pretrained(model_name)\nmodel = LlamaForCausalLM.from_pretrained(model_name)\n\n# è¾“å…¥æ–‡æœ¬\ntext = \"The future of AI is\"\n\n# åˆ†è¯\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# ç”Ÿæˆ\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_length=100,\n        temperature=0.7,\n        do_sample=True\n    )\n\n# è§£ç \ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)"
        },
        {
          "type": "code-box",
          "title": "æ‰‹åŠ¨å®ç° RMSNorm å’Œ SwiGLU",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass RMSNorm(nn.Module):\n    \"\"\"RMSNorm å®ç°\"\"\"\n    def __init__(self, dim, eps=1e-8):\n        super(RMSNorm, self).__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n    \n    def forward(self, x):\n        # è®¡ç®— RMS\n        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n        # å½’ä¸€åŒ–å¹¶ç¼©æ”¾\n        return x / rms * self.weight\n\nclass SwiGLU(nn.Module):\n    \"\"\"SwiGLU æ¿€æ´»å‡½æ•°\"\"\"\n    def __init__(self, dim):\n        super(SwiGLU, self).__init__()\n        self.gate_proj = nn.Linear(dim, dim)\n        self.up_proj = nn.Linear(dim, dim)\n    \n    def forward(self, x):\n        gate = F.silu(self.gate_proj(x))  # Swish = SiLU\n        up = self.up_proj(x)\n        return gate * up\n\nclass RoPE(nn.Module):\n    \"\"\"æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆç®€åŒ–ç‰ˆï¼‰\"\"\"\n    def __init__(self, dim, max_seq_len=2048, base=10000):\n        super(RoPE, self).__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n        self.max_seq_len = max_seq_len\n    \n    def forward(self, x, seq_len=None):\n        if seq_len is None:\n            seq_len = x.shape[-2]\n        \n        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n        emb = torch.cat((freqs, freqs), dim=-1)\n        \n        return emb\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    # RMSNorm\n    rms_norm = RMSNorm(dim=768)\n    x = torch.randn(2, 10, 768)\n    out = rms_norm(x)\n    print(f\"RMSNorm è¾“å‡ºå½¢çŠ¶: {out.shape}\")\n    \n    # SwiGLU\n    swiglu = SwiGLU(dim=768)\n    x = torch.randn(2, 10, 768)\n    out = swiglu(x)\n    print(f\"SwiGLU è¾“å‡ºå½¢çŠ¶: {out.shape}\")\n    \n    # RoPE\n    rope = RoPE(dim=768)\n    pos_emb = rope(x)\n    print(f\"RoPE ä½ç½®ç¼–ç å½¢çŠ¶: {pos_emb.shape}\")"
        }
      ]
    }
  ]
}