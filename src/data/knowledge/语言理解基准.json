{
  "title": "è¯­è¨€ç†è§£åŸºå‡†",
  "subtitle": "GLUEã€SuperGLUEç­‰è‡ªç„¶è¯­è¨€ç†è§£è¯„ä¼°åŸºå‡†ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "è¯­è¨€ç†è§£åŸºå‡†åˆ†ç±»",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "comparison",
                "title": "è¯­è¨€ç†è§£åŸºå‡†åˆ†ç±»",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "GLUEåŸºå‡†ç»“æ„",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "GLUEåŸºå‡†ç»“æ„",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨GLUEåŸºå‡†è¯„ä¼°",
          "language": "python",
          "code": "from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\n\n# åŠ è½½GLUEæ•°æ®é›†\nmnli_dataset = load_dataset(\"glue\", \"mnli\")\n\n# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, \n    num_labels=3  # MNLIæœ‰3ä¸ªæ ‡ç­¾\n)\n\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"premise\"],\n        examples[\"hypothesis\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128\n    )\n\ntokenized_dataset = mnli_dataset.map(tokenize_function, batched=True)\n\n# è®­ç»ƒå’Œè¯„ä¼°\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    num_train_epochs=3\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation_matched\"]\n)\n\ntrainer.train()\nresults = trainer.evaluate()\nprint(f\"GLUE MNLI Accuracy: {results['eval_accuracy']:.4f}\")"
        },
        {
          "type": "code-box",
          "title": "ä½¿ç”¨SuperGLUEåŸºå‡†",
          "language": "python",
          "code": "from datasets import load_dataset\n\n# SuperGLUEåŒ…å«æ›´éš¾çš„NLUä»»åŠ¡\nsuperglue_tasks = [\n    'boolq',      # å¸ƒå°”é—®ç­”\n    'cb',         # CommitmentBank\n    'copa',       # Choice of Plausible Alternatives\n    'multirc',    # Multi-Sentence Reading Comprehension\n    'record',     # Reading Comprehension with Commonsense Reasoning\n    'rte',        # Recognizing Textual Entailment\n    'wic',        # Word-in-Context\n    'wsc'         # Winograd Schema Challenge\n]\n\n# åŠ è½½SuperGLUEä»»åŠ¡\ndef load_superglue_task(task_name):\n    dataset = load_dataset(\"super_glue\", task_name)\n    return dataset\n\n# è¯„ä¼°æ¨¡å‹åœ¨SuperGLUEä¸Šçš„è¡¨ç°\nfor task in superglue_tasks:\n    dataset = load_superglue_task(task)\n    print(f\"{task}: {len(dataset['train'])} training examples\")"
        }
      ]
    }
  ]
}
