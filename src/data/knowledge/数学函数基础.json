{
  "title": "æ•°å­¦å‡½æ•°åŸºç¡€",
  "subtitle": "æ·±åº¦å­¦ä¹ ä¸­å¸¸ç”¨çš„æ•°å­¦å‡½æ•°åŠå…¶æ•°å­¦è¡¨è¾¾",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "æ•°å­¦å‡½æ•°æ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ã€‚ç†è§£å„ç§å‡½æ•°çš„æ•°å­¦è¡¨è¾¾å¼ã€æ€§è´¨å’Œå›¾åƒï¼Œæœ‰åŠ©äºæ·±å…¥ç†è§£ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†ã€‚æœ¬éƒ¨åˆ†è¯¦ç»†ä»‹ç»æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å‡½æ•°ç­‰å¸¸ç”¨æ•°å­¦å‡½æ•°ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ å‡½æ•°åˆ†ç±»",
      "content": [
        {
          "type": "features",
          "items": [
            "æ¿€æ´»å‡½æ•°ï¼šReLUã€Sigmoidã€Tanhã€GELUã€Swishç­‰",
            "æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µã€MSEã€MAEã€Huber Lossç­‰",
            "ä¼˜åŒ–å‡½æ•°ï¼šSGDã€Adamã€AdamWç­‰ä¼˜åŒ–å™¨çš„æ›´æ–°è§„åˆ™",
            "å½’ä¸€åŒ–å‡½æ•°ï¼šSoftmaxã€LayerNormã€BatchNormç­‰",
            "è·ç¦»å‡½æ•°ï¼šæ¬§æ°è·ç¦»ã€ä½™å¼¦ç›¸ä¼¼åº¦ã€æ›¼å“ˆé¡¿è·ç¦»ç­‰"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š å‡½æ•°å¯è§†åŒ–",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "MathFunctionDiagram",
              "caption": "æ¿€æ´»å‡½æ•°å›¾åƒå¯¹æ¯”",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "activation",
                "title": "æ¿€æ´»å‡½æ•°å›¾åƒå¯¹æ¯”"
              }
            },
            {
              "type": "svg-d3",
              "component": "MathFunctionDiagram",
              "caption": "æŸå¤±å‡½æ•°å›¾åƒå¯¹æ¯”",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "loss",
                "title": "æŸå¤±å‡½æ•°å›¾åƒå¯¹æ¯”"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ¿€æ´»å‡½æ•°æ•°å­¦è¡¨è¾¾",
      "content": [
        {
          "type": "math-box",
          "title": "ReLU (Rectified Linear Unit)",
          "formulas": [
            {
              "text": "ReLUæ˜¯æœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼Œå®šä¹‰ä¸ºï¼š"
            },
            {
              "display": "\\text{ReLU}(x) = \\max(0, x) = \\begin{cases} x & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}"
            },
            {
              "text": "å¯¼æ•°ï¼š"
            },
            {
              "display": "\\frac{d}{dx}\\text{ReLU}(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}"
            },
            {
              "text": "ç‰¹ç‚¹ï¼šè®¡ç®—ç®€å•ã€éé¥±å’Œã€ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼Œä½†å¯èƒ½å¯¼è‡´æ­»ç¥ç»å…ƒ"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Leaky ReLU",
          "formulas": [
            {
              "text": "Leaky ReLUè§£å†³ReLUçš„æ­»ç¥ç»å…ƒé—®é¢˜ï¼š"
            },
            {
              "display": "\\text{LeakyReLU}(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases}"
            },
            {
              "text": "å…¶ä¸­ $\\alpha$ æ˜¯å°çš„æ­£æ•°ï¼ˆé€šå¸¸ä¸º0.01ï¼‰ï¼Œå¯¼æ•°ï¼š"
            },
            {
              "display": "\\frac{d}{dx}\\text{LeakyReLU}(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ \\alpha & \\text{if } x \\leq 0 \\end{cases}"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Sigmoid",
          "formulas": [
            {
              "text": "Sigmoidå°†è¾“å‡ºå‹ç¼©åˆ°(0,1)åŒºé—´ï¼š"
            },
            {
              "display": "\\sigma(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{e^x + 1}"
            },
            {
              "text": "å¯¼æ•°ï¼š"
            },
            {
              "display": "\\frac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x)) = \\sigma(x) \\cdot \\sigma(-x)"
            },
            {
              "text": "ç‰¹ç‚¹ï¼šè¾“å‡ºæœ‰ç•Œã€å¹³æ»‘ï¼Œä½†å®¹æ˜“é¥±å’Œã€æ¢¯åº¦æ¶ˆå¤±"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Tanh (åŒæ›²æ­£åˆ‡)",
          "formulas": [
            {
              "text": "Tanhå°†è¾“å‡ºå‹ç¼©åˆ°(-1,1)åŒºé—´ï¼š"
            },
            {
              "display": "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = \\frac{\\sinh(x)}{\\cosh(x)}"
            },
            {
              "text": "å¯¼æ•°ï¼š"
            },
            {
              "display": "\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x) = \\text{sech}^2(x)"
            },
            {
              "text": "ç‰¹ç‚¹ï¼šé›¶ä¸­å¿ƒåŒ–ã€æ”¶æ•›æ›´å¿«ï¼Œä½†ä»å­˜åœ¨é¥±å’Œé—®é¢˜"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "GELU (Gaussian Error Linear Unit)",
          "formulas": [
            {
              "text": "GELUæ˜¯Transformerä¸­å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼š"
            },
            {
              "display": "\\text{GELU}(x) = x \\cdot \\Phi(x)"
            },
            {
              "text": "å…¶ä¸­ $\\Phi(x)$ æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼š"
            },
            {
              "display": "\\Phi(x) = \\frac{1}{2}\\left[1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right]"
            },
            {
              "text": "è¿‘ä¼¼è®¡ç®—ï¼ˆå¸¸ç”¨ï¼‰ï¼š"
            },
            {
              "display": "\\text{GELU}(x) \\approx 0.5x\\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}}\\left(x + 0.044715x^3\\right)\\right]\\right)"
            },
            {
              "text": "å¯¼æ•°ï¼š"
            },
            {
              "display": "\\frac{d}{dx}\\text{GELU}(x) = \\Phi(x) + x \\cdot \\phi(x)"
            },
            {
              "text": "å…¶ä¸­ $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}$ æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Swish/SiLU (Sigmoid Linear Unit)",
          "formulas": [
            {
              "text": "Swishæ˜¯è‡ªé—¨æ§æ¿€æ´»å‡½æ•°ï¼š"
            },
            {
              "display": "\\text{Swish}(x) = x \\cdot \\sigma(x) = \\frac{x}{1 + e^{-x}}"
            },
            {
              "text": "å¯¼æ•°ï¼š"
            },
            {
              "display": "\\frac{d}{dx}\\text{Swish}(x) = \\sigma(x) + x \\cdot \\sigma(x)(1 - \\sigma(x)) = \\sigma(x)(1 + x(1 - \\sigma(x)))"
            },
            {
              "text": "ç‰¹ç‚¹ï¼šå¹³æ»‘ã€éå•è°ƒã€æ€§èƒ½ä¼˜äºReLU"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "ELU (Exponential Linear Unit)",
          "formulas": [
            {
              "text": "ELUåœ¨è´Ÿå€¼éƒ¨åˆ†å¹³æ»‘ï¼š"
            },
            {
              "display": "\\text{ELU}(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha(e^x - 1) & \\text{if } x \\leq 0 \\end{cases}"
            },
            {
              "text": "å¯¼æ•°ï¼š"
            },
            {
              "display": "\\frac{d}{dx}\\text{ELU}(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ \\alpha e^x & \\text{if } x \\leq 0 \\end{cases}"
            },
            {
              "text": "ç‰¹ç‚¹ï¼šè´Ÿå€¼éƒ¨åˆ†å¹³æ»‘ï¼Œæœ‰åŠ©äºæ¢¯åº¦ä¼ æ’­"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Mish",
          "formulas": [
            {
              "text": "Mishæ˜¯è‡ªæ­£åˆ™åŒ–çš„æ¿€æ´»å‡½æ•°ï¼š"
            },
            {
              "display": "\\text{Mish}(x) = x \\cdot \\tanh(\\text{softplus}(x)) = x \\cdot \\tanh(\\ln(1 + e^x))"
            },
            {
              "text": "å…¶ä¸­ $\\text{softplus}(x) = \\ln(1 + e^x)$"
            },
            {
              "text": "ç‰¹ç‚¹ï¼šå¹³æ»‘ã€æ— ä¸Šç•Œã€è‡ªæ­£åˆ™åŒ–"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æŸå¤±å‡½æ•°æ•°å­¦è¡¨è¾¾",
      "content": [
        {
          "type": "math-box",
          "title": "å‡æ–¹è¯¯å·® (MSE)",
          "formulas": [
            {
              "text": "MSEç”¨äºå›å½’ä»»åŠ¡ï¼š"
            },
            {
              "display": "L_{\\text{MSE}} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2"
            },
            {
              "text": "æ¢¯åº¦ï¼š"
            },
            {
              "display": "\\frac{\\partial L}{\\partial \\hat{y}_i} = -\\frac{2}{n}(y_i - \\hat{y}_i)"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "äº¤å‰ç†µæŸå¤± (Cross-Entropy)",
          "formulas": [
            {
              "text": "äº¤å‰ç†µç”¨äºåˆ†ç±»ä»»åŠ¡ï¼š"
            },
            {
              "display": "L_{\\text{CE}} = -\\sum_{i=1}^{n}\\sum_{c=1}^{C}y_{i,c} \\log(\\hat{y}_{i,c})"
            },
            {
              "text": "å¯¹äºäºŒåˆ†ç±»ï¼ˆäºŒå…ƒäº¤å‰ç†µï¼‰ï¼š"
            },
            {
              "display": "L_{\\text{BCE}} = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]"
            },
            {
              "text": "æ¢¯åº¦ï¼š"
            },
            {
              "display": "\\frac{\\partial L}{\\partial \\hat{y}_i} = -\\frac{y_i}{\\hat{y}_i} + \\frac{1-y_i}{1-\\hat{y}_i}"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Huber Loss",
          "formulas": [
            {
              "text": "Huber Losså¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼š"
            },
            {
              "display": "L_{\\delta}(y, \\hat{y}) = \\begin{cases} \\frac{1}{2}(y - \\hat{y})^2 & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\ \\delta|y - \\hat{y}| - \\frac{1}{2}\\delta^2 & \\text{otherwise} \\end{cases}"
            },
            {
              "text": "ç‰¹ç‚¹ï¼šç»“åˆMSEå’ŒMAEçš„ä¼˜ç‚¹ï¼Œå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ å½’ä¸€åŒ–å‡½æ•°æ•°å­¦è¡¨è¾¾",
      "content": [
        {
          "type": "math-box",
          "title": "Softmax",
          "formulas": [
            {
              "text": "Softmaxå°†logitsè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼š"
            },
            {
              "display": "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}}"
            },
            {
              "text": "æ€§è´¨ï¼š"
            },
            {
              "text": "- è¾“å‡ºåœ¨(0,1)åŒºé—´ï¼Œä¸”å’Œä¸º1ï¼š$\\sum_{i=1}^{n}\\text{softmax}(x_i) = 1$"
            },
            {
              "text": "- å•è°ƒé€’å¢ï¼š$x_i > x_j \\Rightarrow \\text{softmax}(x_i) > \\text{softmax}(x_j)$"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "LayerNorm",
          "formulas": [
            {
              "text": "LayerNormå¯¹æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼š"
            },
            {
              "display": "\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta"
            },
            {
              "text": "å…¶ä¸­ï¼š"
            },
            {
              "display": "\\mu = \\frac{1}{d}\\sum_{i=1}^{d}x_i, \\quad \\sigma^2 = \\frac{1}{d}\\sum_{i=1}^{d}(x_i - \\mu)^2"
            },
            {
              "text": "$\\gamma$ å’Œ $\\beta$ æ˜¯å¯å­¦ä¹ çš„ç¼©æ”¾å’Œåç§»å‚æ•°"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "BatchNorm",
          "formulas": [
            {
              "text": "BatchNormå¯¹æ‰¹æ¬¡ç»´åº¦è¿›è¡Œå½’ä¸€åŒ–ï¼š"
            },
            {
              "display": "\\text{BatchNorm}(x) = \\gamma \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} + \\beta"
            },
            {
              "text": "å…¶ä¸­ï¼š"
            },
            {
              "display": "\\mu_B = \\frac{1}{m}\\sum_{i=1}^{m}x_i, \\quad \\sigma_B^2 = \\frac{1}{m}\\sum_{i=1}^{m}(x_i - \\mu_B)^2"
            },
            {
              "text": "$m$ æ˜¯æ‰¹æ¬¡å¤§å°"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ è·ç¦»ä¸ç›¸ä¼¼åº¦å‡½æ•°",
      "content": [
        {
          "type": "math-box",
          "title": "æ¬§æ°è·ç¦» (Euclidean Distance)",
          "formulas": [
            {
              "text": "æ¬§æ°è·ç¦»è¡¡é‡å‘é‡é—´çš„ç›´çº¿è·ç¦»ï¼š"
            },
            {
              "display": "d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2} = ||\\mathbf{x} - \\mathbf{y}||_2"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "ä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)",
          "formulas": [
            {
              "text": "ä½™å¼¦ç›¸ä¼¼åº¦è¡¡é‡å‘é‡é—´çš„å¤¹è§’ï¼š"
            },
            {
              "display": "\\text{cos}(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{||\\mathbf{x}|| \\cdot ||\\mathbf{y}||} = \\frac{\\sum_{i=1}^{n}x_i y_i}{\\sqrt{\\sum_{i=1}^{n}x_i^2} \\cdot \\sqrt{\\sum_{i=1}^{n}y_i^2}}"
            },
            {
              "text": "å–å€¼èŒƒå›´ï¼š$[-1, 1]$ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºè¶Šç›¸ä¼¼"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "æ›¼å“ˆé¡¿è·ç¦» (Manhattan Distance)",
          "formulas": [
            {
              "text": "æ›¼å“ˆé¡¿è·ç¦»ï¼ˆL1è·ç¦»ï¼‰ï¼š"
            },
            {
              "display": "d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^{n}|x_i - y_i| = ||\\mathbf{x} - \\mathbf{y}||_1"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "æ•°å­¦å‡½æ•°å®ç°ä¸å¯è§†åŒ–",
          "language": "python",
          "code": "import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn.functional as F\n\n# 1. æ¿€æ´»å‡½æ•°å®ç°\ndef relu(x):\n    return np.maximum(0, x)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef gelu(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))\n\ndef swish(x):\n    return x * sigmoid(x)\n\ndef leaky_relu(x, alpha=0.01):\n    return np.where(x > 0, x, alpha * x)\n\n# 2. æŸå¤±å‡½æ•°å®ç°\ndef mse_loss(y_true, y_pred):\n    return np.mean((y_true - y_pred)**2)\n\ndef cross_entropy_loss(y_true, y_pred):\n    return -np.mean(y_true * np.log(y_pred + 1e-8))\n\ndef huber_loss(y_true, y_pred, delta=1.0):\n    error = y_true - y_pred\n    is_small = np.abs(error) <= delta\n    squared_loss = 0.5 * error**2\n    linear_loss = delta * np.abs(error) - 0.5 * delta**2\n    return np.where(is_small, squared_loss, linear_loss)\n\n# 3. å½’ä¸€åŒ–å‡½æ•°å®ç°\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\ndef layer_norm(x, eps=1e-8):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    var = np.var(x, axis=-1, keepdims=True)\n    return (x - mean) / np.sqrt(var + eps)\n\n# 4. è·ç¦»å‡½æ•°å®ç°\ndef euclidean_distance(x, y):\n    return np.sqrt(np.sum((x - y)**2))\n\ndef cosine_similarity(x, y):\n    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n\ndef manhattan_distance(x, y):\n    return np.sum(np.abs(x - y))\n\n# 5. å¯è§†åŒ–æ¿€æ´»å‡½æ•°\ndef plot_activation_functions():\n    x = np.linspace(-5, 5, 100)\n    \n    functions = {\n        'ReLU': relu(x),\n        'Sigmoid': sigmoid(x),\n        'Tanh': tanh(x),\n        'GELU': gelu(x),\n        'Swish': swish(x),\n        'Leaky ReLU': leaky_relu(x)\n    }\n    \n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.flatten()\n    \n    for i, (name, y) in enumerate(functions.items()):\n        axes[i].plot(x, y, linewidth=2)\n        axes[i].set_title(name, fontsize=14, fontweight='bold')\n        axes[i].grid(True, alpha=0.3)\n        axes[i].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n        axes[i].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n        axes[i].set_xlabel('x')\n        axes[i].set_ylabel('f(x)')\n    \n    plt.tight_layout()\n    plt.savefig('activation_functions.png', dpi=300)\n    print('æ¿€æ´»å‡½æ•°å›¾åƒå·²ä¿å­˜')\n\n# 6. å¯è§†åŒ–æŸå¤±å‡½æ•°\ndef plot_loss_functions():\n    y_true = 0.5\n    y_pred = np.linspace(0, 1, 100)\n    \n    mse = (y_true - y_pred)**2\n    bce = -(y_true * np.log(y_pred + 1e-8) + (1-y_true) * np.log(1-y_pred + 1e-8))\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(y_pred, mse, label='MSE', linewidth=2)\n    plt.plot(y_pred, bce, label='BCE', linewidth=2)\n    plt.axvline(x=y_true, color='r', linestyle='--', alpha=0.5, label=f'True value: {y_true}')\n    plt.xlabel('Predicted value')\n    plt.ylabel('Loss')\n    plt.title('Loss Functions Comparison')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.savefig('loss_functions.png', dpi=300)\n    print('æŸå¤±å‡½æ•°å›¾åƒå·²ä¿å­˜')\n\n# 7. ä½¿ç”¨PyTorchå®ç°\nx = torch.linspace(-5, 5, 100)\n\n# æ¿€æ´»å‡½æ•°\nrelu_torch = F.relu(x)\nsigmoid_torch = torch.sigmoid(x)\ntanh_torch = torch.tanh(x)\ngelu_torch = F.gelu(x)\nsilu_torch = F.silu(x)\n\n# æŸå¤±å‡½æ•°\ny_true = torch.tensor([1.0, 0.0, 1.0])\ny_pred = torch.tensor([0.9, 0.1, 0.8])\nmse_loss_torch = F.mse_loss(y_pred, y_true)\nbce_loss_torch = F.binary_cross_entropy(y_pred, y_true)\n\nprint(f'MSE Loss: {mse_loss_torch.item():.4f}')\nprint(f'BCE Loss: {bce_loss_torch.item():.4f}')\n\n# 8. Softmaxç¤ºä¾‹\nlogits = torch.randn(3, 5)  # [batch_size, num_classes]\nprobs = F.softmax(logits, dim=1)\nprint(f'Softmaxè¾“å‡ºå½¢çŠ¶: {probs.shape}')\nprint(f'æ¯è¡Œå’Œä¸º1: {probs.sum(dim=1)}')\n\n# 9. è·ç¦»è®¡ç®—ç¤ºä¾‹\nx = torch.tensor([1.0, 2.0, 3.0])\ny = torch.tensor([4.0, 5.0, 6.0])\n\neuclidean = torch.norm(x - y)\ncosine = F.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0))\nmanhattan = torch.sum(torch.abs(x - y))\n\nprint(f'æ¬§æ°è·ç¦»: {euclidean.item():.4f}')\nprint(f'ä½™å¼¦ç›¸ä¼¼åº¦: {cosine.item():.4f}')\nprint(f'æ›¼å“ˆé¡¿è·ç¦»: {manhattan.item():.4f}')"
        }
      ]
    }
  ]
}
