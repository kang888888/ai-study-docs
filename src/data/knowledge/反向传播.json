{
  "title": "åå‘ä¼ æ’­ (Backpropagation)",
  "subtitle": "é€šè¿‡é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦ï¼Œä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚ä¼ æ’­",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•ï¼Œé€šè¿‡é“¾å¼æ³•åˆ™ä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚åå‘ä¼ æ’­è¯¯å·®ï¼Œè®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚åå‘ä¼ æ’­ä½¿å¾—æ·±å±‚ç¥ç»ç½‘ç»œçš„è®­ç»ƒæˆä¸ºå¯èƒ½ï¼Œæ˜¯ç°ä»£æ·±åº¦å­¦ä¹ çš„åŸºç¡€ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "é“¾å¼æ³•åˆ™ï¼šåˆ©ç”¨å¤åˆå‡½æ•°æ±‚å¯¼çš„é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦",
            "åå‘ä¼ æ’­ï¼šä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚é€å±‚è®¡ç®—æ¢¯åº¦",
            "å‰å‘ä¼ æ’­ï¼šå…ˆè¿›è¡Œå‰å‘ä¼ æ’­è®¡ç®—è¾“å‡ºå’ŒæŸå¤±",
            "æ¢¯åº¦ç´¯ç§¯ï¼šæ¯ä¸ªèŠ‚ç‚¹çš„æ¢¯åº¦æ˜¯åç»­èŠ‚ç‚¹æ¢¯åº¦çš„åŠ æƒå’Œ",
            "è®¡ç®—é«˜æ•ˆï¼šç›¸æ¯”æ•°å€¼æ¢¯åº¦ï¼Œè®¡ç®—æ•ˆç‡é«˜å¾—å¤š"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "é“¾å¼æ³•åˆ™ã€è®¡ç®—å›¾ã€è‡ªåŠ¨å¾®åˆ†ã€æ¢¯åº¦æ£€æŸ¥ã€æ¢¯åº¦ç´¯ç§¯"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "ç¥ç»ç½‘ç»œè®­ç»ƒã€æ·±åº¦å­¦ä¹ ã€å‚æ•°ä¼˜åŒ–ã€æ¢¯åº¦è®¡ç®—"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "åå‘ä¼ æ’­æµç¨‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "åå‘ä¼ æ’­æµç¨‹",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "åå‘ä¼ æ’­ç¤ºæ„å›¾",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "åå‘ä¼ æ’­ç¤ºæ„å›¾",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "é“¾å¼æ³•åˆ™",
          "formulas": [
            {
              "text": "å¯¹äºå¤åˆå‡½æ•° $z = f(g(x))$ï¼Œæ¢¯åº¦è®¡ç®—ï¼š"
            },
            {
              "display": "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial g} \\cdot \\frac{\\partial g}{\\partial x}"
            },
            {
              "text": "åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œä»è¾“å‡ºå±‚ $L$ åˆ°è¾“å…¥å±‚ $x$ï¼š"
            },
            {
              "display": "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial h_n} \\cdot \\frac{\\partial h_n}{\\partial h_{n-1}} \\cdots \\frac{\\partial h_1}{\\partial x}"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "åå‘ä¼ æ’­ç®—æ³•",
          "formulas": [
            {
              "text": "å¯¹äºç¬¬ $l$ å±‚ï¼Œæ¢¯åº¦è®¡ç®—ï¼š"
            },
            {
              "display": "\\frac{\\partial L}{\\partial W_l} = \\frac{\\partial L}{\\partial h_l} \\cdot \\frac{\\partial h_l}{\\partial W_l}"
            },
            {
              "text": "å…¶ä¸­ $h_l$ æ˜¯ç¬¬ $l$ å±‚çš„è¾“å‡ºï¼Œ$W_l$ æ˜¯ç¬¬ $l$ å±‚çš„æƒé‡"
            },
            {
              "text": "è¯¯å·®åå‘ä¼ æ’­ï¼š"
            },
            {
              "display": "\\delta_l = \\frac{\\partial L}{\\partial h_l} = \\delta_{l+1} \\cdot \\frac{\\partial h_{l+1}}{\\partial h_l}"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "æƒé‡æ›´æ–°",
          "formulas": [
            {
              "text": "è®¡ç®—æ¢¯åº¦åï¼Œæ›´æ–°æƒé‡ï¼š"
            },
            {
              "display": "W_l \\leftarrow W_l - \\eta \\frac{\\partial L}{\\partial W_l}"
            },
            {
              "text": "å…¶ä¸­ $\\eta$ æ˜¯å­¦ä¹ ç‡",
              "inline": "\\eta"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "PyTorch ä¸­çš„åå‘ä¼ æ’­",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# 1. è‡ªåŠ¨åå‘ä¼ æ’­ï¼ˆæœ€ç®€å•ï¼‰\nx = torch.tensor([2.0], requires_grad=True)\ny = x ** 2\nz = y * 3\n\nz.backward()  # è‡ªåŠ¨è®¡ç®—æ¢¯åº¦\nprint(f\"dz/dx = {x.grad}\")  # è¾“å‡º: tensor([12.]) å› ä¸º dz/dx = 3 * 2x = 12\n\n# 2. å¤šå±‚ç½‘ç»œçš„åå‘ä¼ æ’­\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.fc1 = nn.Linear(10, 5)\n        self.fc2 = nn.Linear(5, 1)\n    \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nmodel = SimpleNet()\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# å‰å‘ä¼ æ’­\nx = torch.randn(32, 10)\ntarget = torch.randn(32, 1)\noutput = model(x)\n\n# è®¡ç®—æŸå¤±\nloss = criterion(output, target)\nprint(f\"Loss: {loss.item()}\")\n\n# åå‘ä¼ æ’­\noptimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦\nloss.backward()  # è®¡ç®—æ¢¯åº¦\n\n# æŸ¥çœ‹æ¢¯åº¦\nfor name, param in model.named_parameters():\n    if param.grad is not None:\n        print(f\"{name} æ¢¯åº¦å½¢çŠ¶: {param.grad.shape}\")\n\n# æ›´æ–°å‚æ•°\noptimizer.step()\n\n# 3. æ‰‹åŠ¨å®ç°åå‘ä¼ æ’­ï¼ˆç†è§£åŸç†ï¼‰\nclass ManualBackprop:\n    def __init__(self):\n        # åˆå§‹åŒ–æƒé‡\n        self.W1 = torch.randn(10, 5, requires_grad=False)\n        self.b1 = torch.randn(5, requires_grad=False)\n        self.W2 = torch.randn(5, 1, requires_grad=False)\n        self.b2 = torch.randn(1, requires_grad=False)\n    \n    def forward(self, x):\n        # å‰å‘ä¼ æ’­\n        self.z1 = x @ self.W1 + self.b1\n        self.h1 = torch.relu(self.z1)\n        self.z2 = self.h1 @ self.W2 + self.b2\n        return self.z2\n    \n    def backward(self, x, y, output, lr=0.01):\n        # æ‰‹åŠ¨åå‘ä¼ æ’­\n        m = x.size(0)\n        \n        # è¾“å‡ºå±‚æ¢¯åº¦\n        dz2 = (output - y) / m\n        \n        # ç¬¬äºŒå±‚æ¢¯åº¦\n        dW2 = self.h1.t() @ dz2\n        db2 = dz2.sum(0)\n        \n        # ç¬¬ä¸€å±‚æ¢¯åº¦\n        dh1 = dz2 @ self.W2.t()\n        dz1 = dh1 * (self.z1 > 0).float()  # ReLUçš„å¯¼æ•°\n        dW1 = x.t() @ dz1\n        db1 = dz1.sum(0)\n        \n        # æ›´æ–°æƒé‡\n        self.W2 -= lr * dW2\n        self.b2 -= lr * db2\n        self.W1 -= lr * dW1\n        self.b1 -= lr * db1\n\n# ä½¿ç”¨æ‰‹åŠ¨åå‘ä¼ æ’­\nmanual_model = ManualBackprop()\nx = torch.randn(32, 10)\ny = torch.randn(32, 1)\n\nfor epoch in range(100):\n    output = manual_model.forward(x)\n    loss = F.mse_loss(output, y)\n    manual_model.backward(x, y, output)\n    \n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n\n# 4. æ¢¯åº¦æ£€æŸ¥ï¼ˆéªŒè¯åå‘ä¼ æ’­æ­£ç¡®æ€§ï¼‰\ndef gradient_check(model, x, y, epsilon=1e-7):\n    \"\"\"æ•°å€¼æ¢¯åº¦æ£€æŸ¥\"\"\"\n    model.zero_grad()\n    output = model(x)\n    loss = nn.MSELoss()(output, y)\n    loss.backward()\n    \n    # è·å–è§£ææ¢¯åº¦\n    analytical_grads = {}\n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            analytical_grads[name] = param.grad.clone()\n    \n    # è®¡ç®—æ•°å€¼æ¢¯åº¦\n    numerical_grads = {}\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            grad = torch.zeros_like(param)\n            for i in range(param.numel()):\n                flat_param = param.view(-1)\n                orig_val = flat_param[i].item()\n                \n                # f(x + epsilon)\n                flat_param[i] = orig_val + epsilon\n                output_plus = model(x)\n                loss_plus = nn.MSELoss()(output_plus, y)\n                \n                # f(x - epsilon)\n                flat_param[i] = orig_val - epsilon\n                output_minus = model(x)\n                loss_minus = nn.MSELoss()(output_minus, y)\n                \n                # æ•°å€¼æ¢¯åº¦\n                grad.view(-1)[i] = (loss_plus - loss_minus) / (2 * epsilon)\n                flat_param[i] = orig_val  # æ¢å¤åŸå€¼\n            \n            numerical_grads[name] = grad\n    \n    # æ¯”è¾ƒæ¢¯åº¦\n    for name in analytical_grads:\n        diff = (analytical_grads[name] - numerical_grads[name]).abs().max().item()\n        print(f\"{name}: æœ€å¤§å·®å¼‚ = {diff:.2e}\")\n        if diff > 1e-5:\n            print(f\"è­¦å‘Š: {name} çš„æ¢¯åº¦å¯èƒ½ä¸æ­£ç¡®ï¼\")\n\n# ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥\nmodel = SimpleNet()\ngradient_check(model, torch.randn(32, 10), torch.randn(32, 1))"
        }
      ]
    }
  ]
}