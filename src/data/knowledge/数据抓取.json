{
  "title": "æ•°æ®æŠ“å–",
  "subtitle": "é€šè¿‡ç½‘ç»œçˆ¬è™«ã€APIè°ƒç”¨ç­‰æ–¹å¼æ”¶é›†è®­ç»ƒæ•°æ®ï¼Œå¹¶è¿›è¡Œæ¸…æ´—ä¸å»é‡ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æ•°æ®æŠ“å–æµç¨‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "flow",
                "title": "æ•°æ®æŠ“å–æµç¨‹",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æ•°æ®æŠ“å–æ¶æ„",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "æ•°æ®æŠ“å–æ¶æ„",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ç½‘ç»œçˆ¬è™«ç¤ºä¾‹",
          "language": "python",
          "code": "import requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin, urlparse\n\nclass WebCrawler:\n    def __init__(self, base_url):\n        self.base_url = base_url\n        self.visited = set()\n    \n    def crawl(self, url, max_depth=3):\n        if url in self.visited or max_depth == 0:\n            return []\n        \n        self.visited.add(url)\n        try:\n            response = requests.get(url, timeout=10)\n            soup = BeautifulSoup(response.content, 'html.parser')\n            \n            # æå–æ–‡æœ¬\n            text = soup.get_text(separator='\\n', strip=True)\n            \n            # æå–é“¾æ¥\n            links = [urljoin(url, a['href']) for a in soup.find_all('a', href=True)]\n            \n            return [text] + [self.crawl(link, max_depth-1) for link in links[:5]]\n        except Exception as e:\n            print(f\"Error crawling {url}: {e}\")\n            return []"
        },
        {
          "type": "code-box",
          "title": "APIæ•°æ®è·å–",
          "language": "python",
          "code": "import requests\nimport json\nfrom typing import List, Dict\n\ndef fetch_api_data(api_url: str, params: Dict = None) -> List[Dict]:\n    \"\"\"ä»APIè·å–æ•°æ®\"\"\"\n    all_data = []\n    page = 1\n    \n    while True:\n        params = params or {}\n        params['page'] = page\n        \n        response = requests.get(api_url, params=params)\n        data = response.json()\n        \n        if not data.get('results'):\n            break\n        \n        all_data.extend(data['results'])\n        \n        if not data.get('next'):\n            break\n        \n        page += 1\n    \n    return all_data"
        }
      ]
    }
  ]
}
