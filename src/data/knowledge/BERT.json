{
  "title": "BERT (Bidirectional Encoder Representations from Transformers)",
  "subtitle": "Googleçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "Googleåœ¨2018å¹´æå‡ºçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œåªä½¿ç”¨Transformerçš„Encoderéƒ¨åˆ†ã€‚é€šè¿‡æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰å’Œä¸‹ä¸€å¥é¢„æµ‹ï¼ˆNSPï¼‰ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ åŒå‘ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "åŒå‘ç†è§£ï¼šåŒæ—¶åˆ©ç”¨å·¦ä¾§å’Œå³ä¾§çš„ä¸Šä¸‹æ–‡ä¿¡æ¯",
            "æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰ï¼šéšæœºé®ç›–15%çš„è¯ï¼Œé¢„æµ‹è¢«é®ç›–çš„è¯",
            "é¢„è®­ç»ƒ+å¾®è°ƒï¼šåœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šé¢„è®­ç»ƒï¼Œç„¶ååœ¨ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒ",
            "åªæœ‰Encoderï¼šä¸åŒ…å«Decoderï¼Œä¸é€‚åˆç”Ÿæˆä»»åŠ¡",
            "SOTAæ€§èƒ½ï¼šåœ¨å¤šä¸ªNLPç†è§£ä»»åŠ¡ä¸Šåˆ·æ–°è®°å½•"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "Masked Language Modelã€Next Sentence Predictionã€WordPieceåˆ†è¯ã€[CLS]å’Œ[SEP]ç‰¹æ®ŠToken"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€é—®ç­”ç³»ç»Ÿï¼ˆQAï¼‰ã€è¯­ä¹‰ç›¸ä¼¼åº¦ã€æƒ…æ„Ÿåˆ†æ"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "BERTDiagram",
              "caption": "BERTæ¶æ„å›¾",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "BERTæ¶æ„å›¾"
              }
            },
            {
              "type": "svg-d3",
              "component": "BERTDiagram",
              "caption": "BERT MLMå¯è§†åŒ–",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "mlm",
                "title": "BERT MLMå¯è§†åŒ–"
              }
            },
            {
              "type": "svg-d3",
              "component": "BERTDiagram",
              "caption": "BERTåŒå‘æ³¨æ„åŠ›",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "attention",
                "title": "BERTåŒå‘æ³¨æ„åŠ›"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰æŸå¤±",
          "formulas": [
            {
              "text": "å¯¹äºè¢«æ©ç çš„ä½ç½® $m$ï¼Œé¢„æµ‹è¢«æ©ç çš„è¯ï¼š",
              "inline": "m"
            },
            {
              "display": "L_{MLM} = -\\sum_{m \\in M} \\log P(x_m | x_{\\backslash m})"
            },
            {
              "text": "å…¶ä¸­ $M$ æ˜¯è¢«æ©ç çš„ä½ç½®é›†åˆï¼Œ$x_{\\backslash m}$ æ˜¯é™¤ä½ç½® $m$ å¤–çš„æ‰€æœ‰è¯",
              "inline": "M"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "ä¸‹ä¸€å¥é¢„æµ‹ï¼ˆNSPï¼‰æŸå¤±",
          "formulas": [
            {
              "text": "é¢„æµ‹å¥å­Bæ˜¯å¦æ˜¯å¥å­Açš„ä¸‹ä¸€å¥ï¼š"
            },
            {
              "display": "L_{NSP} = -\\log P(\\text{IsNext} | \\text{CLS})"
            },
            {
              "text": "æ€»æŸå¤±ï¼š$L = L_{MLM} + L_{NSP}$",
              "inline": "L = L_{MLM} + L_{NSP}"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "åŒå‘æ³¨æ„åŠ›",
          "formulas": [
            {
              "text": "BERTä½¿ç”¨åŒå‘è‡ªæ³¨æ„åŠ›ï¼Œæ¯ä¸ªè¯å¯ä»¥åŒæ—¶çœ‹åˆ°å·¦å³ä¸¤ä¾§çš„ä¸Šä¸‹æ–‡ï¼š"
            },
            {
              "display": "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V"
            },
            {
              "text": "ä¸GPTçš„å•å‘æ³¨æ„åŠ›ä¸åŒï¼ŒBERTå¯ä»¥åŒæ—¶åˆ©ç”¨å‰åæ–‡ä¿¡æ¯"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ Transformers åº“åŠ è½½ BERT",
          "language": "python",
          "code": "from transformers import BertModel, BertTokenizer, BertForMaskedLM\nimport torch\n\n# åŠ è½½é¢„è®­ç»ƒçš„BERTæ¨¡å‹å’Œåˆ†è¯å™¨\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# è¾“å…¥æ–‡æœ¬\ntext = \"The cat sat on the [MASK].\"\n\n# åˆ†è¯å’Œç¼–ç \ninputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n\n# å‰å‘ä¼ æ’­\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# è·å–è¯åµŒå…¥\nembeddings = outputs.last_hidden_state\nprint(f\"è¯åµŒå…¥å½¢çŠ¶: {embeddings.shape}\")  # [batch_size, seq_len, hidden_size]\n\n# ä½¿ç”¨MLMæ¨¡å‹è¿›è¡Œæ©ç é¢„æµ‹\nmlm_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\nwith torch.no_grad():\n    mlm_outputs = mlm_model(**inputs)\n    predictions = mlm_outputs.logits\n\n# é¢„æµ‹è¢«æ©ç çš„è¯\nmasked_index = inputs['input_ids'][0].tolist().index(tokenizer.mask_token_id)\npredicted_token_id = predictions[0, masked_index].argmax().item()\npredicted_token = tokenizer.decode([predicted_token_id])\nprint(f\"é¢„æµ‹çš„è¯: {predicted_token}\")"
        },
        {
          "type": "code-box",
          "title": "æ‰‹åŠ¨å®ç° BERT çš„æ©ç è¯­è¨€æ¨¡å‹",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass BertEmbedding(nn.Module):\n    \"\"\"BERTè¯åµŒå…¥å±‚\"\"\"\n    def __init__(self, vocab_size, hidden_size, max_seq_length, dropout=0.1):\n        super(BertEmbedding, self).__init__()\n        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n        self.position_embedding = nn.Embedding(max_seq_length, hidden_size)\n        self.segment_embedding = nn.Embedding(2, hidden_size)  # å¥å­Aå’ŒB\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input_ids, segment_ids=None):\n        seq_length = input_ids.size(1)\n        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        \n        if segment_ids is None:\n            segment_ids = torch.zeros_like(input_ids)\n        \n        token_emb = self.token_embedding(input_ids)\n        position_emb = self.position_embedding(position_ids)\n        segment_emb = self.segment_embedding(segment_ids)\n        \n        embeddings = token_emb + position_emb + segment_emb\n        embeddings = self.layer_norm(embeddings)\n        embeddings = self.dropout(embeddings)\n        \n        return embeddings\n\nclass BertMLMHead(nn.Module):\n    \"\"\"BERT MLMé¢„æµ‹å¤´\"\"\"\n    def __init__(self, hidden_size, vocab_size):\n        super(BertMLMHead, self).__init__()\n        self.dense = nn.Linear(hidden_size, hidden_size)\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        self.decoder = nn.Linear(hidden_size, vocab_size)\n    \n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = F.gelu(hidden_states)\n        hidden_states = self.layer_norm(hidden_states)\n        logits = self.decoder(hidden_states)\n        return logits\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    vocab_size = 30522  # BERT-baseè¯æ±‡è¡¨å¤§å°\n    hidden_size = 768\n    max_seq_length = 512\n    \n    embedding = BertEmbedding(vocab_size, hidden_size, max_seq_length)\n    mlm_head = BertMLMHead(hidden_size, vocab_size)\n    \n    # æ¨¡æ‹Ÿè¾“å…¥\n    input_ids = torch.randint(0, vocab_size, (2, 128))  # batch_size=2, seq_len=128\n    segment_ids = torch.zeros(2, 128, dtype=torch.long)\n    \n    # å‰å‘ä¼ æ’­\n    embeddings = embedding(input_ids, segment_ids)\n    print(f\"åµŒå…¥å½¢çŠ¶: {embeddings.shape}\")  # [2, 128, 768]\n    \n    # MLMé¢„æµ‹\n    logits = mlm_head(embeddings)\n    print(f\"MLM logitså½¢çŠ¶: {logits.shape}\")  # [2, 128, 30522]"
        }
      ]
    }
  ]
}