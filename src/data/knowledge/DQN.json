{
  "title": "DQN (Deep Q-Network) æ·±åº¦Qç½‘ç»œ",
  "subtitle": "ç»“åˆæ·±åº¦å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ çš„é©å‘½æ€§ç®—æ³•",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "DeepMindæå‡ºçš„å°†æ·±åº¦å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆçš„ç®—æ³•ã€‚ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿‘ä¼¼Qå‡½æ•°ï¼ˆçŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°ï¼‰ï¼Œé€šè¿‡ç»éªŒå›æ”¾å’Œç›®æ ‡ç½‘ç»œç¨³å®šè®­ç»ƒã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "Qå‡½æ•°è¿‘ä¼¼ï¼šç”¨ç¥ç»ç½‘ç»œæ‹ŸåˆQ(s, a)ï¼Œè§£å†³çŠ¶æ€ç©ºé—´è¿‡å¤§é—®é¢˜",
            "ç»éªŒå›æ”¾ï¼šå­˜å‚¨å†å²ç»éªŒï¼Œæ‰“ç ´æ•°æ®ç›¸å…³æ€§",
            "ç›®æ ‡ç½‘ç»œï¼šå›ºå®šç›®æ ‡Qå€¼ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹",
            "Îµ-è´ªå©ªç­–ç•¥ï¼šå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨",
            "ç«¯åˆ°ç«¯å­¦ä¹ ï¼šç›´æ¥ä»åƒç´ è¾“å…¥å­¦ä¹ ç­–ç•¥"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "Q-Learningã€ç»éªŒå›æ”¾ç¼“å†²åŒºï¼ˆReplay Bufferï¼‰ã€ç›®æ ‡ç½‘ç»œï¼ˆTarget Networkï¼‰ã€TDè¯¯å·®"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "æ¸¸æˆAIï¼ˆAtariæ¸¸æˆï¼‰ã€æœºå™¨äººæ§åˆ¶ã€èµ„æºè°ƒåº¦ã€è‡ªåŠ¨é©¾é©¶å†³ç­–"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "DQNDiagram",
              "caption": "DQNæ¶æ„å›¾",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "DQNæ¶æ„å›¾",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "DQNDiagram",
              "caption": "DQNè®­ç»ƒè¿‡ç¨‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "DQNè®­ç»ƒè¿‡ç¨‹",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "DQNDiagram",
              "caption": "Qå€¼å­¦ä¹ è¿‡ç¨‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Qå€¼å­¦ä¹ è¿‡ç¨‹",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "DQNDiagram",
              "caption": "DQNå˜ä½“å¯¹æ¯”",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "DQNå˜ä½“å¯¹æ¯”",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "DQNDiagram",
              "caption": "ä¸¤ç§è¾“å…¥è¾“å‡ºç»“æ„çš„DQNæ¨¡å‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "ä¸¤ç§è¾“å…¥è¾“å‡ºç»“æ„çš„DQNæ¨¡å‹",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "DQNDiagram",
              "caption": "DQNçš„å®é™…åº”ç”¨ç¤ºä¾‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "DQNçš„å®é™…åº”ç”¨ç¤ºä¾‹",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "DQNDiagram",
              "caption": "DQNçš„\"é«˜ä¼°\"é—®é¢˜",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "DQNçš„\"é«˜ä¼°\"é—®é¢˜",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "DQNDiagram",
              "caption": "ä»·å€¼å‡½æ•°QÏ€ä¸VÏ€çš„å…³ç³»",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "ä»·å€¼å‡½æ•°QÏ€ä¸VÏ€çš„å…³ç³»",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "DQNDiagram",
              "caption": "Æ-è´ªå©ªç­–ç•¥ä¸‹ä½¿ç”¨åŠ¨æ€çš„Æå€¼",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Æ-è´ªå©ªç­–ç•¥ä¸‹ä½¿ç”¨åŠ¨æ€çš„Æå€¼",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "DQNDiagram",
              "caption": "TDç›®æ ‡ä¸TDè¯¯å·®çš„å…³ç³»",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "TDç›®æ ‡ä¸TDè¯¯å·®çš„å…³ç³»",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "DQNDiagram",
              "caption": "TD(0)ã€å¤šæ­¥TDä¸è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰çš„å…³ç³»",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "TD(0)ã€å¤šæ­¥TDä¸è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰çš„å…³ç³»",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "DQNDiagram",
              "caption": "è’™ç‰¹å¡æ´›æ–¹æ³•ä¸TDæ–¹æ³•çš„ç‰¹æ€§",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "è’™ç‰¹å¡æ´›æ–¹æ³•ä¸TDæ–¹æ³•çš„ç‰¹æ€§",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "DQNDiagram",
              "caption": "å›æŠ¥ï¼ˆç´¯è®¡å¥–åŠ±ï¼‰",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "å›æŠ¥ï¼ˆç´¯è®¡å¥–åŠ±ï¼‰",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "DQNDiagram",
              "caption": "åå‘è¿­ä»£å¹¶è®¡ç®—å›æŠ¥G",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "åå‘è¿­ä»£å¹¶è®¡ç®—å›æŠ¥G",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "Q-Learning æ›´æ–°è§„åˆ™",
          "formulas": [
            {
              "text": "Qå€¼çš„æ›´æ–°å…¬å¼ï¼š"
            },
            {
              "display": "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha[r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)]"
            },
            {
              "text": "å…¶ä¸­ï¼š"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "DQN æŸå¤±å‡½æ•°",
          "formulas": [
            {
              "text": "ä½¿ç”¨ç¥ç»ç½‘ç»œè¿‘ä¼¼Qå‡½æ•°ï¼ŒæŸå¤±å‡½æ•°ä¸ºï¼š"
            },
            {
              "display": "L(\\theta) = \\mathbb{E}[(r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) - Q(s, a; \\theta))^2]"
            },
            {
              "text": "å…¶ä¸­ $\\theta$ æ˜¯ä¸»ç½‘ç»œå‚æ•°ï¼Œ$\\theta^-$ æ˜¯ç›®æ ‡ç½‘ç»œå‚æ•°",
              "inline": "\\theta"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Îµ-è´ªå©ªç­–ç•¥",
          "formulas": [
            {
              "text": "å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼š"
            },
            {
              "display": "a_t = \\begin{cases}\n                        \\text{éšæœºåŠ¨ä½œ} &amp; \\text{ä»¥æ¦‚ç‡ } \\epsilon \\\\\n                        \\arg\\max_a Q(s_t, a) &amp; \\text{ä»¥æ¦‚ç‡ } 1-\\epsilon\n                        \\end{cases}"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ PyTorch å®ç° DQN",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections import deque\nimport random\n\nclass DQN(nn.Module):\n    \"\"\"Deep Q-Network æ¨¡å‹\"\"\"\n    def __init__(self, state_size, action_size, hidden_size=128):\n        super(DQN, self).__init__()\n        \n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, action_size)\n    \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\nclass ReplayBuffer:\n    \"\"\"ç»éªŒå›æ”¾ç¼“å†²åŒº\"\"\"\n    def __init__(self, capacity=10000):\n        self.buffer = deque(maxlen=capacity)\n    \n    def push(self, state, action, reward, next_state, done):\n        self.buffer.append((state, action, reward, next_state, done))\n    \n    def sample(self, batch_size):\n        batch = random.sample(self.buffer, batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n        \n        return (np.array(states), np.array(actions), np.array(rewards),\n                np.array(next_states), np.array(dones))\n    \n    def __len__(self):\n        return len(self.buffer)\n\nclass DQNAgent:\n    \"\"\"DQN æ™ºèƒ½ä½“\"\"\"\n    def __init__(self, state_size, action_size, lr=0.001, gamma=0.99,\n                 epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995,\n                 memory_size=10000, batch_size=64, target_update=100):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.batch_size = batch_size\n        self.target_update = target_update\n        self.update_counter = 0\n        \n        # ä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ\n        self.q_network = DQN(state_size, action_size)\n        self.target_network = DQN(state_size, action_size)\n        self.target_network.load_state_dict(self.q_network.state_dict())\n        \n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n        self.memory = ReplayBuffer(memory_size)\n    \n    def remember(self, state, action, reward, next_state, done):\n        \"\"\"å­˜å‚¨ç»éªŒ\"\"\"\n        self.memory.push(state, action, reward, next_state, done)\n    \n    def act(self, state, training=True):\n        \"\"\"é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-è´ªå©ªç­–ç•¥ï¼‰\"\"\"\n        if training and np.random.random() <= self.epsilon:\n            return random.randrange(self.action_size)\n        \n        state = torch.FloatTensor(state).unsqueeze(0)\n        q_values = self.q_network(state)\n        return q_values.argmax().item()\n    \n    def replay(self):\n        \"\"\"ç»éªŒå›æ”¾è®­ç»ƒ\"\"\"\n        if len(self.memory) < self.batch_size:\n            return\n        \n        # ä»ç¼“å†²åŒºé‡‡æ ·\n        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n        \n        states = torch.FloatTensor(states)\n        actions = torch.LongTensor(actions)\n        rewards = torch.FloatTensor(rewards)\n        next_states = torch.FloatTensor(next_states)\n        dones = torch.BoolTensor(dones)\n        \n        # å½“å‰Qå€¼\n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n        \n        # ç›®æ ‡Qå€¼\n        with torch.no_grad():\n            next_q_values = self.target_network(next_states).max(1)[0]\n            target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n        \n        # è®¡ç®—æŸå¤±\n        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n        \n        # ä¼˜åŒ–\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        # æ›´æ–°epsilon\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n        \n        # æ›´æ–°ç›®æ ‡ç½‘ç»œ\n        self.update_counter += 1\n        if self.update_counter % self.target_update == 0:\n            self.target_network.load_state_dict(self.q_network.state_dict())\n        \n        return loss.item()\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    # åˆ›å»ºæ™ºèƒ½ä½“\n    agent = DQNAgent(state_size=4, action_size=2)\n    \n    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹\n    for episode in range(100):\n        state = np.random.randn(4)  # åˆå§‹çŠ¶æ€\n        \n        for step in range(200):\n            # é€‰æ‹©åŠ¨ä½œ\n            action = agent.act(state)\n            \n            # æ‰§è¡ŒåŠ¨ä½œï¼Œè·å¾—å¥–åŠ±å’Œä¸‹ä¸€çŠ¶æ€ï¼ˆè¿™é‡Œç”¨éšæœºå€¼æ¨¡æ‹Ÿï¼‰\n            next_state = np.random.randn(4)\n            reward = np.random.randn()\n            done = step == 199\n            \n            # å­˜å‚¨ç»éªŒ\n            agent.remember(state, action, reward, next_state, done)\n            \n            # è®­ç»ƒ\n            if len(agent.memory) > agent.batch_size:\n                loss = agent.replay()\n                if step % 10 == 0:\n                    print(f\"Episode {episode}, Step {step}, Loss: {loss:.4f}\")\n            \n            state = next_state\n            \n            if done:\n                break"
        }
      ]
    }
  ]
}