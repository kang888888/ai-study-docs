{
  "title": "å­¦ä¹ ç‡è°ƒåº¦ (Learning Rate Scheduling)",
  "subtitle": "å­¦ä¹ ç‡è¡°å‡ç­–ç•¥ã€Warmupã€Cosine Annealingç­‰è°ƒåº¦æ–¹æ³•",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "å­¦ä¹ ç‡è°ƒåº¦æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡çš„ç­–ç•¥ã€‚å›ºå®šçš„å­¦ä¹ ç‡å¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šæˆ–æ”¶æ•›ç¼“æ…¢ï¼Œè€Œåˆç†çš„å­¦ä¹ ç‡è°ƒåº¦èƒ½å¤ŸåŠ é€Ÿæ”¶æ•›ã€æé«˜æ¨¡å‹æ€§èƒ½ã€‚å¸¸è§çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥åŒ…æ‹¬å›ºå®šè¡°å‡ã€ä½™å¼¦é€€ç«ã€Warmupç­‰ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "StepLRï¼šå›ºå®šæ­¥é•¿è¡°å‡ï¼Œæ¯éš”ä¸€å®šepoché™ä½å­¦ä¹ ç‡",
            "CosineAnnealingLRï¼šä½™å¼¦é€€ç«ï¼Œå­¦ä¹ ç‡æŒ‰ä½™å¼¦å‡½æ•°å¹³æ»‘ä¸‹é™",
            "ReduceLROnPlateauï¼šè‡ªé€‚åº”è¡°å‡ï¼Œå½“æŸå¤±ä¸å†ä¸‹é™æ—¶é™ä½å­¦ä¹ ç‡",
            "Warmupï¼šè®­ç»ƒåˆæœŸé€æ­¥å¢åŠ å­¦ä¹ ç‡ï¼Œé¿å…è®­ç»ƒä¸ç¨³å®š",
            "OneCycleLRï¼šå•å‘¨æœŸç­–ç•¥ï¼Œå…ˆå¢åå‡çš„å­¦ä¹ ç‡æ›²çº¿",
            "PolynomialLRï¼šå¤šé¡¹å¼è¡°å‡ï¼ŒæŒ‰å¤šé¡¹å¼å‡½æ•°è¡°å‡",
            "ExponentialLRï¼šæŒ‡æ•°è¡°å‡ï¼Œå­¦ä¹ ç‡æŒ‰æŒ‡æ•°å‡½æ•°è¡°å‡"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "å­¦ä¹ ç‡è¡°å‡ã€Warmupç­–ç•¥ã€ä½™å¼¦é€€ç«ã€è‡ªé€‚åº”è°ƒåº¦ã€å­¦ä¹ ç‡æŸ¥æ‰¾ã€å­¦ä¹ ç‡é¢„çƒ­"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "æ·±åº¦å­¦ä¹ è®­ç»ƒï¼ˆCosineAnnealingLRï¼‰ã€Transformerè®­ç»ƒï¼ˆWarmup + CosineAnnealingï¼‰ã€å›¾åƒåˆ†ç±»ï¼ˆStepLRï¼‰ã€å¤§æ¨¡å‹è®­ç»ƒï¼ˆWarmup + å¤šé¡¹å¼è¡°å‡ï¼‰ã€å¿«é€Ÿè®­ç»ƒï¼ˆOneCycleLRï¼‰"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥å¯¹æ¯”",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "å­¦ä¹ ç‡è°ƒåº¦"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "StepLRï¼ˆå›ºå®šæ­¥é•¿è¡°å‡ï¼‰",
          "formulas": [
            {
              "text": "æ¯éš” $\\gamma$ ä¸ªepochï¼Œå­¦ä¹ ç‡ä¹˜ä»¥è¡°å‡å› å­ï¼š"
            },
            {
              "display": "\\eta_t = \\begin{cases} \\eta_0 & \\text{if } t < \\gamma \\\\ \\eta_0 \\times \\alpha^{\\lfloor t/\\gamma \\rfloor} & \\text{otherwise} \\end{cases}"
            },
            {
              "text": "å…¶ä¸­ $\\eta_0$ æ˜¯åˆå§‹å­¦ä¹ ç‡ï¼Œ$\\alpha$ æ˜¯è¡°å‡å› å­ï¼Œé€šå¸¸å–0.1"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "CosineAnnealingLRï¼ˆä½™å¼¦é€€ç«ï¼‰",
          "formulas": [
            {
              "text": "å­¦ä¹ ç‡æŒ‰ä½™å¼¦å‡½æ•°ä»æœ€å¤§å€¼è¡°å‡åˆ°æœ€å°å€¼ï¼š"
            },
            {
              "display": "\\eta_t = \\eta_{\\min} + (\\eta_{\\max} - \\eta_{\\min}) \\times \\frac{1 + \\cos(\\pi t / T)}{2}"
            },
            {
              "text": "å…¶ä¸­ $T$ æ˜¯æ€»epochæ•°ï¼Œ$\\eta_{\\max}$ å’Œ $\\eta_{\\min}$ æ˜¯æœ€å¤§å’Œæœ€å°å­¦ä¹ ç‡"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Warmupï¼ˆé¢„çƒ­ï¼‰",
          "formulas": [
            {
              "text": "è®­ç»ƒåˆæœŸçº¿æ€§å¢åŠ å­¦ä¹ ç‡ï¼š"
            },
            {
              "display": "\\eta_t = \\begin{cases} \\eta_{\\max} \\times \\frac{t}{T_{warmup}} & \\text{if } t < T_{warmup} \\\\ \\eta_{\\max} & \\text{otherwise} \\end{cases}"
            },
            {
              "text": "å…¶ä¸­ $T_{warmup}$ æ˜¯é¢„çƒ­æ­¥æ•°ï¼Œé€šå¸¸ä¸ºæ€»æ­¥æ•°çš„5-10%"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "OneCycleLRï¼ˆå•å‘¨æœŸç­–ç•¥ï¼‰",
          "formulas": [
            {
              "text": "å­¦ä¹ ç‡å…ˆå¢åå‡ï¼Œå½¢æˆå•å‘¨æœŸæ›²çº¿ï¼š"
            },
            {
              "display": "\\eta_t = \\begin{cases} \\eta_{\\min} + (\\eta_{\\max} - \\eta_{\\min}) \\times \\frac{t}{T_{rise}} & \\text{if } t < T_{rise} \\\\ \\eta_{\\max} - (\\eta_{\\max} - \\eta_{\\min}) \\times \\frac{t - T_{rise}}{T - T_{rise}} & \\text{otherwise} \\end{cases}"
            },
            {
              "text": "å…¶ä¸­ $T_{rise}$ æ˜¯ä¸Šå‡é˜¶æ®µæ­¥æ•°ï¼Œé€šå¸¸ä¸ºæ€»æ­¥æ•°çš„30-40%"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "å­¦ä¹ ç‡è°ƒåº¦å™¨å®ç°ä¸ä½¿ç”¨",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# å®šä¹‰æ¨¡å‹\nmodel = nn.Sequential(\n    nn.Linear(10, 50),\n    nn.ReLU(),\n    nn.Linear(50, 1)\n)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 1. StepLRï¼šå›ºå®šæ­¥é•¿è¡°å‡\nscheduler_step = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n\n# 2. CosineAnnealingLRï¼šä½™å¼¦é€€ç«\nscheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n\n# 3. ReduceLROnPlateauï¼šè‡ªé€‚åº”è¡°å‡\nscheduler_plateau = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=10, verbose=True\n)\n\n# 4. ExponentialLRï¼šæŒ‡æ•°è¡°å‡\nscheduler_exp = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n\n# 5. PolynomialLRï¼šå¤šé¡¹å¼è¡°å‡\nscheduler_poly = optim.lr_scheduler.PolynomialLR(optimizer, total_iters=100, power=0.9)\n\n# 6. OneCycleLRï¼šå•å‘¨æœŸç­–ç•¥\nscheduler_onecycle = optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=0.01, total_steps=100, pct_start=0.3\n)\n\n# 7. è‡ªå®šä¹‰Warmupè°ƒåº¦å™¨\nclass WarmupScheduler:\n    def __init__(self, optimizer, warmup_steps, base_lr):\n        self.optimizer = optimizer\n        self.warmup_steps = warmup_steps\n        self.base_lr = base_lr\n        self.step_count = 0\n    \n    def step(self):\n        self.step_count += 1\n        if self.step_count <= self.warmup_steps:\n            lr = self.base_lr * (self.step_count / self.warmup_steps)\n        else:\n            lr = self.base_lr\n        \n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n        \n        return lr\n\n# 8. Warmup + CosineAnnealingç»„åˆ\nclass WarmupCosineScheduler:\n    def __init__(self, optimizer, warmup_steps, total_steps, max_lr, min_lr=1e-6):\n        self.optimizer = optimizer\n        self.warmup_steps = warmup_steps\n        self.total_steps = total_steps\n        self.max_lr = max_lr\n        self.min_lr = min_lr\n        self.step_count = 0\n    \n    def step(self):\n        self.step_count += 1\n        \n        if self.step_count <= self.warmup_steps:\n            # Warmupé˜¶æ®µï¼šçº¿æ€§å¢åŠ \n            lr = self.max_lr * (self.step_count / self.warmup_steps)\n        else:\n            # CosineAnnealingé˜¶æ®µ\n            progress = (self.step_count - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n            lr = self.min_lr + (self.max_lr - self.min_lr) * 0.5 * (1 + np.cos(np.pi * progress))\n        \n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n        \n        return lr\n\n# 9. å¯è§†åŒ–ä¸åŒè°ƒåº¦ç­–ç•¥\nschedulers = {\n    'StepLR': optim.lr_scheduler.StepLR(optim.Adam(model.parameters(), lr=0.001), step_size=30, gamma=0.1),\n    'CosineAnnealing': optim.lr_scheduler.CosineAnnealingLR(optim.Adam(model.parameters(), lr=0.001), T_max=100, eta_min=1e-6),\n    'Exponential': optim.lr_scheduler.ExponentialLR(optim.Adam(model.parameters(), lr=0.001), gamma=0.95),\n    'OneCycle': optim.lr_scheduler.OneCycleLR(optim.Adam(model.parameters(), lr=0.001), max_lr=0.01, total_steps=100, pct_start=0.3)\n}\n\n# è®°å½•å­¦ä¹ ç‡å˜åŒ–\nlr_history = {name: [] for name in schedulers.keys()}\n\nfor name, scheduler in schedulers.items():\n    # é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    scheduler.optimizer = optimizer\n    \n    for epoch in range(100):\n        lr = optimizer.param_groups[0]['lr']\n        lr_history[name].append(lr)\n        scheduler.step()\n\n# ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿\nplt.figure(figsize=(14, 8))\nfor name, lrs in lr_history.items():\n    plt.plot(lrs, label=name, linewidth=2)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Learning Rate', fontsize=12)\nplt.title('å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥å¯¹æ¯”', fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.yscale('log')\nplt.tight_layout()\nplt.savefig('lr_scheduler_comparison.png', dpi=300)\nprint(\"å­¦ä¹ ç‡è°ƒåº¦å¯¹æ¯”å›¾å·²ä¿å­˜\")\n\n# 10. å®é™…è®­ç»ƒç¤ºä¾‹\ncriterion = nn.MSELoss()\nx_train = torch.randn(100, 10)\ny_train = torch.randn(100, 1)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# ä½¿ç”¨Warmup + CosineAnnealing\nwarmup_steps = 10\ntotal_steps = 100\nscheduler = WarmupCosineScheduler(optimizer, warmup_steps, total_steps, max_lr=0.001, min_lr=1e-6)\n\nlosses = []\nlrs = []\n\nfor epoch in range(total_steps):\n    # å‰å‘ä¼ æ’­\n    output = model(x_train)\n    loss = criterion(output, y_train)\n    \n    # åå‘ä¼ æ’­\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    # æ›´æ–°å­¦ä¹ ç‡\n    current_lr = scheduler.step()\n    \n    losses.append(loss.item())\n    lrs.append(current_lr)\n    \n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, LR: {current_lr:.6f}\")\n\n# 11. å­¦ä¹ ç‡æŸ¥æ‰¾ï¼ˆLearning Rate Finderï¼‰\ndef find_learning_rate(model, train_loader, criterion, min_lr=1e-8, max_lr=1, num_iter=100):\n    \"\"\"æŸ¥æ‰¾æœ€ä¼˜å­¦ä¹ ç‡\"\"\"\n    lrs = np.logspace(np.log10(min_lr), np.log10(max_lr), num_iter)\n    losses = []\n    \n    for lr in lrs:\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        \n        # ä¸€ä¸ªbatchçš„è®­ç»ƒ\n        for batch_idx, (data, target) in enumerate(train_loader):\n            output = model(data)\n            loss = criterion(output, target)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            losses.append(loss.item())\n            break  # åªç”¨ä¸€ä¸ªbatch\n    \n    # æ‰¾åˆ°æŸå¤±ä¸‹é™æœ€å¿«çš„å­¦ä¹ ç‡\n    best_lr_idx = np.argmin(np.gradient(losses))\n    best_lr = lrs[best_lr_idx]\n    \n    return lrs, losses, best_lr\n\nprint(\"\\n=== å­¦ä¹ ç‡è°ƒåº¦å»ºè®® ===\")\nprint(\"1. å°è§„æ¨¡æ¨¡å‹ï¼šStepLR æˆ– ExponentialLR\")\nprint(\"2. å¤§è§„æ¨¡æ¨¡å‹ï¼šCosineAnnealingLR\")\nprint(\"3. Transformerï¼šWarmup + CosineAnnealingLR\")\nprint(\"4. å¿«é€Ÿè®­ç»ƒï¼šOneCycleLR\")\nprint(\"5. ä¸ç¨³å®šè®­ç»ƒï¼šWarmup + å›ºå®šå­¦ä¹ ç‡\")"
        }
      ]
    }
  ]
}
