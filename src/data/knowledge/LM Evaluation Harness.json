{
  "title": "LM Evaluation Harness",
  "subtitle": "è¦†ç›–ä¸»æµåŸºå‡†çš„ç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒå¿«é€Ÿè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "LM Evaluation Harnessæ¶æ„",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "LM Evaluation Harnessæ¶æ„",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "è¯„ä¼°æµç¨‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "flow",
                "title": "è¯„ä¼°æµç¨‹",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "åŸºæœ¬ä½¿ç”¨",
          "language": "python",
          "code": "from lm_eval import evaluator\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# åŠ è½½æ¨¡å‹\nmodel_name = \"gpt2\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# å®šä¹‰è¯„ä¼°ä»»åŠ¡\ntasks = [\"hellaswag\", \"mmlu\", \"truthfulqa\"]\n\n# è¿è¡Œè¯„ä¼°\nresults = evaluator.simple_evaluate(\n    model=model,\n    model_args=f\"pretrained={model_name}\",\n    tasks=tasks,\n    batch_size=4,\n    device=\"cuda\"\n)\n\n# æŸ¥çœ‹ç»“æœ\nprint(f\"HellaSwag Accuracy: {results['results']['hellaswag']['acc']:.4f}\")\nprint(f\"MMLU Accuracy: {results['results']['mmlu']['acc']:.4f}\")\nprint(f\"TruthfulQA Accuracy: {results['results']['truthfulqa']['acc']:.4f}\")"
        },
        {
          "type": "code-box",
          "title": "è‡ªå®šä¹‰è¯„ä¼°é…ç½®",
          "language": "python",
          "code": "from lm_eval import evaluator\nfrom lm_eval.tasks import TaskManager\n\n# åˆ›å»ºä»»åŠ¡ç®¡ç†å™¨\ntask_manager = TaskManager()\n\n# é…ç½®è¯„ä¼°å‚æ•°\nconfig = {\n    \"model\": \"gpt2\",\n    \"model_args\": \"pretrained=gpt2\",\n    \"batch_size\": 8,\n    \"device\": \"cuda\",\n    \"num_fewshot\": 0,  # few-shotç¤ºä¾‹æ•°é‡\n    \"limit\": 100,      # é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°\n    \"output_path\": \"./eval_results.json\"\n}\n\n# è¿è¡Œè¯„ä¼°\nresults = evaluator.simple_evaluate(\n    model=config[\"model\"],\n    model_args=config[\"model_args\"],\n    tasks=[\"hellaswag\", \"mmlu\"],\n    batch_size=config[\"batch_size\"],\n    device=config[\"device\"],\n    num_fewshot=config[\"num_fewshot\"],\n    limit=config[\"limit\"]\n)\n\n# ä¿å­˜ç»“æœ\nimport json\nwith open(config[\"output_path\"], \"w\") as f:\n    json.dump(results, f, indent=2)\n\nprint(\"è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜\")"
        }
      ]
    }
  ]
}
