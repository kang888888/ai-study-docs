{
  "title": "ä½ç½®ç¼–ç  (Positional Encoding)",
  "subtitle": "ä¸ºåºåˆ—æ•°æ®æ³¨å…¥ä½ç½®ä¿¡æ¯",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "ä½ç½®ç¼–ç æ˜¯Transformeræ¶æ„ä¸­çš„å…³é”®ç»„ä»¶ï¼Œç”¨äºä¸ºåºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®æ·»åŠ ä½ç½®ä¿¡æ¯ã€‚ç”±äºTransformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯ç½®æ¢ä¸å˜çš„ï¼Œéœ€è¦æ˜¾å¼åœ°æ³¨å…¥ä½ç½®ä¿¡æ¯æ¥åŒºåˆ†ä¸åŒä½ç½®çš„tokenã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ ä¸»è¦ç±»å‹",
      "content": [
        {
          "type": "features",
          "items": [
            "ç»å¯¹ä½ç½®ç¼–ç ï¼šä¸ºæ¯ä¸ªä½ç½®åˆ†é…å›ºå®šçš„ç¼–ç ï¼Œå¦‚æ­£å¼¦ä½ç½®ç¼–ç ",
            "ç›¸å¯¹ä½ç½®ç¼–ç ï¼šç¼–ç tokenä¹‹é—´çš„ç›¸å¯¹è·ç¦»å…³ç³»",
            "å­¦ä¹ ä½ç½®ç¼–ç ï¼šå¯è®­ç»ƒçš„ä½ç½®åµŒå…¥ï¼Œé€šè¿‡åå‘ä¼ æ’­å­¦ä¹ ",
            "æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼šé€šè¿‡æ—‹è½¬çŸ©é˜µç¼–ç ç›¸å¯¹ä½ç½®ï¼Œå¹¿æ³›åº”ç”¨äºç°ä»£LLM",
            "ALiBiï¼šçº¿æ€§åç½®æ³¨æ„åŠ›ï¼Œæ— éœ€æ˜¾å¼ä½ç½®ç¼–ç "
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "æ­£å¼¦ä½ç½®ç¼–ç ã€å­¦ä¹ ä½ç½®åµŒå…¥ã€ç›¸å¯¹ä½ç½®ç¼–ç ã€RoPEã€ALiBi"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "Transformeræ¨¡å‹ã€BERTã€GPTã€LLaMAã€æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ã€åºåˆ—å»ºæ¨¡"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "ä½ç½®ç¼–ç å¯¹æ¯”",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "ä½ç½®ç¼–ç å¯¹æ¯”"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "æ­£å¼¦ä½ç½®ç¼–ç ï¼ˆSinusoidalï¼‰",
          "formulas": [
            {
              "text": "åŸå§‹Transformerä½¿ç”¨çš„æ­£å¼¦ä½ç½®ç¼–ç ï¼š"
            },
            {
              "display": "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)"
            },
            {
              "display": "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)"
            },
            {
              "text": "å…¶ä¸­ $pos$ æ˜¯ä½ç½®ï¼Œ$i$ æ˜¯ç»´åº¦ç´¢å¼•ï¼Œ$d_{model}$ æ˜¯æ¨¡å‹ç»´åº¦",
              "inline": "pos, i, d_{model}"
            },
            {
              "text": "ç¼–ç åçš„å‘é‡ä¸è¯åµŒå…¥ç›¸åŠ ï¼š"
            },
            {
              "display": "x_i = \\text{Embedding}(token_i) + PE_i"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰",
          "formulas": [
            {
              "text": "RoPEé€šè¿‡æ—‹è½¬çŸ©é˜µç¼–ç ç›¸å¯¹ä½ç½®ï¼š"
            },
            {
              "display": "R_{\\theta} = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix}"
            },
            {
              "text": "å¯¹äºä½ç½® $m$ çš„æŸ¥è¯¢å’Œä½ç½® $n$ çš„é”®ï¼Œæ—‹è½¬è§’åº¦ä¸ºï¼š"
            },
            {
              "display": "\\theta_i = 10000^{-2i/d}"
            },
            {
              "text": "æ³¨æ„åŠ›åˆ†æ•°è‡ªç„¶åŒ…å«ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œæ”¯æŒå¤–æ¨åˆ°æ›´é•¿åºåˆ—"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "å­¦ä¹ ä½ç½®ç¼–ç ",
          "formulas": [
            {
              "text": "å¯è®­ç»ƒçš„ä½ç½®åµŒå…¥çŸ©é˜µï¼š"
            },
            {
              "display": "PE = \\text{Embedding}(max\\_len, d_{model})"
            },
            {
              "text": "é€šè¿‡åå‘ä¼ æ’­å­¦ä¹ æœ€ä¼˜ä½ç½®è¡¨ç¤ºï¼Œä½†æ— æ³•å¤–æ¨åˆ°è®­ç»ƒæ—¶æœªè§è¿‡çš„é•¿åº¦"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "PyTorch å®ç°ä½ç½®ç¼–ç ",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport math\n\n# 1. æ­£å¼¦ä½ç½®ç¼–ç ï¼ˆåŸå§‹Transformerï¼‰\nclass SinusoidalPositionalEncoding(nn.Module):\n    \"\"\"æ­£å¼¦ä½ç½®ç¼–ç \"\"\"\n    def __init__(self, d_model, max_len=5000, dropout=0.1):\n        super(SinusoidalPositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                            (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ç»´åº¦ç”¨sin\n        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ç»´åº¦ç”¨cos\n        \n        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n        self.register_buffer('pe', pe)  # ä¸å‚ä¸è®­ç»ƒ\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: [batch_size, seq_len, d_model]\n        Returns:\n            x + positional_encoding: [batch_size, seq_len, d_model]\n        \"\"\"\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)\n\n# 2. å­¦ä¹ ä½ç½®ç¼–ç ï¼ˆå¯è®­ç»ƒï¼‰\nclass LearnedPositionalEncoding(nn.Module):\n    \"\"\"å¯è®­ç»ƒçš„ä½ç½®åµŒå…¥\"\"\"\n    def __init__(self, d_model, max_len=5000, dropout=0.1):\n        super(LearnedPositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.pe = nn.Embedding(max_len, d_model)\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: [batch_size, seq_len, d_model]\n        \"\"\"\n        seq_len = x.size(1)\n        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n        pos_emb = self.pe(positions)  # [1, seq_len, d_model]\n        x = x + pos_emb\n        return self.dropout(x)\n\n# 3. æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ç®€åŒ–ç‰ˆ\nclass RotaryPositionalEncoding(nn.Module):\n    \"\"\"æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰\"\"\"\n    def __init__(self, d_model, max_len=5000):\n        super(RotaryPositionalEncoding, self).__init__()\n        \n        # è®¡ç®—é¢‘ç‡\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))\n        self.register_buffer('inv_freq', inv_freq)\n    \n    def forward(self, x, seq_len=None):\n        \"\"\"\n        Args:\n            x: [batch_size, seq_len, d_model] æˆ– [batch_size, nhead, seq_len, head_dim]\n        \"\"\"\n        if x.dim() == 3:\n            batch_size, seq_len, d_model = x.shape\n            # é‡å¡‘ä¸º [batch_size, seq_len, d_model//2, 2]\n            x_complex = x.view(batch_size, seq_len, d_model // 2, 2)\n        else:\n            batch_size, nhead, seq_len, head_dim = x.shape\n            x_complex = x.view(batch_size, nhead, seq_len, head_dim // 2, 2)\n        \n        # ç”Ÿæˆä½ç½®è§’åº¦\n        t = torch.arange(seq_len, device=x.device, dtype=torch.float32)\n        freqs = torch.outer(t, self.inv_freq)  # [seq_len, d_model//2]\n        \n        # åˆ›å»ºæ—‹è½¬çŸ©é˜µ\n        cos = torch.cos(freqs).unsqueeze(-1)  # [seq_len, d_model//2, 1]\n        sin = torch.sin(freqs).unsqueeze(-1)  # [seq_len, d_model//2, 1]\n        \n        # åº”ç”¨æ—‹è½¬ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…å®ç°æ›´å¤æ‚ï¼‰\n        # è¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…RoPEéœ€è¦æ›´ç²¾ç»†çš„å®ç°\n        return x\n\n# 4. ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    d_model = 512\n    seq_len = 100\n    batch_size = 32\n    \n    # è¯åµŒå…¥\n    vocab_size = 10000\n    embedding = nn.Embedding(vocab_size, d_model)\n    \n    # è¾“å…¥token ids\n    tokens = torch.randint(0, vocab_size, (batch_size, seq_len))\n    x = embedding(tokens)  # [batch_size, seq_len, d_model]\n    \n    # æ­£å¼¦ä½ç½®ç¼–ç \n    pos_encoder1 = SinusoidalPositionalEncoding(d_model, max_len=5000)\n    x1 = pos_encoder1(x)\n    print(f\"æ­£å¼¦ä½ç½®ç¼–ç è¾“å‡ºå½¢çŠ¶: {x1.shape}\")\n    \n    # å­¦ä¹ ä½ç½®ç¼–ç \n    pos_encoder2 = LearnedPositionalEncoding(d_model, max_len=5000)\n    x2 = pos_encoder2(x)\n    print(f\"å­¦ä¹ ä½ç½®ç¼–ç è¾“å‡ºå½¢çŠ¶: {x2.shape}\")\n    \n    # å®Œæ•´çš„Transformerç¼–ç å™¨ç¤ºä¾‹\n    class TransformerEncoder(nn.Module):\n        def __init__(self, vocab_size, d_model, nhead, num_layers, max_len=5000):\n            super(TransformerEncoder, self).__init__()\n            self.embedding = nn.Embedding(vocab_size, d_model)\n            self.pos_encoder = SinusoidalPositionalEncoding(d_model, max_len)\n            encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, batch_first=True)\n            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n        \n        def forward(self, x):\n            x = self.embedding(x)\n            x = self.pos_encoder(x)\n            x = self.transformer(x)\n            return x\n    \n    model = TransformerEncoder(vocab_size, d_model, nhead=8, num_layers=6)\n    output = model(tokens)\n    print(f\"Transformerç¼–ç å™¨è¾“å‡ºå½¢çŠ¶: {output.shape}\")"
        }
      ]
    }
  ]
}