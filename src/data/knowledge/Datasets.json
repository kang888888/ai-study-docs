{
  "title": "HuggingFace Datasets",
  "subtitle": "é«˜æ•ˆçš„æ•°æ®åŠ è½½ã€å¤„ç†ã€åˆ›å»ºä¸åˆ‡åˆ†åº“",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "HuggingFace Datasets æ˜¯ä¸€ä¸ªç”¨äºé«˜æ•ˆåŠ è½½ã€å¤„ç†å’Œå…±äº«æ•°æ®é›†çš„Pythonåº“ã€‚å®ƒæä¾›äº†ç»Ÿä¸€çš„æ•°æ®æ¥å£ï¼Œæ”¯æŒå¤šç§æ•°æ®æ ¼å¼ï¼Œå†…ç½®ç¼“å­˜æœºåˆ¶ï¼Œå¯ä»¥å¤§å¹…æå‡æ•°æ®åŠ è½½å’Œå¤„ç†æ•ˆç‡ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "ç»Ÿä¸€æ¥å£ï¼šæ”¯æŒå¤šç§æ•°æ®æ ¼å¼ï¼ˆCSVã€JSONã€Parquetã€Arrowç­‰ï¼‰",
            "é«˜æ•ˆåŠ è½½ï¼šåŸºäºApache Arrowï¼Œå†…å­˜å ç”¨å°ï¼ŒåŠ è½½é€Ÿåº¦å¿«",
            "ç¼“å­˜æœºåˆ¶ï¼šè‡ªåŠ¨ç¼“å­˜å¤„ç†ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—",
            "æµå¼å¤„ç†ï¼šæ”¯æŒå¤§æ•°æ®é›†çš„æµå¼åŠ è½½ï¼Œæ— éœ€å…¨éƒ¨åŠ è½½åˆ°å†…å­˜",
            "æ•°æ®åˆ‡åˆ†ï¼šå†…ç½®train/val/teståˆ‡åˆ†åŠŸèƒ½",
            "ç‰ˆæœ¬ç®¡ç†ï¼šæ”¯æŒæ•°æ®é›†ç‰ˆæœ¬åŒ–å’Œç‰ˆæœ¬å›é€€",
            "ç¤¾åŒºå…±äº«ï¼šå¯ä»¥è½»æ¾ä¸Šä¼ å’Œåˆ†äº«æ•°æ®é›†åˆ°HuggingFace Hub"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "Apache Arrowã€å†…å­˜æ˜ å°„ã€æµå¼å¤„ç†ã€æ•°æ®ç¼“å­˜ã€å¤šè¿›ç¨‹åŠ è½½ã€æ•°æ®è½¬æ¢ç®¡é“"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "å¤§è§„æ¨¡æ•°æ®é›†åŠ è½½ï¼šå¤„ç†GBåˆ°TBçº§åˆ«çš„æ•°æ®é›†\n                    æ•°æ®é¢„å¤„ç†ï¼šæ¸…æ´—ã€è½¬æ¢ã€å¢å¼ºç­‰æ“ä½œ\n                    æ¨¡å‹è®­ç»ƒï¼šä¸ºè®­ç»ƒæµç¨‹æä¾›é«˜æ•ˆçš„æ•°æ®åŠ è½½\n                    æ•°æ®æ¢ç´¢ï¼šå¿«é€ŸæŸ¥çœ‹å’Œåˆ†ææ•°æ®é›†\n                    æ•°æ®å…±äº«ï¼šåœ¨HuggingFace Hubä¸Šåˆ†äº«æ•°æ®é›†"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "Datasetsæ¶æ„",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Datasetsæ¶æ„",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "Datasetsæµç¨‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Datasetsæµç¨‹",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "åŠ è½½æ•°æ®é›†",
          "language": "python",
          "code": "from datasets import load_dataset\n\n# åŠ è½½HuggingFace Hubä¸Šçš„æ•°æ®é›†\ndataset = load_dataset(\"imdb\")\nprint(dataset)\n# DatasetDict({\n#     train: Dataset({\n#         features: ['text', 'label'],\n#         num_rows: 25000\n#     })\n#     test: Dataset({\n#         features: ['text', 'label'],\n#         num_rows: 25000\n#     })\n# })\n\n# åŠ è½½æœ¬åœ°æ•°æ®é›†\nlocal_dataset = load_dataset(\"json\", data_files=\"data.json\")\n\n# åŠ è½½CSVæ–‡ä»¶\ncsv_dataset = load_dataset(\"csv\", data_files=\"data.csv\")"
        },
        {
          "type": "code-box",
          "title": "æ•°æ®å¤„ç†",
          "language": "python",
          "code": "from datasets import load_dataset\n\n# åŠ è½½æ•°æ®é›†\ndataset = load_dataset(\"squad\")\n\n# æ•°æ®è¿‡æ»¤\ndefiltered = dataset.filter(lambda x: len(x[\"question\"]) > 10)\n\n# æ•°æ®æ˜ å°„ï¼ˆè½¬æ¢ï¼‰\ndef add_length(example):\n    example[\"question_length\"] = len(example[\"question\"])\n    return example\n\nmapped = dataset.map(add_length)\n\n# æ•°æ®é‡å‘½å\nrenamed = dataset.rename_column(\"question\", \"query\")\n\n# æ•°æ®é€‰æ‹©åˆ—\nselected = dataset.select_columns([\"question\", \"context\"])"
        },
        {
          "type": "code-box",
          "title": "æ•°æ®åˆ‡åˆ†",
          "language": "python",
          "code": "from datasets import load_dataset\n\n# åŠ è½½æ•°æ®é›†\ndataset = load_dataset(\"imdb\", split=\"train\")\n\n# åˆ‡åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†\nsplit_dataset = dataset.train_test_split(test_size=0.2, seed=42)\nprint(split_dataset)\n# DatasetDict({\n#     train: Dataset(...),\n#     test: Dataset(...)\n# })\n\n# å¤šæŠ˜äº¤å‰éªŒè¯\nfrom datasets import Dataset\n\nk_folds = dataset.train_test_split(test_size=0.2, seed=42)\n# å¯ä»¥è¿›ä¸€æ­¥åˆ‡åˆ†ä¸ºå¤šæŠ˜"
        },
        {
          "type": "code-box",
          "title": "æµå¼å¤„ç†å¤§æ•°æ®é›†",
          "language": "python",
          "code": "from datasets import load_dataset\n\n# æµå¼åŠ è½½å¤§æ•°æ®é›†ï¼ˆä¸å…¨éƒ¨åŠ è½½åˆ°å†…å­˜ï¼‰\ndataset = load_dataset(\"c4\", \"en\", streaming=True, split=\"train\")\n\n# è¿­ä»£å¤„ç†\nfor example in dataset:\n    # å¤„ç†æ¯ä¸ªæ ·æœ¬\n    print(example[\"text\"][:100])\n    break  # åªå¤„ç†ç¬¬ä¸€ä¸ªæ ·æœ¬\n\n# æµå¼è¿‡æ»¤å’Œæ˜ å°„\nfiltered = dataset.filter(lambda x: len(x[\"text\"]) > 1000)\nmapped = filtered.map(lambda x: {\"length\": len(x[\"text\"])})"
        },
        {
          "type": "code-box",
          "title": "åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†",
          "language": "python",
          "code": "from datasets import Dataset\nimport pandas as pd\n\n# ä»å­—å…¸åˆ›å»º\ndata = {\n    \"text\": [\"Hello\", \"World\", \"AI\"],\n    \"label\": [0, 1, 0]\n}\ndataset = Dataset.from_dict(data)\n\n# ä»Pandas DataFrameåˆ›å»º\ndf = pd.DataFrame({\"text\": [\"Hello\"], \"label\": [0]})\ndataset = Dataset.from_pandas(df)\n\n# ä»ç”Ÿæˆå™¨åˆ›å»º\ndef data_generator():\n    for i in range(100):\n        yield {\"id\": i, \"text\": f\"Sample {i}\"}\n\ndataset = Dataset.from_generator(data_generator)\n\n# ä¿å­˜æ•°æ®é›†\ndataset.save_to_disk(\"./my_dataset\")\n\n# åŠ è½½ä¿å­˜çš„æ•°æ®é›†\nloaded = Dataset.load_from_disk(\"./my_dataset\")"
        }
      ]
    }
  ]
}