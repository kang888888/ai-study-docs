{
  "title": "MLP (Multilayer Perceptron) å¤šå±‚æ„ŸçŸ¥æœº",
  "subtitle": "æœ€åŸºç¡€çš„å‰é¦ˆç¥ç»ç½‘ç»œ",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "æœ€åŸºç¡€çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œç”±è¾“å…¥å±‚ã€å¤šä¸ªéšè—å±‚å’Œè¾“å‡ºå±‚ç»„æˆã€‚å±‚ä¸å±‚ä¹‹é—´å…¨è¿æ¥ï¼ˆFully Connectedï¼‰ï¼Œæ¯ä¸ªç¥ç»å…ƒä¸ä¸‹ä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒç›¸è¿ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "ç»“æ„ç®€å•ï¼šæ˜“äºç†è§£å’Œå®ç°ï¼Œæ˜¯æ·±åº¦å­¦ä¹ å…¥é—¨çš„ç¬¬ä¸€æ­¥",
            "å…¨è¿æ¥ï¼šæ¯å±‚ç¥ç»å…ƒä¸ä¸‹ä¸€å±‚æ‰€æœ‰ç¥ç»å…ƒè¿æ¥",
            "éçº¿æ€§æ¿€æ´»ï¼šé€šè¿‡æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ReLUã€Sigmoidï¼‰å¼•å…¥éçº¿æ€§",
            "å‚æ•°é‡å¤§ï¼šå¯¹äºé«˜ç»´è¾“å…¥ï¼ˆå¦‚å›¾åƒï¼‰ï¼Œå‚æ•°é‡ä¼šçˆ†ç‚¸å¼å¢é•¿",
            "æ— ç©ºé—´ç»“æ„ï¼šä¸è€ƒè™‘è¾“å…¥æ•°æ®çš„ç©ºé—´å…³ç³»ï¼ˆå¦‚å›¾åƒçš„åƒç´ é‚»è¿‘æ€§ï¼‰"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "åå‘ä¼ æ’­ç®—æ³•ï¼ˆBackpropagationï¼‰ã€æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ã€æ¿€æ´»å‡½æ•°ï¼ˆReLU/Sigmoid/Tanhï¼‰"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "åˆ†ç±»ä»»åŠ¡ã€å›å½’é¢„æµ‹ã€ç‰¹å¾å­¦ä¹ ã€ç®€å•çš„è¡¨æ ¼æ•°æ®å¤„ç†"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "MLPDiagram",
              "caption": "MLPæ¶æ„å›¾",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "MLPæ¶æ„å›¾"
              }
            },
            {
              "type": "svg-d3",
              "component": "MLPDiagram",
              "caption": "MLPå‰å‘ä¼ æ’­",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "MLPå‰å‘ä¼ æ’­"
              }
            },
            {
              "type": "svg-d3",
              "component": "MLPDiagram",
              "caption": "MLPåå‘ä¼ æ’­",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "MLPåå‘ä¼ æ’­"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "å‰å‘ä¼ æ’­",
          "formulas": [
            {
              "text": "å¯¹äºç¬¬ $l$ å±‚ï¼Œå‰å‘ä¼ æ’­å…¬å¼ä¸ºï¼š",
              "inline": "l"
            },
            {
              "display": "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}"
            },
            {
              "display": "a^{(l)} = \\sigma(z^{(l)})"
            },
            {
              "text": "å…¶ä¸­ï¼š"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "åå‘ä¼ æ’­",
          "formulas": [
            {
              "text": "è¾“å‡ºå±‚è¯¯å·®ï¼š"
            },
            {
              "display": "\\delta^{(L)} = \\nabla_a J \\odot \\sigma'(z^{(L)})"
            },
            {
              "text": "éšè—å±‚è¯¯å·®ï¼ˆä»åå‘å‰ä¼ æ’­ï¼‰ï¼š"
            },
            {
              "display": "\\delta^{(l)} = ((W^{(l+1)})^T \\delta^{(l+1)}) \\odot \\sigma'(z^{(l)})"
            },
            {
              "text": "æ¢¯åº¦è®¡ç®—ï¼š"
            },
            {
              "display": "\\frac{\\partial J}{\\partial W^{(l)}} = \\delta^{(l)} (a^{(l-1)})^T"
            },
            {
              "display": "\\frac{\\partial J}{\\partial b^{(l)}} = \\delta^{(l)}"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "æ¿€æ´»å‡½æ•°",
          "formulas": [
            {
              "text": "ReLU: $f(x) = \\max(0, x)$",
              "inline": "f(x) = \\max(0, x)"
            },
            {
              "text": "Sigmoid: $f(x) = \\frac{1}{1 + e^{-x}}$",
              "inline": "f(x) = \\frac{1}{1 + e^{-x}}"
            },
            {
              "text": "Tanh: $f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$",
              "inline": "f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ PyTorch å®ç° MLP",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MLP(nn.Module):\n    \"\"\"å¤šå±‚æ„ŸçŸ¥æœºå®ç°\"\"\"\n    def __init__(self, input_size, hidden_sizes, output_size, activation='relu'):\n        super(MLP, self).__init__()\n        \n        # æ„å»ºå±‚\n        layers = []\n        prev_size = input_size\n        \n        for hidden_size in hidden_sizes:\n            layers.append(nn.Linear(prev_size, hidden_size))\n            if activation == 'relu':\n                layers.append(nn.ReLU())\n            elif activation == 'sigmoid':\n                layers.append(nn.Sigmoid())\n            elif activation == 'tanh':\n                layers.append(nn.Tanh())\n            layers.append(nn.Dropout(0.2))  # é˜²æ­¢è¿‡æ‹Ÿåˆ\n            prev_size = hidden_size\n        \n        # è¾“å‡ºå±‚\n        layers.append(nn.Linear(prev_size, output_size))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.network(x)\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    # åˆ›å»ºæ¨¡å‹ï¼šè¾“å…¥784ç»´ï¼Œä¸¤ä¸ªéšè—å±‚[128, 64]ï¼Œè¾“å‡º10ç±»\n    model = MLP(input_size=784, hidden_sizes=[128, 64], output_size=10)\n    \n    # å‰å‘ä¼ æ’­\n    x = torch.randn(32, 784)  # batch_size=32\n    output = model(x)\n    print(f\"è¾“å‡ºå½¢çŠ¶: {output.shape}\")  # [32, 10]\n    \n    # è®¡ç®—æŸå¤±\n    criterion = nn.CrossEntropyLoss()\n    target = torch.randint(0, 10, (32,))\n    loss = criterion(output, target)\n    print(f\"æŸå¤±å€¼: {loss.item():.4f}\")\n    \n    # åå‘ä¼ æ’­\n    loss.backward()\n    print(\"æ¢¯åº¦å·²è®¡ç®—å®Œæˆ\")"
        },
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ NumPy æ‰‹åŠ¨å®ç°å‰å‘å’Œåå‘ä¼ æ’­",
          "language": "python",
          "code": "import numpy as np\n\nclass MLP_Numpy:\n    \"\"\"ä½¿ç”¨NumPyæ‰‹åŠ¨å®ç°MLP\"\"\"\n    def __init__(self, layer_sizes, learning_rate=0.01):\n        self.layer_sizes = layer_sizes\n        self.learning_rate = learning_rate\n        self.weights = []\n        self.biases = []\n        \n        # åˆå§‹åŒ–æƒé‡å’Œåç½®\n        for i in range(len(layer_sizes) - 1):\n            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.1\n            b = np.zeros((1, layer_sizes[i+1]))\n            self.weights.append(w)\n            self.biases.append(b)\n    \n    def relu(self, x):\n        \"\"\"ReLUæ¿€æ´»å‡½æ•°\"\"\"\n        return np.maximum(0, x)\n    \n    def relu_derivative(self, x):\n        \"\"\"ReLUçš„å¯¼æ•°\"\"\"\n        return (x > 0).astype(float)\n    \n    def sigmoid(self, x):\n        \"\"\"Sigmoidæ¿€æ´»å‡½æ•°\"\"\"\n        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n    \n    def forward(self, X):\n        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n        self.activations = [X]\n        self.z_values = []\n        \n        for i in range(len(self.weights)):\n            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n            self.z_values.append(z)\n            if i < len(self.weights) - 1:  # éšè—å±‚ä½¿ç”¨ReLU\n                a = self.relu(z)\n            else:  # è¾“å‡ºå±‚ä½¿ç”¨Sigmoid\n                a = self.sigmoid(z)\n            self.activations.append(a)\n        \n        return self.activations[-1]\n    \n    def backward(self, X, y, output):\n        \"\"\"åå‘ä¼ æ’­\"\"\"\n        m = X.shape[0]\n        \n        # è¾“å‡ºå±‚è¯¯å·®\n        delta = output - y\n        \n        # ä»åå‘å‰æ›´æ–°æƒé‡å’Œåç½®\n        for i in range(len(self.weights) - 1, -1, -1):\n            # è®¡ç®—æ¢¯åº¦\n            dW = np.dot(self.activations[i].T, delta) / m\n            db = np.sum(delta, axis=0, keepdims=True) / m\n            \n            # æ›´æ–°æƒé‡å’Œåç½®\n            self.weights[i] -= self.learning_rate * dW\n            self.biases[i] -= self.learning_rate * db\n            \n            # è®¡ç®—å‰ä¸€å±‚è¯¯å·®ï¼ˆå¦‚æœä¸æ˜¯ç¬¬ä¸€å±‚ï¼‰\n            if i > 0:\n                delta = np.dot(delta, self.weights[i].T) * self.relu_derivative(self.z_values[i-1])\n    \n    def train(self, X, y, epochs=1000):\n        \"\"\"è®­ç»ƒæ¨¡å‹\"\"\"\n        for epoch in range(epochs):\n            output = self.forward(X)\n            self.backward(X, y, output)\n            \n            if epoch % 100 == 0:\n                loss = np.mean((output - y) ** 2)\n                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    # åˆ›å»ºç®€å•çš„æ•°æ®é›†ï¼ˆXORé—®é¢˜ï¼‰\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    y = np.array([[0], [1], [1], [0]])\n    \n    # åˆ›å»ºæ¨¡å‹ï¼š2è¾“å…¥ -> 4éšè— -> 1è¾“å‡º\n    model = MLP_Numpy([2, 4, 1], learning_rate=0.1)\n    \n    # è®­ç»ƒ\n    model.train(X, y, epochs=1000)\n    \n    # æµ‹è¯•\n    predictions = model.forward(X)\n    print(\"\\né¢„æµ‹ç»“æœ:\")\n    print(predictions)"
        }
      ]
    }
  ]
}