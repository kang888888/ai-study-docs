{
  "title": "ReLU (Rectified Linear Unit)",
  "subtitle": "æœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°",
  "content": [
    {
      "type": "section",
      "title": "ðŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "ReLUï¼ˆRectified Linear Unitï¼Œä¿®æ­£çº¿æ€§å•å…ƒï¼‰æ˜¯æ·±åº¦ç¥žç»ç½‘ç»œä¸­æœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚å®ƒå°†æ‰€æœ‰è´Ÿå€¼ç½®ä¸º0ï¼Œæ­£å€¼ä¿æŒä¸å˜ï¼Œè®¡ç®—ç®€å•ä¸”æœ‰æ•ˆç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ðŸ“ æ•°å­¦è¡¨è¾¾",
      "content": [
        {
          "type": "math-box",
          "title": "ReLUå‡½æ•°å®šä¹‰",
          "formulas": [
            {
              "text": "ReLUå‡½æ•°å®šä¹‰ä¸ºï¼š"
            },
            {
              "display": "\\text{ReLU}(x) = \\max(0, x) = \\begin{cases} x & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}"
            },
            {
              "text": "åˆ†æ®µå‡½æ•°å½¢å¼ï¼š"
            },
            {
              "display": "f(x) = \\begin{cases} x & x > 0 \\\\ 0 & x \\leq 0 \\end{cases}"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "å¯¼æ•°",
          "formulas": [
            {
              "text": "ReLUçš„å¯¼æ•°ä¸ºï¼š"
            },
            {
              "display": "\\frac{d}{dx}\\text{ReLU}(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}"
            },
            {
              "text": "åœ¨ $x=0$ å¤„ä¸å¯å¯¼ï¼ˆä½†å®žé™…åº”ç”¨ä¸­é€šå¸¸å®šä¹‰ä¸º0ï¼‰"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ðŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "è®¡ç®—ç®€å•ï¼šåªéœ€æ¯”è¾ƒå’Œé€‰æ‹©æ“ä½œï¼Œè®¡ç®—é€Ÿåº¦å¿«",
            "éžé¥±å’Œï¼šæ­£å€¼åŒºåŸŸæ¢¯åº¦æ’ä¸º1ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±",
            "ç¨€ç–æ¿€æ´»ï¼šè´Ÿå€¼è¾“å‡ºä¸º0ï¼Œäº§ç”Ÿç¨€ç–è¡¨ç¤º",
            "æ­»ç¥žç»å…ƒé—®é¢˜ï¼šè´Ÿå€¼æ¢¯åº¦ä¸º0ï¼Œå¯èƒ½å¯¼è‡´ç¥žç»å…ƒæ°¸ä¹…å¤±æ´»",
            "éžé›¶ä¸­å¿ƒï¼šè¾“å‡ºå‡å€¼ä¸ä¸º0ï¼Œå¯èƒ½å½±å“æ”¶æ•›"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ðŸ“Š å‡½æ•°å›¾åƒ",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "MathFunctionDiagram",
              "caption": "ReLUå‡½æ•°å›¾åƒ",
              "width": 1000,
              "height": 600,
              "interactive": true,
              "props": {
                "type": "activation",
                "title": "ReLUå‡½æ•°å›¾åƒ",
                "function": "relu"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "åˆ†æ®µçº¿æ€§æ¿€æ´»ã€æ¢¯åº¦æˆªæ–­ã€ç¨€ç–æ¿€æ´»ã€æ­»ç¥žç»å…ƒé—®é¢˜ã€Leaky ReLUå˜ä½“ã€PReLUå¯å­¦ä¹ å‚æ•°ã€RReLUéšæœºæ€§ã€ELUå¹³æ»‘è´Ÿå€¼"
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å˜ä½“",
      "content": [
        {
          "type": "tech-box",
          "content": "Leaky ReLU: f(x) = max(Î±x, x)ï¼Œå…¶ä¸­Î±é€šå¸¸ä¸º0.01ï¼Œè§£å†³æ­»ç¥žç»å…ƒé—®é¢˜\nPReLU: å¯å­¦ä¹ çš„Leaky ReLUï¼ŒÎ±ä½œä¸ºå¯è®­ç»ƒå‚æ•°\nRReLU: éšæœºLeaky ReLUï¼Œè®­ç»ƒæ—¶Î±éšæœºé‡‡æ ·ï¼Œæµ‹è¯•æ—¶å–å¹³å‡\nELU: æŒ‡æ•°çº¿æ€§å•å…ƒï¼Œè´Ÿå€¼éƒ¨åˆ†å¹³æ»‘"
        }
      ]
    },
    {
      "type": "section",
      "title": "ðŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "CNNå·ç§¯å±‚ã€å…¨è¿žæŽ¥å±‚ã€Transformerçš„FFNå±‚ã€å¤§å¤šæ•°æ·±åº¦ç¥žç»ç½‘ç»œçš„éšè—å±‚"
        }
      ]
    },
    {
      "type": "section",
      "title": "ðŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ReLUå®žçŽ°ä¸Žä½¿ç”¨",
          "language": "python",
          "code": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# 1. åŸºç¡€å®žçŽ°\ndef relu(x):\n    \"\"\"ReLUå‡½æ•°å®žçŽ°\"\"\"\n    return np.maximum(0, x)\n\n# æµ‹è¯•\nx = np.array([-2, -1, 0, 1, 2])\nprint(f\"è¾“å…¥: {x}\")\nprint(f\"ReLUè¾“å‡º: {relu(x)}\")\n# è¾“å‡º: [0, 0, 0, 1, 2]\n\n# 2. PyTorchå®žçŽ°\nx_tensor = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\nrelu_output = F.relu(x_tensor)\nprint(f\"PyTorch ReLU: {relu_output}\")\n\n# 3. åœ¨ç¥žç»ç½‘ç»œä¸­ä½¿ç”¨\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.fc1 = nn.Linear(10, 50)\n        self.relu = nn.ReLU()  # ReLUæ¿€æ´»å±‚\n        self.fc2 = nn.Linear(50, 1)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)  # åº”ç”¨ReLU\n        x = self.fc2(x)\n        return x\n\n# 4. å¯è§†åŒ–ReLUå‡½æ•°\nx = np.linspace(-5, 5, 100)\ny = relu(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, linewidth=2, label='ReLU')\nplt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\nplt.xlabel('x')\nplt.ylabel('ReLU(x)')\nplt.title('ReLU Activation Function')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.savefig('relu_function.png')\n\n# 5. å¯¹æ¯”ReLUå’ŒLeaky ReLU\nleaky_relu = lambda x, alpha=0.01: np.where(x > 0, x, alpha * x)\n\nx = np.linspace(-3, 3, 100)\ny_relu = relu(x)\ny_leaky = leaky_relu(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y_relu, linewidth=2, label='ReLU')\nplt.plot(x, y_leaky, linewidth=2, label='Leaky ReLU (Î±=0.01)', linestyle='--')\nplt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('ReLU vs Leaky ReLU')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.savefig('relu_comparison.png')\n\n# 6. æ¢¯åº¦æ£€æŸ¥\nx = torch.tensor([-1.0, 0.0, 1.0], requires_grad=True)\ny = F.relu(x)\ny.sum().backward()\nprint(f\"è¾“å…¥: {x}\")\nprint(f\"è¾“å‡º: {y}\")\nprint(f\"æ¢¯åº¦: {x.grad}\")\n# æ¢¯åº¦: [0, 0, 1] - è´Ÿå€¼æ¢¯åº¦ä¸º0ï¼Œæ­£å€¼æ¢¯åº¦ä¸º1\n\n# 7. å®žé™…åº”ç”¨ï¼šCNNä¸­çš„ReLU\nclass CNNWithReLU(nn.Module):\n    def __init__(self, num_classes=10):\n        super(CNNWithReLU, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n        self.relu1 = nn.ReLU()\n        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n        self.relu2 = nn.ReLU()\n        self.fc = nn.Linear(128 * 8 * 8, num_classes)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu1(x)  # ReLUæ¿€æ´»\n        x = self.conv2(x)\n        x = self.relu2(x)  # ReLUæ¿€æ´»\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# 8. ç»Ÿè®¡ReLUçš„æ¿€æ´»çŽ‡\nmodel = SimpleNet()\nx = torch.randn(100, 10)\nwith torch.no_grad():\n    output = model(x)\n    # è®¡ç®—æ¿€æ´»çŽ‡ï¼ˆéžé›¶è¾“å‡ºçš„æ¯”ä¾‹ï¼‰\n    activation_rate = (output > 0).float().mean()\n    print(f\"æ¿€æ´»çŽ‡: {activation_rate.item():.2%}\")"
        }
      ]
    }
  ]
}
