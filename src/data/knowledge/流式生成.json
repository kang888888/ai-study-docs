{
  "title": "æµå¼ç”Ÿæˆï¼ˆStreaming Generationï¼‰",
  "subtitle": "å®æ—¶é€tokenç”Ÿæˆå’Œè¿”å›æ–‡æœ¬çš„æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æµå¼ç”ŸæˆåŸç†",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "æµå¼ç”ŸæˆåŸç†",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æµå¼ç”Ÿæˆå¯¹æ¯”",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "æµå¼ç”Ÿæˆå¯¹æ¯”",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ç”Ÿæˆå™¨å®ç°æµå¼ç”Ÿæˆ",
          "language": "python",
          "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ndef generate_stream(model, tokenizer, prompt, max_length=100):\n    \"\"\"æµå¼ç”Ÿæˆæ–‡æœ¬\"\"\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = inputs.input_ids\n    \n    for _ in range(max_length):\n        # ç”Ÿæˆä¸‹ä¸€ä¸ªtoken\n        with torch.no_grad():\n            outputs = model(input_ids)\n            logits = outputs.logits[:, -1, :]\n            next_token = torch.argmax(logits, dim=-1)\n        \n        # è§£ç å¹¶è¿”å›token\n        token_text = tokenizer.decode(next_token, skip_special_tokens=True)\n        yield token_text\n        \n        # æ›´æ–°input_ids\n        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n        \n        # æ£€æŸ¥æ˜¯å¦ç»“æŸ\n        if next_token.item() == tokenizer.eos_token_id:\n            break\n\n# ä½¿ç”¨ç¤ºä¾‹\nfor token in generate_stream(model, tokenizer, \"Hello\"):\n    print(token, end=\"\", flush=True)"
        },
        {
          "type": "code-box",
          "title": "FastAPIæµå¼å“åº”",
          "language": "python",
          "code": "from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\n\napp = FastAPI()\n\n@app.post(\"/stream\")\nasync def stream_generate(prompt: str):\n    \"\"\"æµå¼ç”ŸæˆAPI\"\"\"\n    def generate():\n        for token in generate_stream(model, tokenizer, prompt):\n            yield f\"data: {token}\\n\\n\"\n    \n    return StreamingResponse(\n        generate(),\n        media_type=\"text/event-stream\"\n    )"
        }
      ]
    }
  ]
}