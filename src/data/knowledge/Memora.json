{
  "title": "Memora",
  "subtitle": "åŸºäºMirasæ¡†æ¶çš„é•¿æœŸè®°å¿†ç®¡ç†æ¨¡å‹",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "Memora æ˜¯åŸºäº Miras æ¡†æ¶æå‡ºçš„é•¿æœŸè®°å¿†ç®¡ç†æ¨¡å‹ï¼Œä¸“æ³¨äºé«˜æ•ˆå­˜å‚¨å’Œæ£€ç´¢å†å²ä¿¡æ¯ã€‚Memora é€šè¿‡å±‚æ¬¡åŒ–çš„è®°å¿†æ¶æ„å’Œæ™ºèƒ½çš„è®°å¿†ç®¡ç†æœºåˆ¶ï¼Œèƒ½å¤Ÿå¤„ç†é•¿åºåˆ—å»ºæ¨¡ä»»åŠ¡ï¼Œæ”¯æŒé•¿æœŸä¾èµ–å…³ç³»çš„å­¦ä¹ ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "**é•¿æœŸè®°å¿†ç®¡ç†**ï¼šå±‚æ¬¡åŒ–çš„è®°å¿†æ¶æ„ï¼Œæ”¯æŒå¤§è§„æ¨¡å†å²ä¿¡æ¯å­˜å‚¨",
            "**é«˜æ•ˆå­˜å‚¨å’Œæ£€ç´¢**ï¼šå‹ç¼©è¡¨ç¤ºå’Œé€‰æ‹©æ€§æ£€ç´¢ï¼Œæé«˜å­˜å‚¨å’Œæ£€ç´¢æ•ˆç‡",
            "**é•¿æœŸä¾èµ–å»ºæ¨¡**ï¼šèƒ½å¤Ÿæ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œæ”¯æŒé•¿åºåˆ—å»ºæ¨¡",
            "**è®°å¿†å®¹é‡å¤§**ï¼šæ”¯æŒå¤§è§„æ¨¡è®°å¿†å­˜å‚¨ï¼Œé€‚åº”é•¿åºåˆ—ä»»åŠ¡éœ€æ±‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "è®°å¿†ç®¡ç†ã€KV Cacheä¼˜åŒ–ã€é•¿ä¸Šä¸‹æ–‡æ”¯æŒ"
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ æŠ€æœ¯æ¶æ„",
      "content": [
        {
          "type": "tech-box",
          "content": "å±‚æ¬¡è®°å¿†æ¶æ„ï¼šé‡‡ç”¨å±‚æ¬¡åŒ–çš„è®°å¿†ç»„ç»‡æ–¹å¼ï¼Œæ”¯æŒå¤šå°ºåº¦è®°å¿†ç®¡ç†"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "é•¿åºåˆ—å»ºæ¨¡ä»»åŠ¡ï¼šéœ€è¦å¤„ç†è¶…é•¿åºåˆ—çš„ä»»åŠ¡ï¼Œå¦‚é•¿æ–‡æœ¬ç†è§£ã€ä»£ç åˆ†æ\n                    å¤šè½®å¯¹è¯ç³»ç»Ÿï¼šéœ€è¦é•¿æœŸè®°å¿†å†å²å¯¹è¯çš„æ™ºèƒ½åŠ©æ‰‹ã€å®¢æœç³»ç»Ÿ\n                    å†å²ä¿¡æ¯æ£€ç´¢ï¼šéœ€è¦ä»å¤§é‡å†å²ä¿¡æ¯ä¸­æ£€ç´¢ç›¸å…³å†…å®¹çš„åœºæ™¯\n                    æ—¶é—´åºåˆ—é¢„æµ‹ï¼šéœ€è¦åˆ©ç”¨é•¿æœŸå†å²æ¨¡å¼è¿›è¡Œé¢„æµ‹çš„ä»»åŠ¡"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "Memoraæ¶æ„å›¾",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Memoraæ¶æ„å›¾"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "Memoraç»„ä»¶è¯¦è§£",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Memoraç»„ä»¶è¯¦è§£"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "Memora é•¿æœŸè®°å¿†ç®¡ç†æ¨¡å—",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MemoraLongTermMemory(nn.Module):\n    \"\"\"Memora é•¿æœŸè®°å¿†ç®¡ç†æ¨¡å—\"\"\"\n    def __init__(self, d_model, memory_size, num_levels=3):\n        super(MemoraLongTermMemory, self).__init__()\n        self.d_model = d_model\n        self.memory_size = memory_size\n        self.num_levels = num_levels\n        \n        # å±‚æ¬¡è®°å¿†ï¼šä¸åŒå±‚æ¬¡å­˜å‚¨ä¸åŒæ—¶é—´å°ºåº¦çš„ä¿¡æ¯\n        self.memory_levels = nn.ModuleList([\n            nn.Parameter(torch.randn(memory_size // (2 ** i), d_model))\n            for i in range(num_levels)\n        ])\n        \n        # è®°å¿†ç¼–ç å™¨ï¼ˆå‹ç¼©è¡¨ç¤ºï¼‰\n        self.encoder = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Linear(d_model // 2, d_model)\n        )\n        \n        # è®°å¿†æ£€ç´¢å™¨\n        self.query_proj = nn.Linear(d_model, d_model)\n        self.key_proj = nn.Linear(d_model, d_model)\n        self.value_proj = nn.Linear(d_model, d_model)\n        \n        # è®°å¿†æ›´æ–°å™¨ï¼ˆé€‰æ‹©æ€§æ›´æ–°ï¼‰\n        self.update_gate = nn.Linear(d_model * 2, d_model)\n        self.importance_score = nn.Linear(d_model, 1)\n    \n    def encode(self, x):\n        \"\"\"å‹ç¼©ç¼–ç è¾“å…¥\"\"\"\n        return self.encoder(x)\n    \n    def retrieve(self, query):\n        \"\"\"ä»å±‚æ¬¡è®°å¿†ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯\"\"\"\n        batch_size = query.shape[0]\n        q = self.query_proj(query)\n        \n        all_retrieved = []\n        all_attention = []\n        \n        # ä»æ¯ä¸ªå±‚æ¬¡æ£€ç´¢\n        for level_memory in self.memory_levels:\n            k = self.key_proj(level_memory)\n            v = self.value_proj(level_memory)\n            \n            scores = torch.matmul(q, k.t()) / (self.d_model ** 0.5)\n            attention = F.softmax(scores, dim=-1)\n            retrieved = torch.matmul(attention, v)\n            \n            all_retrieved.append(retrieved)\n            all_attention.append(attention)\n        \n        # èåˆä¸åŒå±‚æ¬¡çš„æ£€ç´¢ç»“æœ\n        combined = torch.stack(all_retrieved, dim=1)  # [batch_size, num_levels, d_model]\n        # ç®€å•çš„å¹³å‡èåˆï¼ˆå¯ä»¥æ”¹ä¸ºåŠ æƒèåˆï¼‰\n        final_retrieved = combined.mean(dim=1)  # [batch_size, d_model]\n        \n        return final_retrieved, all_attention\n    \n    def update(self, new_info, retrieved_memory):\n        \"\"\"é€‰æ‹©æ€§æ›´æ–°é•¿æœŸè®°å¿†\"\"\"\n        # è®¡ç®—é‡è¦æ€§åˆ†æ•°\n        importance = torch.sigmoid(self.importance_score(new_info))  # [batch_size, 1]\n        \n        # é—¨æ§æ›´æ–°\n        combined = torch.cat([new_info, retrieved_memory], dim=-1)\n        gate = torch.sigmoid(self.update_gate(combined))\n        updated = gate * new_info + (1 - gate) * retrieved_memory\n        \n        # æ ¹æ®é‡è¦æ€§é€‰æ‹©æ€§åœ°æ›´æ–°è®°å¿†\n        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ›´æ–°æœ€ç›¸å…³çš„è®°å¿†ä½ç½®\n        return updated, importance\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    memora = MemoraLongTermMemory(d_model=512, memory_size=1000, num_levels=3)\n    query = torch.randn(2, 512)\n    new_info = torch.randn(2, 512)\n    \n    # æ£€ç´¢é•¿æœŸè®°å¿†\n    retrieved, attention = memora.retrieve(query)\n    print(f\"æ£€ç´¢ç»“æœå½¢çŠ¶: {retrieved.shape}\")  # [2, 512]\n    \n    # æ›´æ–°è®°å¿†\n    updated, importance = memora.update(new_info, retrieved)\n    print(f\"æ›´æ–°åå½¢çŠ¶: {updated.shape}\")  # [2, 512]\n    print(f\"é‡è¦æ€§åˆ†æ•°: {importance.squeeze()}\")"
        }
      ]
    }
  ]
}