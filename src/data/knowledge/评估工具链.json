{
  "title": "è¯„ä¼°å·¥å…·é“¾",
  "subtitle": "è¯„ä¼°æµç¨‹è‡ªåŠ¨åŒ–ã€ç»“æœåˆ†æä¸æŠ¥å‘Šç”Ÿæˆå·¥å…·ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "è¯„ä¼°å·¥å…·é“¾æ˜¯ç”¨äºæ¨¡å‹è¯„ä¼°çš„å®Œæ•´å·¥å…·é›†ï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€æŒ‡æ ‡è®¡ç®—ã€ç»“æœåˆ†æç­‰ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "åŠŸèƒ½å®Œæ•´ï¼šå®Œæ•´çš„è¯„ä¼°å·¥å…·é›†",
            "æ˜“äºä½¿ç”¨ï¼šç®€å•çš„ä½¿ç”¨æ¥å£",
            "å¯æ‰©å±•ï¼šå¯æ‰©å±•æ–°åŠŸèƒ½",
            "æŒç»­å‘å±•ï¼šä¸æ–­æ”¹è¿›",
            "å¹¿æ³›åº”ç”¨ï¼šæ ‡å‡†è¯„ä¼°å·¥å…·"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "æ•°æ®åŠ è½½ã€æŒ‡æ ‡è®¡ç®—ã€ç»“æœåˆ†æã€å¯è§†åŒ–"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "æ¨¡å‹è¯„ä¼°ã€æ€§èƒ½åˆ†æã€ç»“æœåˆ†æã€è¯„ä¼°æµç¨‹"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "è¯„ä¼°å·¥å…·é“¾æ¶æ„",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "è¯„ä¼°å·¥å…·é“¾æ¶æ„"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "è¯„ä¼°æµç¨‹è‡ªåŠ¨åŒ–",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "flow",
                "title": "è¯„ä¼°æµç¨‹è‡ªåŠ¨åŒ–"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "è¯„ä¼°æµç¨‹è‡ªåŠ¨åŒ–",
          "language": "python",
          "code": "import json\nimport pandas as pd\nfrom datetime import datetime\nfrom pathlib import Path\n\nclass EvaluationPipeline:\n    \"\"\"è¯„ä¼°æµç¨‹è‡ªåŠ¨åŒ–å·¥å…·\"\"\"\n    def __init__(self, output_dir=\"./eval_results\"):\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(exist_ok=True)\n        self.results = []\n    \n    def run_evaluation(self, model_name, tasks, config=None):\n        \"\"\"è¿è¡Œè¯„ä¼°ä»»åŠ¡\"\"\"\n        from lm_eval import evaluator\n        \n        config = config or {}\n        \n        # è¿è¡Œè¯„ä¼°\n        results = evaluator.simple_evaluate(\n            model=model_name,\n            model_args=f\"pretrained={model_name}\",\n            tasks=tasks,\n            **config\n        )\n        \n        # è®°å½•ç»“æœ\n        result_entry = {\n            'model': model_name,\n            'timestamp': datetime.now().isoformat(),\n            'tasks': tasks,\n            'results': results['results']\n        }\n        \n        self.results.append(result_entry)\n        \n        # ä¿å­˜ç»“æœ\n        self.save_results(result_entry)\n        \n        return results\n    \n    def save_results(self, result_entry):\n        \"\"\"ä¿å­˜è¯„ä¼°ç»“æœ\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"{result_entry['model']}_{timestamp}.json\"\n        filepath = self.output_dir / filename\n        \n        with open(filepath, 'w') as f:\n            json.dump(result_entry, f, indent=2)\n        \n        print(f\"ç»“æœå·²ä¿å­˜åˆ°: {filepath}\")\n    \n    def generate_report(self):\n        \"\"\"ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š\"\"\"\n        if not self.results:\n            return\n        \n        # æå–å…³é”®æŒ‡æ ‡\n        report_data = []\n        for result in self.results:\n            for task, metrics in result['results'].items():\n                report_data.append({\n                    'model': result['model'],\n                    'task': task,\n                    'accuracy': metrics.get('acc', 0),\n                    'timestamp': result['timestamp']\n                })\n        \n        # ç”ŸæˆDataFrame\n        df = pd.DataFrame(report_data)\n        \n        # ä¿å­˜ä¸ºCSV\n        report_path = self.output_dir / \"evaluation_report.csv\"\n        df.to_csv(report_path, index=False)\n        \n        print(f\"æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}\")\n        return df\n\n# ä½¿ç”¨ç¤ºä¾‹\npipeline = EvaluationPipeline()\npipeline.run_evaluation(\"gpt2\", [\"hellaswag\", \"mmlu\"])\npipeline.generate_report()"
        },
        {
          "type": "code-box",
          "title": "ç»“æœåˆ†æä¸å¯è§†åŒ–",
          "language": "python",
          "code": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass ResultAnalyzer:\n    \"\"\"è¯„ä¼°ç»“æœåˆ†æå·¥å…·\"\"\"\n    def __init__(self, results_file):\n        self.df = pd.read_csv(results_file)\n    \n    def compare_models(self, models, task):\n        \"\"\"æ¯”è¾ƒä¸åŒæ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°\"\"\"\n        task_data = self.df[(self.df['task'] == task) & (self.df['model'].isin(models))]\n        \n        plt.figure(figsize=(10, 6))\n        sns.barplot(data=task_data, x='model', y='accuracy')\n        plt.title(f'æ¨¡å‹å¯¹æ¯” - {task}')\n        plt.ylabel('Accuracy')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.savefig(f'{task}_comparison.png')\n        plt.show()\n    \n    def task_performance_matrix(self):\n        \"\"\"ç”Ÿæˆä»»åŠ¡æ€§èƒ½çŸ©é˜µ\"\"\"\n        pivot_df = self.df.pivot_table(\n            index='model',\n            columns='task',\n            values='accuracy',\n            aggfunc='mean'\n        )\n        \n        plt.figure(figsize=(12, 8))\n        sns.heatmap(pivot_df, annot=True, fmt='.3f', cmap='YlOrRd')\n        plt.title('æ¨¡å‹ä»»åŠ¡æ€§èƒ½çŸ©é˜µ')\n        plt.tight_layout()\n        plt.savefig('performance_matrix.png')\n        plt.show()\n        \n        return pivot_df\n\n# ä½¿ç”¨ç¤ºä¾‹\nanalyzer = ResultAnalyzer('evaluation_report.csv')\nanalyzer.compare_models(['gpt2', 'gpt2-medium'], 'hellaswag')\nanalyzer.task_performance_matrix()"
        }
      ]
    }
  ]
}