{
  "title": "æ¿€æ´»å‡½æ•° (Activation Function)",
  "subtitle": "å¼•å…¥éçº¿æ€§ï¼Œå¦‚ReLUã€Sigmoidã€Tanhã€GELUç­‰",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "æ¿€æ´»å‡½æ•°æ˜¯ç¥ç»ç½‘ç»œä¸­å¼•å…¥éçº¿æ€§çš„å…³é”®ç»„ä»¶ã€‚å¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œå¤šå±‚ç¥ç»ç½‘ç»œç­‰ä»·äºå•å±‚çº¿æ€§æ¨¡å‹ã€‚æ¿€æ´»å‡½æ•°ä½¿å¾—ç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å’Œè¡¨ç¤ºå¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ ä¸»è¦ç±»å‹",
      "content": [
        {
          "type": "features",
          "items": [
            "ReLUï¼šæœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼Œè®¡ç®—ç®€å•ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±",
            "Sigmoidï¼šå°†è¾“å‡ºå‹ç¼©åˆ°(0,1)ï¼Œç”¨äºäºŒåˆ†ç±»è¾“å‡ºå±‚",
            "Tanhï¼šå°†è¾“å‡ºå‹ç¼©åˆ°(-1,1)ï¼Œé›¶ä¸­å¿ƒåŒ–",
            "GELUï¼šTransformerä¸­å¸¸ç”¨ï¼Œå¹³æ»‘çš„ReLUå˜ä½“",
            "Swish/SiLUï¼šè‡ªé—¨æ§æ¿€æ´»å‡½æ•°ï¼Œæ€§èƒ½ä¼˜äºReLU",
            "Leaky ReLUï¼šè§£å†³ReLUçš„æ­»ç¥ç»å…ƒé—®é¢˜",
            "ELUï¼šæŒ‡æ•°çº¿æ€§å•å…ƒï¼Œè´Ÿå€¼éƒ¨åˆ†å¹³æ»‘"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "éçº¿æ€§å˜æ¢ã€æ¢¯åº¦ä¼ æ’­ã€æ­»ç¥ç»å…ƒã€é¥±å’Œé—®é¢˜"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "CNNï¼ˆReLUï¼‰ã€Transformerï¼ˆGELUï¼‰ã€RNNï¼ˆTanhï¼‰ã€è¾“å‡ºå±‚ï¼ˆSigmoid/Softmaxï¼‰"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æ¿€æ´»å‡½æ•°å¯¹æ¯”",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "æ¿€æ´»å‡½æ•°å¯¹æ¯”",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æ¿€æ´»å‡½æ•°æ›²çº¿",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "æ¿€æ´»å‡½æ•°æ›²çº¿",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "ReLUï¼ˆRectified Linear Unitï¼‰",
          "formulas": [
            {
              "text": "ReLUæ˜¯æœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼š"
            },
            {
              "display": "\\text{ReLU}(x) = \\max(0, x) = \\begin{cases} x & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}"
            },
            {
              "text": "å¯¼æ•°ï¼š"
            },
            {
              "display": "\\frac{d}{dx}\\text{ReLU}(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}"
            },
            {
              "text": "ä¼˜ç‚¹ï¼šè®¡ç®—ç®€å•ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼›ç¼ºç‚¹ï¼šå¯èƒ½å¯¼è‡´æ­»ç¥ç»å…ƒ"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Sigmoid",
          "formulas": [
            {
              "text": "Sigmoidå°†è¾“å‡ºå‹ç¼©åˆ°(0,1)åŒºé—´ï¼š"
            },
            {
              "display": "\\sigma(x) = \\frac{1}{1 + e^{-x}}"
            },
            {
              "text": "å¯¼æ•°ï¼š"
            },
            {
              "display": "\\frac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x))"
            },
            {
              "text": "ä¼˜ç‚¹ï¼šè¾“å‡ºæœ‰ç•Œï¼Œé€‚åˆæ¦‚ç‡è¾“å‡ºï¼›ç¼ºç‚¹ï¼šå®¹æ˜“é¥±å’Œï¼Œæ¢¯åº¦æ¶ˆå¤±"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Tanhï¼ˆåŒæ›²æ­£åˆ‡ï¼‰",
          "formulas": [
            {
              "text": "Tanhå°†è¾“å‡ºå‹ç¼©åˆ°(-1,1)åŒºé—´ï¼š"
            },
            {
              "display": "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}"
            },
            {
              "text": "å¯¼æ•°ï¼š"
            },
            {
              "display": "\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x)"
            },
            {
              "text": "ä¼˜ç‚¹ï¼šé›¶ä¸­å¿ƒåŒ–ï¼Œæ”¶æ•›æ›´å¿«ï¼›ç¼ºç‚¹ï¼šä»å­˜åœ¨é¥±å’Œé—®é¢˜"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "GELUï¼ˆGaussian Error Linear Unitï¼‰",
          "formulas": [
            {
              "text": "GELUæ˜¯Transformerä¸­å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼š"
            },
            {
              "display": "\\text{GELU}(x) = x \\cdot \\Phi(x)"
            },
            {
              "text": "å…¶ä¸­ $\\Phi(x)$ æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°"
            },
            {
              "text": "è¿‘ä¼¼è®¡ç®—ï¼š"
            },
            {
              "display": "\\text{GELU}(x) \\approx 0.5x\\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}}\\left(x + 0.044715x^3\\right)\\right]\\right)"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Swish/SiLU",
          "formulas": [
            {
              "text": "Swishæ˜¯è‡ªé—¨æ§æ¿€æ´»å‡½æ•°ï¼š"
            },
            {
              "display": "\\text{Swish}(x) = x \\cdot \\sigma(x) = \\frac{x}{1 + e^{-x}}"
            },
            {
              "text": "ä¼˜ç‚¹ï¼šå¹³æ»‘ã€éå•è°ƒï¼Œæ€§èƒ½ä¼˜äºReLU"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "PyTorch ä¸­çš„æ¿€æ´»å‡½æ•°",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 1. åŸºç¡€æ¿€æ´»å‡½æ•°\nx = torch.linspace(-5, 5, 100)\n\n# ReLU\nrelu = F.relu(x)\nprint(f\"ReLUç¤ºä¾‹: {F.relu(torch.tensor([-1.0, 0.0, 1.0]))}\")\n\n# Sigmoid\nsigmoid = torch.sigmoid(x)\nprint(f\"Sigmoidç¤ºä¾‹: {torch.sigmoid(torch.tensor([-1.0, 0.0, 1.0]))}\")\n\n# Tanh\ntanh = torch.tanh(x)\nprint(f\"Tanhç¤ºä¾‹: {torch.tanh(torch.tensor([-1.0, 0.0, 1.0]))}\")\n\n# GELU\ngelu = F.gelu(x)\nprint(f\"GELUç¤ºä¾‹: {F.gelu(torch.tensor([-1.0, 0.0, 1.0]))}\")\n\n# Swish/SiLU\nsilu = F.silu(x)\nprint(f\"SiLUç¤ºä¾‹: {F.silu(torch.tensor([-1.0, 0.0, 1.0]))}\")\n\n# 2. åœ¨ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨\nclass MLPWithActivations(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, activation='relu'):\n        super(MLPWithActivations, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        \n        # é€‰æ‹©æ¿€æ´»å‡½æ•°\n        if activation == 'relu':\n            self.activation = nn.ReLU()\n        elif activation == 'gelu':\n            self.activation = nn.GELU()\n        elif activation == 'tanh':\n            self.activation = nn.Tanh()\n        elif activation == 'sigmoid':\n            self.activation = nn.Sigmoid()\n        elif activation == 'silu':\n            self.activation = nn.SiLU()\n        else:\n            raise ValueError(f\"Unknown activation: {activation}\")\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.activation(x)\n        x = self.fc2(x)\n        return x\n\n# ä½¿ç”¨ä¸åŒæ¿€æ´»å‡½æ•°\nmodel_relu = MLPWithActivations(10, 50, 1, activation='relu')\nmodel_gelu = MLPWithActivations(10, 50, 1, activation='gelu')\n\n# 3. Leaky ReLUï¼ˆè§£å†³æ­»ç¥ç»å…ƒé—®é¢˜ï¼‰\nleaky_relu = nn.LeakyReLU(negative_slope=0.01)\nx = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\nprint(f\"Leaky ReLU: {leaky_relu(x)}\")\n\n# 4. PReLUï¼ˆå¯å­¦ä¹ çš„Leaky ReLUï¼‰\nprelu = nn.PReLU()\nx = torch.randn(10, 20)\noutput = prelu(x)\n\n# 5. ELUï¼ˆæŒ‡æ•°çº¿æ€§å•å…ƒï¼‰\nelu = nn.ELU(alpha=1.0)\nx = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\nprint(f\"ELU: {elu(x)}\")\n\n# 6. Softmaxï¼ˆç”¨äºå¤šåˆ†ç±»è¾“å‡ºå±‚ï¼‰\nlogits = torch.randn(32, 10)  # [batch_size, num_classes]\nprobs = F.softmax(logits, dim=1)  # åœ¨ç±»åˆ«ç»´åº¦ä¸Šsoftmax\nprint(f\"Softmaxè¾“å‡ºå½¢çŠ¶: {probs.shape}\")\nprint(f\"æ¯è¡Œå’Œä¸º1: {probs.sum(dim=1)[0].item()}\")\n\n# 7. è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°\nclass Mish(nn.Module):\n    \"\"\"Mishæ¿€æ´»å‡½æ•°: x * tanh(softplus(x))\"\"\"\n    def forward(self, x):\n        return x * torch.tanh(F.softplus(x))\n\nmish = Mish()\nx = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\nprint(f\"Mish: {mish(x)}\")\n\n# 8. å¯è§†åŒ–ä¸åŒæ¿€æ´»å‡½æ•°\ndef plot_activations():\n    x = np.linspace(-5, 5, 100)\n    x_tensor = torch.tensor(x, dtype=torch.float32)\n    \n    activations = {\n        'ReLU': F.relu(x_tensor),\n        'Sigmoid': torch.sigmoid(x_tensor),\n        'Tanh': torch.tanh(x_tensor),\n        'GELU': F.gelu(x_tensor),\n        'SiLU': F.silu(x_tensor),\n        'Leaky ReLU': F.leaky_relu(x_tensor, 0.01)\n    }\n    \n    plt.figure(figsize=(12, 8))\n    for i, (name, y) in enumerate(activations.items(), 1):\n        plt.subplot(2, 3, i)\n        plt.plot(x, y.numpy())\n        plt.title(name)\n        plt.grid(True)\n        plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n        plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('activation_functions.png')\n    print(\"æ¿€æ´»å‡½æ•°å¯è§†åŒ–å·²ä¿å­˜\")\n\n# 9. å®é™…åº”ç”¨ç¤ºä¾‹\nclass ConvNetWithActivations(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ConvNetWithActivations, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.fc = nn.Linear(128 * 8 * 8, num_classes)\n        \n        # ä½¿ç”¨GELUæ¿€æ´»å‡½æ•°\n        self.activation = nn.GELU()\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.activation(x)\n        x = F.max_pool2d(x, 2)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.activation(x)\n        x = F.max_pool2d(x, 2)\n        \n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# 10. æ¿€æ´»å‡½æ•°é€‰æ‹©å»ºè®®\ndef get_activation(name):\n    \"\"\"æ ¹æ®åç§°è¿”å›æ¿€æ´»å‡½æ•°\"\"\"\n    activations = {\n        'relu': nn.ReLU(),\n        'gelu': nn.GELU(),\n        'tanh': nn.Tanh(),\n        'sigmoid': nn.Sigmoid(),\n        'silu': nn.SiLU(),\n        'leaky_relu': nn.LeakyReLU(0.01),\n        'elu': nn.ELU(),\n    }\n    return activations.get(name.lower(), nn.ReLU())\n\n# ä½¿ç”¨å»ºè®®ï¼š\n# - CNN: ReLUæˆ–Leaky ReLU\n# - Transformer: GELU\n# - RNN: Tanh\n# - è¾“å‡ºå±‚ï¼ˆäºŒåˆ†ç±»ï¼‰: Sigmoid\n# - è¾“å‡ºå±‚ï¼ˆå¤šåˆ†ç±»ï¼‰: Softmax"
        }
      ]
    }
  ]
}