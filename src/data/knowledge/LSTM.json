{
  "title": "LSTM (Long Short-Term Memory) é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ",
  "subtitle": "è§£å†³é•¿ç¨‹ä¾èµ–é—®é¢˜çš„RNNæ”¹è¿›ç‰ˆæœ¬",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "RNNçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œé€šè¿‡å¼•å…¥é—¨æ§æœºåˆ¶ï¼ˆé—å¿˜é—¨ã€è¾“å…¥é—¨ã€è¾“å‡ºé—¨ï¼‰å’Œç»†èƒçŠ¶æ€ï¼ˆCell Stateï¼‰ï¼Œæœ‰æ•ˆè§£å†³äº†é•¿ç¨‹ä¾èµ–é—®é¢˜å’Œæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "é—¨æ§æœºåˆ¶ï¼šé€šè¿‡ä¸‰ä¸ªé—¨ï¼ˆé—å¿˜ã€è¾“å…¥ã€è¾“å‡ºï¼‰æ§åˆ¶ä¿¡æ¯æµåŠ¨",
            "ç»†èƒçŠ¶æ€ï¼šé•¿æœŸè®°å¿†é€šé“ï¼Œæ¢¯åº¦å¯ä»¥æ— æŸä¼ æ’­",
            "é•¿ç¨‹ä¾èµ–ï¼šèƒ½å¤Ÿæ•æ‰åºåˆ—ä¸­ç›¸è·è¾ƒè¿œçš„ä¾èµ–å…³ç³»",
            "å‚æ•°é‡è¾ƒå¤§ï¼šç›¸æ¯”RNNï¼Œå‚æ•°é‡å¢åŠ çº¦4å€",
            "è®­ç»ƒç¨³å®šï¼šæ¢¯åº¦æµåŠ¨æ›´åŠ ç¨³å®š"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "é—¨æ§å•å…ƒã€ç»†èƒçŠ¶æ€ã€Peepholeè¿æ¥ï¼ˆå¯é€‰ï¼‰ã€é—å¿˜é—¨åç½®åˆå§‹åŒ–"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€è¯­éŸ³è¯†åˆ«ã€æ—¶é—´åºåˆ—é¢„æµ‹ã€æƒ…æ„Ÿåˆ†æ"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "LSTMDiagram",
              "caption": "LSTMå•å…ƒç»“æ„",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "cell",
                "title": "LSTMå•å…ƒç»“æ„"
              }
            },
            {
              "type": "svg-d3",
              "component": "LSTMDiagram",
              "caption": "LSTMåºåˆ—å±•å¼€",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "unfolded",
                "title": "LSTMåºåˆ—å±•å¼€"
              }
            },
            {
              "type": "svg-d3",
              "component": "LSTMDiagram",
              "caption": "LSTMé—¨æ§æœºåˆ¶",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "gates",
                "title": "LSTMé—¨æ§æœºåˆ¶"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "LSTM æ ¸å¿ƒå…¬å¼",
          "formulas": [
            {
              "text": "åœ¨æ—¶é—´æ­¥ $t$ï¼ŒLSTM çš„è®¡ç®—è¿‡ç¨‹ï¼š",
              "inline": "t"
            },
            {
              "text": "é—å¿˜é—¨ï¼ˆForget Gateï¼‰ï¼š"
            },
            {
              "display": "f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)"
            },
            {
              "text": "è¾“å…¥é—¨ï¼ˆInput Gateï¼‰ï¼š"
            },
            {
              "display": "i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)"
            },
            {
              "display": "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)"
            },
            {
              "text": "ç»†èƒçŠ¶æ€æ›´æ–°ï¼š"
            },
            {
              "display": "C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t"
            },
            {
              "text": "è¾“å‡ºé—¨ï¼ˆOutput Gateï¼‰ï¼š"
            },
            {
              "display": "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)"
            },
            {
              "display": "h_t = o_t * \\tanh(C_t)"
            },
            {
              "text": "å…¶ä¸­ï¼š"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ PyTorch å®ç° LSTM",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\n\nclass LSTMCell(nn.Module):\n    \"\"\"æ‰‹åŠ¨å®ç° LSTM å•å…ƒ\"\"\"\n    def __init__(self, input_size, hidden_size):\n        super(LSTMCell, self).__init__()\n        self.hidden_size = hidden_size\n        \n        # é—å¿˜é—¨å‚æ•°\n        self.W_f = nn.Linear(input_size + hidden_size, hidden_size)\n        \n        # è¾“å…¥é—¨å‚æ•°\n        self.W_i = nn.Linear(input_size + hidden_size, hidden_size)\n        self.W_C = nn.Linear(input_size + hidden_size, hidden_size)\n        \n        # è¾“å‡ºé—¨å‚æ•°\n        self.W_o = nn.Linear(input_size + hidden_size, hidden_size)\n    \n    def forward(self, x, h_prev, C_prev):\n        \"\"\"\n        å‰å‘ä¼ æ’­\n        \n        å‚æ•°:\n            x: å½“å‰è¾“å…¥ (batch_size, input_size)\n            h_prev: å‰ä¸€ä¸ªéšè—çŠ¶æ€ (batch_size, hidden_size)\n            C_prev: å‰ä¸€ä¸ªç»†èƒçŠ¶æ€ (batch_size, hidden_size)\n        \"\"\"\n        # æ‹¼æ¥è¾“å…¥å’Œéšè—çŠ¶æ€\n        combined = torch.cat([x, h_prev], dim=1)\n        \n        # é—å¿˜é—¨\n        f_t = torch.sigmoid(self.W_f(combined))\n        \n        # è¾“å…¥é—¨\n        i_t = torch.sigmoid(self.W_i(combined))\n        C_tilde = torch.tanh(self.W_C(combined))\n        \n        # æ›´æ–°ç»†èƒçŠ¶æ€\n        C_t = f_t * C_prev + i_t * C_tilde\n        \n        # è¾“å‡ºé—¨\n        o_t = torch.sigmoid(self.W_o(combined))\n        \n        # è®¡ç®—éšè—çŠ¶æ€\n        h_t = o_t * torch.tanh(C_t)\n        \n        return h_t, C_t\n\nclass LSTM_Model(nn.Module):\n    \"\"\"ä½¿ç”¨ PyTorch å†…ç½® LSTM\"\"\"\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(LSTM_Model, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # LSTM å±‚\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n                           batch_first=True, dropout=0.2)\n        \n        # å…¨è¿æ¥å±‚\n        self.fc = nn.Linear(hidden_size, num_classes)\n    \n    def forward(self, x):\n        # x shape: (batch_size, seq_length, input_size)\n        # åˆå§‹åŒ–éšè—çŠ¶æ€å’Œç»†èƒçŠ¶æ€\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        \n        # LSTM å‰å‘ä¼ æ’­\n        out, (h_n, c_n) = self.lstm(x, (h0, c0))\n        \n        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º\n        out = self.fc(out[:, -1, :])\n        \n        return out\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    # ä½¿ç”¨ PyTorch å†…ç½® LSTM\n    model = LSTM_Model(input_size=128, hidden_size=256, \n                      num_layers=2, num_classes=10)\n    \n    # æ¨¡æ‹Ÿè¾“å…¥ (batch_size=32, seq_length=50, input_size=128)\n    x = torch.randn(32, 50, 128)\n    \n    # å‰å‘ä¼ æ’­\n    output = model(x)\n    print(f\"è¾“å‡ºå½¢çŠ¶: {output.shape}\")  # [32, 10]\n    \n    # æ‰‹åŠ¨å®ç° LSTM Cell\n    lstm_cell = LSTMCell(input_size=128, hidden_size=256)\n    \n    # åˆå§‹åŒ–çŠ¶æ€\n    h = torch.zeros(32, 256)\n    C = torch.zeros(32, 256)\n    \n    # å¤„ç†åºåˆ—\n    for t in range(50):\n        x_t = torch.randn(32, 128)\n        h, C = lstm_cell(x_t, h, C)\n    \n    print(f\"æœ€ç»ˆéšè—çŠ¶æ€å½¢çŠ¶: {h.shape}\")"
        },
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ NumPy æ‰‹åŠ¨å®ç° LSTM",
          "language": "python",
          "code": "import numpy as np\n\nclass LSTM_Numpy:\n    \"\"\"ä½¿ç”¨ NumPy æ‰‹åŠ¨å®ç° LSTM\"\"\"\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        # åˆå§‹åŒ–æƒé‡çŸ©é˜µ\n        # æƒé‡å½¢çŠ¶: (input_size + hidden_size, hidden_size)\n        scale = 1.0 / np.sqrt(input_size + hidden_size)\n        \n        # é—å¿˜é—¨æƒé‡\n        self.W_f = np.random.randn(input_size + hidden_size, hidden_size) * scale\n        self.b_f = np.zeros((1, hidden_size))\n        \n        # è¾“å…¥é—¨æƒé‡\n        self.W_i = np.random.randn(input_size + hidden_size, hidden_size) * scale\n        self.b_i = np.zeros((1, hidden_size))\n        \n        # å€™é€‰å€¼æƒé‡\n        self.W_C = np.random.randn(input_size + hidden_size, hidden_size) * scale\n        self.b_C = np.zeros((1, hidden_size))\n        \n        # è¾“å‡ºé—¨æƒé‡\n        self.W_o = np.random.randn(input_size + hidden_size, hidden_size) * scale\n        self.b_o = np.zeros((1, hidden_size))\n    \n    def sigmoid(self, x):\n        \"\"\"Sigmoid æ¿€æ´»å‡½æ•°\"\"\"\n        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n    \n    def tanh(self, x):\n        \"\"\"Tanh æ¿€æ´»å‡½æ•°\"\"\"\n        return np.tanh(x)\n    \n    def forward_step(self, x_t, h_prev, C_prev):\n        \"\"\"\n        å•ä¸ªæ—¶é—´æ­¥çš„å‰å‘ä¼ æ’­\n        \n        å‚æ•°:\n            x_t: å½“å‰è¾“å…¥ (batch_size, input_size)\n            h_prev: å‰ä¸€ä¸ªéšè—çŠ¶æ€ (batch_size, hidden_size)\n            C_prev: å‰ä¸€ä¸ªç»†èƒçŠ¶æ€ (batch_size, hidden_size)\n        \"\"\"\n        # æ‹¼æ¥è¾“å…¥å’Œéšè—çŠ¶æ€\n        combined = np.concatenate([x_t, h_prev], axis=1)\n        \n        # é—å¿˜é—¨\n        f_t = self.sigmoid(np.dot(combined, self.W_f) + self.b_f)\n        \n        # è¾“å…¥é—¨\n        i_t = self.sigmoid(np.dot(combined, self.W_i) + self.b_i)\n        C_tilde = self.tanh(np.dot(combined, self.W_C) + self.b_C)\n        \n        # æ›´æ–°ç»†èƒçŠ¶æ€\n        C_t = f_t * C_prev + i_t * C_tilde\n        \n        # è¾“å‡ºé—¨\n        o_t = self.sigmoid(np.dot(combined, self.W_o) + self.b_o)\n        \n        # è®¡ç®—éšè—çŠ¶æ€\n        h_t = o_t * self.tanh(C_t)\n        \n        return h_t, C_t\n    \n    def forward(self, X):\n        \"\"\"\n        å¤„ç†æ•´ä¸ªåºåˆ—\n        \n        å‚æ•°:\n            X: è¾“å…¥åºåˆ— (batch_size, seq_length, input_size)\n        \"\"\"\n        batch_size, seq_length, _ = X.shape\n        \n        # åˆå§‹åŒ–çŠ¶æ€\n        h = np.zeros((batch_size, self.hidden_size))\n        C = np.zeros((batch_size, self.hidden_size))\n        \n        # å­˜å‚¨æ‰€æœ‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€\n        hidden_states = []\n        \n        for t in range(seq_length):\n            x_t = X[:, t, :]\n            h, C = self.forward_step(x_t, h, C)\n            hidden_states.append(h)\n        \n        # è¿”å›æ‰€æœ‰éšè—çŠ¶æ€å’Œæœ€ç»ˆçŠ¶æ€\n        return np.array(hidden_states), h, C\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    # åˆ›å»º LSTM æ¨¡å‹\n    lstm = LSTM_Numpy(input_size=10, hidden_size=20)\n    \n    # åˆ›å»ºè¾“å…¥åºåˆ— (batch_size=5, seq_length=8, input_size=10)\n    X = np.random.randn(5, 8, 10)\n    \n    # å‰å‘ä¼ æ’­\n    hidden_states, final_h, final_C = lstm.forward(X)\n    \n    print(f\"éšè—çŠ¶æ€åºåˆ—å½¢çŠ¶: {hidden_states.shape}\")  # (8, 5, 20)\n    print(f\"æœ€ç»ˆéšè—çŠ¶æ€å½¢çŠ¶: {final_h.shape}\")  # (5, 20)\n    print(f\"æœ€ç»ˆç»†èƒçŠ¶æ€å½¢çŠ¶: {final_C.shape}\")  # (5, 20)"
        }
      ]
    }
  ]
}