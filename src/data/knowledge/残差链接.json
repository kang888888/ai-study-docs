{
  "title": "æ®‹å·®é“¾æ¥ (Residual Connection)",
  "subtitle": "è·³è·ƒè¿æ¥å®ç°æ¢¯åº¦ç›´æ¥ä¼ æ’­",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "æ®‹å·®é“¾æ¥ï¼ˆSkip Connection / Shortcut Connectionï¼‰æ˜¯ä¸€ç§è·³è·ƒè¿æ¥æŠ€æœ¯ï¼Œé€šè¿‡å°†è¾“å…¥ç›´æ¥æ·»åŠ åˆ°å±‚çš„è¾“å‡ºï¼Œå…è®¸ä¿¡æ¯ç›´æ¥ä»å‰å±‚ä¼ é€’åˆ°åå±‚ï¼Œè§£å†³äº†æ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "æ’ç­‰æ˜ å°„ï¼šå…è®¸ç½‘ç»œå­¦ä¹ æ’ç­‰å‡½æ•°ï¼Œé™ä½ä¼˜åŒ–éš¾åº¦",
            "æ¢¯åº¦ç›´é€šï¼šæ¢¯åº¦å¯ä»¥ç›´æ¥é€šè¿‡è·³è·ƒè¿æ¥ä¼ æ’­ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±",
            "ç‰¹å¾å¤ç”¨ï¼šæµ…å±‚ç‰¹å¾å¯ä»¥ç›´æ¥ä¼ é€’åˆ°æ·±å±‚ï¼Œä¿ç•™ç»†èŠ‚ä¿¡æ¯",
            "è®­ç»ƒç¨³å®šï¼šä½¿æ·±å±‚ç½‘ç»œæ›´å®¹æ˜“è®­ç»ƒå’Œæ”¶æ•›",
            "å¹¿æ³›åº”ç”¨ï¼šResNetã€Transformerã€UNetç­‰æ¶æ„çš„æ ¸å¿ƒç»„ä»¶"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "æ®‹å·®å—ã€è·³è·ƒè¿æ¥ã€æ’ç­‰æ˜ å°„ã€æ¢¯åº¦ç›´é€šã€ç‰¹å¾èåˆ"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "ResNetæ®‹å·®ç½‘ç»œã€Transformerå‰é¦ˆç½‘ç»œã€UNetç¼–ç å™¨-è§£ç å™¨ã€GANç”Ÿæˆå™¨ã€æ³¨æ„åŠ›æœºåˆ¶"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æ®‹å·®é“¾æ¥åŸç†",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "æ®‹å·®é“¾æ¥åŸç†"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "æ®‹å·®å­¦ä¹ ",
          "formulas": [
            {
              "text": "æ®‹å·®é“¾æ¥çš„æ ¸å¿ƒæ€æƒ³æ˜¯å­¦ä¹ æ®‹å·®è€Œéç›´æ¥æ˜ å°„ï¼š"
            },
            {
              "display": "y = F(x, W) + x"
            },
            {
              "text": "å…¶ä¸­ $F(x, W)$ æ˜¯æ®‹å·®å‡½æ•°ï¼Œ$x$ æ˜¯è¾“å…¥",
              "inline": "F(x, W)"
            },
            {
              "text": "å¦‚æœæœ€ä¼˜æ˜ å°„æ¥è¿‘æ’ç­‰æ˜ å°„ï¼Œå­¦ä¹ æ®‹å·® $F(x) \\approx 0$ æ¯”ç›´æ¥å­¦ä¹  $H(x) \\approx x$ æ›´å®¹æ˜“"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "æ¢¯åº¦æµåŠ¨",
          "formulas": [
            {
              "text": "æ®‹å·®è¿æ¥ä½¿å¾—æ¢¯åº¦å¯ä»¥ç›´æ¥ä¼ æ’­ï¼š"
            },
            {
              "display": "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\left(1 + \\frac{\\partial F(x)}{\\partial x}\\right)"
            },
            {
              "text": "å³ä½¿ $\\frac{\\partial F(x)}{\\partial x} \\approx 0$ï¼Œæ¢¯åº¦ä»å¯ä»¥é€šè¿‡æ’ç­‰é¡¹ $1$ ä¼ æ’­ï¼Œé¿å…äº†æ¢¯åº¦æ¶ˆå¤±",
              "inline": "\\frac{\\partial F(x)}{\\partial x} \\approx 0"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "ç»´åº¦åŒ¹é…",
          "formulas": [
            {
              "text": "å½“è¾“å…¥è¾“å‡ºç»´åº¦ä¸åŒæ—¶ï¼Œéœ€è¦æŠ•å½±ï¼š"
            },
            {
              "display": "y = F(x, W) + W_s x"
            },
            {
              "text": "å…¶ä¸­ $W_s$ æ˜¯æŠ•å½±çŸ©é˜µï¼Œç”¨äºåŒ¹é…ç»´åº¦"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "PyTorch å®ç°æ®‹å·®é“¾æ¥",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# 1. åŸºç¡€æ®‹å·®å—ï¼ˆç»´åº¦ç›¸åŒï¼‰\nclass ResidualBlock(nn.Module):\n    \"\"\"åŸºç¡€æ®‹å·®å—ï¼Œè¾“å…¥è¾“å‡ºç»´åº¦ç›¸åŒ\"\"\"\n    def __init__(self, dim):\n        super(ResidualBlock, self).__init__()\n        self.fc1 = nn.Linear(dim, dim)\n        self.bn1 = nn.BatchNorm1d(dim)\n        self.fc2 = nn.Linear(dim, dim)\n        self.bn2 = nn.BatchNorm1d(dim)\n    \n    def forward(self, x):\n        identity = x  # ä¿å­˜è¾“å…¥\n        \n        out = self.fc1(x)\n        out = self.bn1(out)\n        out = F.relu(out)\n        \n        out = self.fc2(out)\n        out = self.bn2(out)\n        \n        out += identity  # æ®‹å·®è¿æ¥\n        out = F.relu(out)\n        \n        return out\n\n# 2. å¸¦æŠ•å½±çš„æ®‹å·®å—ï¼ˆç»´åº¦ä¸åŒï¼‰\nclass ResidualBlockWithProjection(nn.Module):\n    \"\"\"æ®‹å·®å—ï¼Œæ”¯æŒè¾“å…¥è¾“å‡ºç»´åº¦ä¸åŒ\"\"\"\n    def __init__(self, in_dim, out_dim):\n        super(ResidualBlockWithProjection, self).__init__()\n        self.fc1 = nn.Linear(in_dim, out_dim)\n        self.bn1 = nn.BatchNorm1d(out_dim)\n        self.fc2 = nn.Linear(out_dim, out_dim)\n        self.bn2 = nn.BatchNorm1d(out_dim)\n        \n        # æŠ•å½±å±‚ï¼Œç”¨äºç»´åº¦åŒ¹é…\n        self.projection = nn.Linear(in_dim, out_dim) if in_dim != out_dim else nn.Identity()\n    \n    def forward(self, x):\n        identity = self.projection(x)  # æŠ•å½±åˆ°ç›¸åŒç»´åº¦\n        \n        out = self.fc1(x)\n        out = self.bn1(out)\n        out = F.relu(out)\n        \n        out = self.fc2(out)\n        out = self.bn2(out)\n        \n        out += identity  # æ®‹å·®è¿æ¥\n        out = F.relu(out)\n        \n        return out\n\n# 3. Transformer ä¸­çš„æ®‹å·®é“¾æ¥\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformer å—ï¼ŒåŒ…å«å¤šå¤´æ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œï¼Œéƒ½æœ‰æ®‹å·®é“¾æ¥\"\"\"\n    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n        super(TransformerBlock, self).__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, dim_feedforward),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim_feedforward, d_model)\n        )\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        # å¤šå¤´æ³¨æ„åŠ› + æ®‹å·®é“¾æ¥ + LayerNorm\n        attn_out, _ = self.self_attn(x, x, x)\n        x = self.norm1(x + self.dropout(attn_out))\n        \n        # å‰é¦ˆç½‘ç»œ + æ®‹å·®é“¾æ¥ + LayerNorm\n        ffn_out = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_out))\n        \n        return x\n\n# 4. ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    # åŸºç¡€æ®‹å·®å—\n    block1 = ResidualBlock(dim=512)\n    x1 = torch.randn(32, 512)  # batch_size=32, dim=512\n    out1 = block1(x1)\n    print(f\"åŸºç¡€æ®‹å·®å—è¾“å‡ºå½¢çŠ¶: {out1.shape}\")\n    \n    # å¸¦æŠ•å½±çš„æ®‹å·®å—\n    block2 = ResidualBlockWithProjection(in_dim=256, out_dim=512)\n    x2 = torch.randn(32, 256)\n    out2 = block2(x2)\n    print(f\"æŠ•å½±æ®‹å·®å—è¾“å‡ºå½¢çŠ¶: {out2.shape}\")\n    \n    # Transformer å—\n    transformer = TransformerBlock(d_model=512, nhead=8, dim_feedforward=2048)\n    x3 = torch.randn(10, 32, 512)  # seq_len=10, batch_size=32, d_model=512\n    out3 = transformer(x3)\n    print(f\"Transformer å—è¾“å‡ºå½¢çŠ¶: {out3.shape}\")"
        }
      ]
    }
  ]
}