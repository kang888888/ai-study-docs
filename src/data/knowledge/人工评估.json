{
  "title": "äººå·¥è¯„ä¼°",
  "subtitle": "å•ç‚¹è¯„ä¼°ã€å¯¹æ¯”è¯„ä¼°ã€å¤šç»´é¢æ¿ç­‰äººå·¥è¯„ä¼°æ–¹æ³•ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "äººå·¥è¯„ä¼°æµç¨‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "flow",
                "title": "äººå·¥è¯„ä¼°æµç¨‹",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "äººå·¥è¯„ä¼°æ–¹æ³•",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "comparison",
                "title": "äººå·¥è¯„ä¼°æ–¹æ³•",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "å•ç‚¹è¯„ä¼°ç³»ç»Ÿ",
          "language": "python",
          "code": "class SinglePointEvaluation:\n    \"\"\"å•ç‚¹è¯„ä¼°ï¼šå¯¹å•ä¸ªå›ç­”è¿›è¡Œå¤šç»´åº¦è¯„åˆ†\"\"\"\n    def __init__(self):\n        self.dimensions = [\n            'usefulness',  # æœ‰ç”¨æ€§\n            'accuracy',    # å‡†ç¡®æ€§\n            'relevance',   # ç›¸å…³æ€§\n            'fluency',     # æµç•…æ€§\n            'safety'       # å®‰å…¨æ€§\n        ]\n    \n    def evaluate(self, response, question, context=None):\n        \"\"\"è¯„ä¼°å•ä¸ªå›ç­”\"\"\"\n        scores = {}\n        \n        for dimension in self.dimensions:\n            # è¿™é‡Œåº”è¯¥æ˜¯äººå·¥è¯„åˆ†ï¼Œç¤ºä¾‹ä¸­ä½¿ç”¨æ¨¡æ‹Ÿè¯„åˆ†\n            score = self._get_human_score(response, question, dimension)\n            scores[dimension] = score\n        \n        return scores\n    \n    def _get_human_score(self, response, question, dimension):\n        \"\"\"è·å–äººå·¥è¯„åˆ†ï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥è°ƒç”¨æ ‡æ³¨æ¥å£ï¼‰\"\"\"\n        # æ¨¡æ‹Ÿè¯„åˆ†é€»è¾‘\n        return 0.85  # å®é™…åº”è¯¥ä»æ ‡æ³¨ç³»ç»Ÿè·å–\n\n# ä½¿ç”¨ç¤ºä¾‹\nevaluator = SinglePointEvaluation()\nquestion = \"What is the capital of France?\"\nresponse = \"The capital of France is Paris.\"\nscores = evaluator.evaluate(response, question)\nprint(scores)"
        },
        {
          "type": "code-box",
          "title": "å¯¹æ¯”è¯„ä¼°ç³»ç»Ÿ",
          "language": "python",
          "code": "class ComparativeEvaluation:\n    \"\"\"å¯¹æ¯”è¯„ä¼°ï¼šæ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„å›ç­”\"\"\"\n    def __init__(self):\n        self.comparisons = []\n    \n    def compare_responses(self, responses, question):\n        \"\"\"å¯¹æ¯”å¤šä¸ªå›ç­”\"\"\"\n        # responses: [{'model': 'model1', 'response': '...'}, ...]\n        \n        # è·å–äººå·¥å¯¹æ¯”è¯„åˆ†\n        ranking = self._get_human_ranking(responses, question)\n        \n        return {\n            'question': question,\n            'ranking': ranking,\n            'best_model': ranking[0]['model'] if ranking else None\n        }\n    \n    def _get_human_ranking(self, responses, question):\n        \"\"\"è·å–äººå·¥æ’åºç»“æœï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥è°ƒç”¨æ ‡æ³¨æ¥å£ï¼‰\"\"\"\n        # æ¨¡æ‹Ÿæ’åºé€»è¾‘\n        # å®é™…åº”è¯¥ä»æ ‡æ³¨ç³»ç»Ÿè·å–äººå·¥æ’åºç»“æœ\n        return sorted(responses, key=lambda x: x.get('score', 0.5), reverse=True)\n\n# ä½¿ç”¨ç¤ºä¾‹\nevaluator = ComparativeEvaluation()\nquestion = \"Explain quantum computing\"\nresponses = [\n    {'model': 'GPT-4', 'response': '...'},\n    {'model': 'Claude', 'response': '...'},\n    {'model': 'LLaMA', 'response': '...'}\n]\nresult = evaluator.compare_responses(responses, question)\nprint(f\"Best model: {result['best_model']}\")"
        }
      ]
    }
  ]
}
