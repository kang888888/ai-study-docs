{
  "title": "GPTQï¼šæ¢¯åº¦é©±åŠ¨çš„åè®­ç»ƒ 4bit é‡åŒ–",
  "subtitle": "é€šè¿‡æœ€å°äºŒä¹˜ + æ¢¯åº¦æ ¡æ­£çš„æ–¹å¼åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°é«˜ç²¾åº¦ 4bit æƒé‡é‡åŒ–ï¼Œè¢«å¹¿æ³›ç”¨äº LLaMA/OPT å®¶æ—ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "GPTQæ˜¯åè®­ç»ƒé‡åŒ–æ–¹æ³•ï¼Œé€šè¿‡é€å±‚é‡åŒ–ä¼˜åŒ–ï¼Œå®ç°é«˜ç²¾åº¦çš„INT4é‡åŒ–ï¼Œå¹¿æ³›ç”¨äºLLMé‡åŒ–ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "é«˜ç²¾åº¦ï¼šæ¥è¿‘FP16çš„é‡åŒ–ç²¾åº¦",
            "å¹¿æ³›æ”¯æŒï¼šæ”¯æŒå¤šç§æ¨¡å‹æ¶æ„",
            "æ¨ç†åŠ é€Ÿï¼šæ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦",
            "æ˜“äºä½¿ç”¨ï¼šç®€å•é…ç½®å³å¯é‡åŒ–",
            "ç”Ÿæ€ä¸°å¯Œï¼šå¤šç§å·¥å…·æ”¯æŒ"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "é€å±‚é‡åŒ–ã€Hessianä¼˜åŒ–ã€INT4é‡åŒ–ã€é‡åŒ–ç²¾åº¦"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "æ¨¡å‹é‡åŒ–ã€æ¨ç†åŠ é€Ÿã€INT4é‡åŒ–ã€æ¨¡å‹å‹ç¼©"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æµç¨‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "flow",
                "title": "æµç¨‹"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "è¯¯å·®è¡¥å¿",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "è¯¯å·®è¡¥å¿"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "ç²¾åº¦ vs æ¨ç†é€Ÿåº¦",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "ç²¾åº¦ vs æ¨ç†é€Ÿåº¦"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "æœ€å°äºŒä¹˜é‡åŒ–",
          "formulas": [
            {
              "display": "\\hat{w} = \\arg\\min_{q} (w - q)^T H (w - q)"
            },
            {
              "text": "å…¶ä¸­ $H$ æ˜¯ Hessian è¿‘ä¼¼ï¼Œé€šè¿‡æ¢¯åº¦ç§¯ç´¯æˆ–è¿‘ä¼¼ Fisher ä¿¡æ¯è·å¾—ã€‚",
              "inline": "H"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "è¯¯å·®å›ä¼ ",
          "formulas": [
            {
              "text": "é‡åŒ–ç¬¬ i åˆ—åæ›´æ–°å‰©ä½™åˆ—ï¼š"
            },
            {
              "display": "W_{j} \\leftarrow W_{j} - \\frac{H_{ji}}{H_{ii}} (w_i - \\hat{w}_i)"
            },
            {
              "text": "é¿å…è¯¯å·®é›†ä¸­ï¼Œæå‡æ•´ä½“ç²¾åº¦ã€‚"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ AutoGPTQ å¯¼å‡º 4bit æ¨¡å‹",
          "language": "python",
          "code": "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom transformers import AutoTokenizer\n\nmodel_name = \"meta-llama/Llama-2-13b-hf\"\nquant_config = BaseQuantizeConfig(\n    bits=4,\n    group_size=128,\n    damp_percent=0.01,\n    desc_act=False\n)\n\nmodel = AutoGPTQForCausalLM.from_pretrained(\n    model_name,\n    quantize_config=quant_config\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\nmodel.quantize(dataset=\"c4\", batch_size=16, cache_examples_on_gpu=False)\nmodel.save_quantized(\"./llama2-13b-gptq\", use_safetensors=True)"
        }
      ]
    }
  ]
}