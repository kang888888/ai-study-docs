{
  "title": "GPTQï¼šæ¢¯åº¦é©±åŠ¨çš„åè®­ç»ƒ 4bit é‡åŒ–",
  "subtitle": "é€šè¿‡æœ€å°äºŒä¹˜ + æ¢¯åº¦æ ¡æ­£çš„æ–¹å¼åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°é«˜ç²¾åº¦ 4bit æƒé‡é‡åŒ–ï¼Œè¢«å¹¿æ³›ç”¨äº LLaMA/OPT å®¶æ—ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“Š å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æµç¨‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "flow",
                "title": "æµç¨‹"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "è¯¯å·®è¡¥å¿",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "è¯¯å·®è¡¥å¿"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "ç²¾åº¦ vs æ¨ç†é€Ÿåº¦",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "ç²¾åº¦ vs æ¨ç†é€Ÿåº¦"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "æœ€å°äºŒä¹˜é‡åŒ–",
          "formulas": [
            {
              "display": "\\hat{w} = \\arg\\min_{q} (w - q)^T H (w - q)"
            },
            {
              "text": "å…¶ä¸­ $H$ æ˜¯ Hessian è¿‘ä¼¼ï¼Œé€šè¿‡æ¢¯åº¦ç§¯ç´¯æˆ–è¿‘ä¼¼ Fisher ä¿¡æ¯è·å¾—ã€‚",
              "inline": "H"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "è¯¯å·®å›ä¼ ",
          "formulas": [
            {
              "text": "é‡åŒ–ç¬¬ i åˆ—åæ›´æ–°å‰©ä½™åˆ—ï¼š"
            },
            {
              "display": "W_{j} \\leftarrow W_{j} - \\frac{H_{ji}}{H_{ii}} (w_i - \\hat{w}_i)"
            },
            {
              "text": "é¿å…è¯¯å·®é›†ä¸­ï¼Œæå‡æ•´ä½“ç²¾åº¦ã€‚"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ AutoGPTQ å¯¼å‡º 4bit æ¨¡å‹",
          "language": "python",
          "code": "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom transformers import AutoTokenizer\n\nmodel_name = \"meta-llama/Llama-2-13b-hf\"\nquant_config = BaseQuantizeConfig(\n    bits=4,\n    group_size=128,\n    damp_percent=0.01,\n    desc_act=False\n)\n\nmodel = AutoGPTQForCausalLM.from_pretrained(\n    model_name,\n    quantize_config=quant_config\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\nmodel.quantize(dataset=\"c4\", batch_size=16, cache_examples_on_gpu=False)\nmodel.save_quantized(\"./llama2-13b-gptq\", use_safetensors=True)"
        }
      ]
    }
  ]
}