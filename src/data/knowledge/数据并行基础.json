{
  "title": "æ•°æ®å¹¶è¡ŒåŸºç¡€",
  "subtitle": "ç†è§£AllReduceã€æ¢¯åº¦åŒæ­¥ã€æ•°æ®åˆ†ç‰‡ç­‰æ•°æ®å¹¶è¡Œçš„æ ¸å¿ƒåŸç†ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æ•°æ®å¹¶è¡ŒåŸç†",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "æ•°æ®å¹¶è¡ŒåŸç†",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "AllReduceæ“ä½œ",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "flow",
                "title": "AllReduceæ“ä½œ",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "AllReduceæ¢¯åº¦åŒæ­¥",
          "language": "python",
          "code": "import torch\nimport torch.distributed as dist\n\ndef allreduce_gradients(model, world_size):\n    \"\"\"æ‰‹åŠ¨å®ç°AllReduceæ¢¯åº¦åŒæ­¥\"\"\"\n    for param in model.parameters():\n        if param.grad is not None:\n            # å°†æ‰€æœ‰è®¾å¤‡çš„æ¢¯åº¦æ±‚å’Œ\n            dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)\n            # è®¡ç®—å¹³å‡æ¢¯åº¦\n            param.grad.data /= world_size"
        },
        {
          "type": "code-box",
          "title": "æ•°æ®åˆ†ç‰‡",
          "language": "python",
          "code": "from torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\n\nclass ShardedDataset(Dataset):\n    def __init__(self, data, rank, world_size):\n        self.data = data\n        self.rank = rank\n        self.world_size = world_size\n        # æ¯ä¸ªè¿›ç¨‹åªå¤„ç†éƒ¨åˆ†æ•°æ®\n        self.shard_size = len(data) // world_size\n        self.start_idx = rank * self.shard_size\n        self.end_idx = self.start_idx + self.shard_size\n    \n    def __len__(self):\n        return self.shard_size\n    \n    def __getitem__(self, idx):\n        global_idx = self.start_idx + idx\n        return self.data[global_idx]\n\n# ä½¿ç”¨DistributedSampler\nsampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\ndataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)"
        }
      ]
    }
  ]
}
