{
  "title": "ViT (Vision Transformer) è§†è§‰Transformer",
  "subtitle": "å°†Transformeråº”ç”¨äºè®¡ç®—æœºè§†è§‰",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "Googleæå‡ºçš„å°†Transformeråº”ç”¨äºè®¡ç®—æœºè§†è§‰çš„æ¶æ„ã€‚å°†å›¾åƒåˆ‡åˆ†æˆå›ºå®šå¤§å°çš„Patchï¼Œç„¶åä½œä¸ºåºåˆ—è¾“å…¥Transformerï¼Œè¯æ˜äº†çº¯Attentionæœºåˆ¶ä¹Ÿèƒ½åœ¨è§†è§‰ä»»åŠ¡ä¸Šè¾¾åˆ°SOTAã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "Patch Embeddingï¼šå°†å›¾åƒåˆ‡åˆ†ä¸º16Ã—16çš„Patchï¼Œå±•å¹³åä½œä¸ºToken",
            "å…¨å±€æ„Ÿå—é‡ï¼šæ¯ä¸ªPatchéƒ½èƒ½å…³æ³¨åˆ°æ•´å¼ å›¾åƒçš„æ‰€æœ‰å…¶ä»–Patch",
            "æ•°æ®é¥¥æ¸´ï¼šåœ¨å°æ•°æ®é›†ä¸Šè¡¨ç°ä¸å¦‚CNNï¼Œéœ€è¦å¤§è§„æ¨¡é¢„è®­ç»ƒ",
            "Swin Transformerï¼šé€šè¿‡ç§»åŠ¨çª—å£å®ç°åˆ†å±‚ç»“æ„ï¼Œé™ä½å¤æ‚åº¦",
            "MAEé¢„è®­ç»ƒï¼šé€šè¿‡æ©ç è‡ªç¼–ç å™¨è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒ"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "Patch Embeddingã€Position Embeddingã€[CLS] Tokenã€Shifted Windowï¼ˆSwinï¼‰"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ï¼ˆDETRï¼‰ã€å›¾åƒåˆ†å‰²ã€è§†é¢‘ç†è§£"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "ViTDiagram",
              "caption": "ViT Patchå¯è§†åŒ–",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "ViT Patchå¯è§†åŒ–"
              }
            },
            {
              "type": "svg-d3",
              "component": "ViTDiagram",
              "caption": "ViTç»„ä»¶è¯¦è§£",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "ViTç»„ä»¶è¯¦è§£"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "Patch Embedding",
          "formulas": [
            {
              "text": "å°†å›¾åƒåˆ‡åˆ†ä¸º $P \\times P$ çš„patchï¼Œæ¯ä¸ªpatchå±•å¹³åé€šè¿‡çº¿æ€§æŠ•å½±ï¼š",
              "inline": "P \\times P"
            },
            {
              "display": "z_0 = [x_{class}; x_p^1 E; x_p^2 E; \\ldots; x_p^N E] + E_{pos}"
            },
            {
              "text": "å…¶ä¸­ï¼š"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Self-Attention",
          "formulas": [
            {
              "text": "ViTä½¿ç”¨æ ‡å‡†çš„Multi-Head Self-Attentionï¼š"
            },
            {
              "display": "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V"
            },
            {
              "text": "æ¯ä¸ªpatchéƒ½èƒ½å…³æ³¨åˆ°å›¾åƒçš„æ‰€æœ‰å…¶ä»–patchï¼Œå®ç°å…¨å±€æ„Ÿå—é‡"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ PyTorch å®ç° ViT æ ¸å¿ƒç»„ä»¶",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass PatchEmbedding(nn.Module):\n    \"\"\"Patch Embeddingå±‚\"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n        super(PatchEmbedding, self).__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.n_patches = (img_size // patch_size) ** 2\n        \n        self.proj = nn.Conv2d(in_channels, embed_dim, \n                              kernel_size=patch_size, stride=patch_size)\n    \n    def forward(self, x):\n        # x: [B, C, H, W]\n        x = self.proj(x)  # [B, embed_dim, H', W']\n        x = x.flatten(2)  # [B, embed_dim, n_patches]\n        x = x.transpose(1, 2)  # [B, n_patches, embed_dim]\n        return x\n\nclass VisionTransformer(nn.Module):\n    \"\"\"Vision Transformeræ¨¡å‹\"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_channels=3, \n                 num_classes=1000, embed_dim=768, depth=12, num_heads=12,\n                 mlp_ratio=4.0, dropout=0.1):\n        super(VisionTransformer, self).__init__()\n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n        num_patches = self.patch_embed.n_patches\n        \n        # åˆ†ç±»token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        \n        # ä½ç½®ç¼–ç \n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        \n        # Transformer Encoder\n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n            for _ in range(depth)\n        ])\n        \n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes)\n        self.dropout = nn.Dropout(dropout)\n        \n        # åˆå§‹åŒ–\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n    \n    def forward(self, x):\n        B = x.shape[0]\n        \n        # Patch embedding\n        x = self.patch_embed(x)  # [B, n_patches, embed_dim]\n        \n        # æ·»åŠ åˆ†ç±»token\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat([cls_tokens, x], dim=1)  # [B, n_patches+1, embed_dim]\n        \n        # æ·»åŠ ä½ç½®ç¼–ç \n        x = x + self.pos_embed\n        x = self.dropout(x)\n        \n        # Transformer blocks\n        for block in self.blocks:\n            x = block(x)\n        \n        # ä½¿ç”¨åˆ†ç±»tokençš„è¾“å‡º\n        x = self.norm(x)\n        cls_token_final = x[:, 0]\n        \n        # åˆ†ç±»å¤´\n        x = self.head(cls_token_final)\n        \n        return x\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformerç¼–ç å™¨å—\"\"\"\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n        super(TransformerBlock, self).__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_hidden_dim, embed_dim),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, x):\n        # Self-attention\n        x_norm = self.norm1(x)\n        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n        x = x + attn_out\n        \n        # MLP\n        x = x + self.mlp(self.norm2(x))\n        \n        return x\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    model = VisionTransformer(\n        img_size=224,\n        patch_size=16,\n        num_classes=1000,\n        embed_dim=768,\n        depth=12,\n        num_heads=12\n    )\n    \n    # æ¨¡æ‹Ÿè¾“å…¥\n    x = torch.randn(4, 3, 224, 224)\n    output = model(x)\n    print(f\"è¾“å‡ºå½¢çŠ¶: {output.shape}\")  # [4, 1000]"
        }
      ]
    }
  ]
}