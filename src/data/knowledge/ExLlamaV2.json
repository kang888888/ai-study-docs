{
  "title": "ExLlamaV2ï¼šé¢å‘ 4bit LLaMA çš„æè‡´æ¨ç†æ¡†æ¶",
  "subtitle": "ä¸“ä¸º GPTQ/AWQ æ¨¡å‹æ‰“é€ çš„é«˜æ€§èƒ½åç«¯ï¼Œä½¿ç”¨è‡ªç ” CUDA kernelã€KV cache ä¼˜åŒ–ä¸æµæ°´çº¿å¹¶è¡Œï¼Œæ¨ç†é€Ÿåº¦é¢†å…ˆ vLLM/vCUDAã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "ExLlamaV2æ˜¯é«˜æ€§èƒ½LLMæ¨ç†å¼•æ“ï¼Œæ”¯æŒå¤šç§é‡åŒ–æ ¼å¼ï¼Œå®ç°å¿«é€Ÿæ¨ç†ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "é«˜æ€§èƒ½ï¼šä¼˜åŒ–çš„æ¨ç†å¼•æ“",
            "å¤šæ ¼å¼æ”¯æŒï¼šæ”¯æŒå¤šç§é‡åŒ–æ ¼å¼",
            "å†…å­˜é«˜æ•ˆï¼šé«˜æ•ˆçš„å†…å­˜ç®¡ç†",
            "æ˜“äºä½¿ç”¨ï¼šç®€å•çš„APIæ¥å£",
            "æŒç»­ä¼˜åŒ–ï¼šä¸æ–­æ”¹è¿›æ€§èƒ½"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "æ¨ç†å¼•æ“ã€é‡åŒ–æ”¯æŒã€å†…å­˜ä¼˜åŒ–ã€æ€§èƒ½ä¼˜åŒ–"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "é«˜æ€§èƒ½æ¨ç†ã€é‡åŒ–æ¨ç†ã€æœ¬åœ°éƒ¨ç½²ã€æ¨ç†å¼•æ“"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æ¶æ„",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "æ¶æ„"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æ€§èƒ½å¯¹æ¯”",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "æ€§èƒ½å¯¹æ¯”"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "ç¼“å­˜ç­–ç•¥",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "ç¼“å­˜ç­–ç•¥"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦/æ€§èƒ½æ¨¡å‹",
      "content": [
        {
          "type": "math-box",
          "title": "ååä¼°ç®—",
          "formulas": [
            {
              "display": "TPS \\approx \\frac{B \\times H \\times d_{model}}{t_{kernel} + t_{io}}"
            },
            {
              "text": "ExLlamaV2 é€šè¿‡å‡å°‘ $t_{io}$ï¼ˆå°‘è§£é‡åŒ–ï¼‰å’Œä¼˜åŒ– $t_{kernel}$ è·å¾—æ›´é«˜ TPSã€‚",
              "inline": "t_{io}"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "KV Cache å†…å­˜",
          "formulas": [
            {
              "display": "\\text{Mem} = 2 \\times L \\times H \\times d_{head} \\times bytes_{dtype}"
            },
            {
              "text": "Paged Cache å°† $L$ åˆ‡å—ï¼Œå¹¶å¤ç”¨é‡Šæ”¾çš„å—å‡å° MEM å³°å€¼ã€‚",
              "inline": "L"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "Python å¿«é€Ÿæ¨ç†",
          "language": "python",
          "code": "from exllamav2 import ExLlamaV2, ExLlamaV2Config, ExLlamaV2Tokenizer\n\nconfig = ExLlamaV2Config(\"./llama-2-13b-gptq\")\nmodel = ExLlamaV2(config)\nmodel.load_autosplit()\n\ntokenizer = ExLlamaV2Tokenizer(config)\nprompt = \"### ç”¨æˆ·: è§£é‡Š ExLlamaV2 çš„ä¼˜åŠ¿\\n### åŠ©æ‰‹:\"\noutput = model.generate(\n    tokenizer.encode(prompt),\n    max_new_tokens=256,\n    temperature=0.7,\n    top_p=0.9\n)\nprint(tokenizer.decode(output))"
        }
      ]
    }
  ]
}