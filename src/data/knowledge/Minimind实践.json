{
  "title": "Minimindï¼šä»é›¶è®­ç»ƒGPTå®è·µ",
  "subtitle": "åœ¨2å°æ—¶å†…ä»é›¶å¼€å§‹è®­ç»ƒä¸€ä¸ª2600ä¸‡å‚æ•°çš„å°å‹GPTæ¨¡å‹ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "Minimindå®è·µæ¶æ„",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Minimindå®è·µæ¶æ„",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "Minimindå®è·µæµç¨‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Minimindå®è·µæµç¨‹",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "GPTæ¨¡å‹å®ç°ç¤ºä¾‹",
          "language": "python",
          "code": "class GPTModel(nn.Module):\n    def __init__(self, vocab_size, n_layer, n_head, n_embd):\n        # åµŒå…¥å±‚\n        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding = nn.Embedding(block_size, n_embd)\n        \n        # Transformerå—\n        self.blocks = nn.ModuleList([\n            TransformerBlock(n_embd, n_head) \n            for _ in range(n_layer)\n        ])\n        \n        # è¾“å‡ºå±‚\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n    \n    def forward(self, idx):\n        # å‰å‘ä¼ æ’­\n        B, T = idx.shape\n        tok_emb = self.token_embedding(idx)\n        pos_emb = self.position_embedding(torch.arange(T))\n        x = tok_emb + pos_emb\n        \n        for block in self.blocks:\n            x = block(x)\n        \n        logits = self.lm_head(x)\n        return logits"
        }
      ]
    }
  ]
}