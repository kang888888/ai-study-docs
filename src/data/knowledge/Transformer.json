{
  "title": "Transformer",
  "subtitle": "åŸºäºSelf-Attentionæœºåˆ¶çš„é©å‘½æ€§æ¶æ„",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "åŸºäºSelf-Attentionæœºåˆ¶çš„é©å‘½æ€§æ¶æ„ï¼Œå®Œå…¨æ‘’å¼ƒäº†å¾ªç¯å’Œå·ç§¯ç»“æ„ã€‚é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ç›´æ¥å»ºæ¨¡åºåˆ—ä¸­ä»»æ„ä¸¤ä¸ªä½ç½®çš„å…³ç³»ï¼Œæ˜¯å½“å‰æ‰€æœ‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆGPTã€BERTã€LLaMAï¼‰çš„åŸºçŸ³ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "Self-Attentionï¼šç›´æ¥è®¡ç®—åºåˆ—ä¸­ä»»æ„ä½ç½®ä¹‹é—´çš„å…³ç³»ï¼ŒO(nÂ²)å¤æ‚åº¦",
            "å¹¶è¡Œè®¡ç®—ï¼šæ‰€æœ‰ä½ç½®åŒæ—¶è®¡ç®—ï¼Œè®­ç»ƒé€Ÿåº¦è¿œè¶…RNN/LSTM",
            "ä½ç½®ç¼–ç ï¼šé€šè¿‡æ­£å¼¦/ä½™å¼¦æˆ–å¯å­¦ä¹ çš„ä½ç½®ç¼–ç ä¿ç•™åºåˆ—é¡ºåºä¿¡æ¯",
            "å¤šå¤´æ³¨æ„åŠ›ï¼šä»å¤šä¸ªå­ç©ºé—´æ•æ‰ä¸åŒçš„è¯­ä¹‰å…³ç³»",
            "Encoder-Decoderç»“æ„ï¼šç¼–ç å™¨ç†è§£è¾“å…¥ï¼Œè§£ç å™¨ç”Ÿæˆè¾“å‡º"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "Multi-Head Attentionã€ä½ç½®ç¼–ç ã€æ®‹å·®è¿æ¥ã€Layer Normalizationã€Feed-Forward Network"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€å¤§è¯­è¨€æ¨¡å‹ï¼ˆGPT/BERTï¼‰ã€å›¾åƒåˆ†ç±»ï¼ˆViTï¼‰ã€è¯­éŸ³è¯†åˆ«"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "TransformerDiagram",
              "caption": "Transformer æ¶æ„åŠ¨æ€å›¾è§£ï¼ˆäº¤äº’å¼ SVG + D3.jsï¼‰",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Transformer æ¶æ„åŠ¨æ€å›¾è§£ï¼ˆäº¤äº’å¼ SVG + D3.jsï¼‰"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "Scaled Dot-Product Attention",
          "formulas": [
            {
              "text": "æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒå…¬å¼ï¼š"
            },
            {
              "display": "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V"
            },
            {
              "text": "å…¶ä¸­ï¼š"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Multi-Head Attention",
          "formulas": [
            {
              "text": "å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼š"
            },
            {
              "display": "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O"
            },
            {
              "display": "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)"
            },
            {
              "text": "å…¶ä¸­ $h$ æ˜¯æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œæ¯ä¸ªå¤´æœ‰ç‹¬ç«‹çš„æƒé‡çŸ©é˜µ $W_i^Q, W_i^K, W_i^V$",
              "inline": "h"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰",
          "formulas": [
            {
              "text": "æ­£å¼¦ä½ç½®ç¼–ç ï¼š"
            },
            {
              "display": "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)"
            },
            {
              "display": "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)"
            },
            {
              "text": "å…¶ä¸­ $pos$ æ˜¯ä½ç½®ï¼Œ$i$ æ˜¯ç»´åº¦ç´¢å¼•ï¼Œ$d_{model}$ æ˜¯æ¨¡å‹ç»´åº¦",
              "inline": "pos"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Feed-Forward Network",
          "formulas": [
            {
              "text": "å‰é¦ˆç½‘ç»œï¼š"
            },
            {
              "display": "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2"
            },
            {
              "text": "é€šå¸¸ $d_{ff} = 4 \\times d_{model}$",
              "inline": "d_{ff} = 4 \\times d_{model}"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ PyTorch å®ç° Transformer",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶\"\"\"\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        # çº¿æ€§å˜æ¢å±‚\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n    \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        \"\"\"ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›\"\"\"\n        # Q, K, V shape: (batch_size, num_heads, seq_len, d_k)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        attention_weights = F.softmax(scores, dim=-1)\n        output = torch.matmul(attention_weights, V)\n        \n        return output, attention_weights\n    \n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n        \n        # çº¿æ€§å˜æ¢å¹¶é‡å¡‘ä¸ºå¤šå¤´\n        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n        # æ³¨æ„åŠ›è®¡ç®—\n        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        # æ‹¼æ¥å¤šå¤´\n        attention_output = attention_output.transpose(1, 2).contiguous().view(\n            batch_size, -1, self.d_model\n        )\n        \n        # è¾“å‡ºæŠ•å½±\n        output = self.W_o(attention_output)\n        \n        return output\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"ä½ç½®ç¼–ç \"\"\"\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        \n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        # x shape: (seq_len, batch_size, d_model)\n        x = x + self.pe[:x.size(0), :]\n        return x\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformer ç¼–ç å™¨å—\"\"\"\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super(TransformerBlock, self).__init__()\n        \n        self.attention = MultiHeadAttention(d_model, num_heads)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        self.feed_forward = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Linear(d_ff, d_model)\n        )\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥\n        attn_output = self.attention(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        \n        return x\n\nclass Transformer(nn.Module):\n    \"\"\"å®Œæ•´çš„ Transformer æ¨¡å‹\"\"\"\n    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len=5000, dropout=0.1):\n        super(Transformer, self).__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model, max_len)\n        \n        self.layers = nn.ModuleList([\n            TransformerBlock(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_layers)\n        ])\n        \n        self.dropout = nn.Dropout(dropout)\n        self.fc_out = nn.Linear(d_model, vocab_size)\n    \n    def forward(self, x, mask=None):\n        # è¯åµŒå…¥\n        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n        x = x.transpose(0, 1)  # (seq_len, batch_size, d_model)\n        \n        # ä½ç½®ç¼–ç \n        x = self.pos_encoding(x)\n        x = self.dropout(x)\n        \n        # Transformer å±‚\n        for layer in self.layers:\n            x = layer(x, mask)\n        \n        # è½¬å› (batch_size, seq_len, d_model)\n        x = x.transpose(0, 1)\n        \n        # è¾“å‡ºå±‚\n        output = self.fc_out(x)\n        \n        return output\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    # åˆ›å»ºæ¨¡å‹\n    model = Transformer(\n        vocab_size=10000,\n        d_model=512,\n        num_heads=8,\n        num_layers=6,\n        d_ff=2048\n    )\n    \n    # æ¨¡æ‹Ÿè¾“å…¥ (batch_size=32, seq_len=50)\n    x = torch.randint(0, 10000, (32, 50))\n    \n    # å‰å‘ä¼ æ’­\n    output = model(x)\n    print(f\"è¾“å‡ºå½¢çŠ¶: {output.shape}\")  # [32, 50, 10000]"
        },
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ NumPy æ‰‹åŠ¨å®ç°æ³¨æ„åŠ›æœºåˆ¶",
          "language": "python",
          "code": "import numpy as np\n\ndef scaled_dot_product_attention(Q, K, V, mask=None):\n    \"\"\"\n    ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›\n    \n    å‚æ•°:\n        Q: æŸ¥è¯¢çŸ©é˜µ (..., seq_len_q, d_k)\n        K: é”®çŸ©é˜µ (..., seq_len_k, d_k)\n        V: å€¼çŸ©é˜µ (..., seq_len_v, d_v)\n        mask: æ©ç çŸ©é˜µ\n    \"\"\"\n    d_k = Q.shape[-1]\n    \n    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n    scores = np.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n    \n    # åº”ç”¨æ©ç \n    if mask is not None:\n        scores = np.where(mask == 0, -1e9, scores)\n    \n    # Softmax\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n    \n    # åŠ æƒæ±‚å’Œ\n    output = np.matmul(attention_weights, V)\n    \n    return output, attention_weights\n\ndef positional_encoding(max_len, d_model):\n    \"\"\"ç”Ÿæˆä½ç½®ç¼–ç \"\"\"\n    pe = np.zeros((max_len, d_model))\n    \n    for pos in range(max_len):\n        for i in range(0, d_model, 2):\n            pe[pos, i] = np.sin(pos / (10000 ** (2 * i / d_model)))\n            if i + 1 < d_model:\n                pe[pos, i + 1] = np.cos(pos / (10000 ** (2 * i / d_model)))\n    \n    return pe\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    # åˆ›å»º Q, K, V\n    batch_size, seq_len, d_k = 2, 10, 64\n    Q = np.random.randn(batch_size, seq_len, d_k)\n    K = np.random.randn(batch_size, seq_len, d_k)\n    V = np.random.randn(batch_size, seq_len, d_k)\n    \n    # è®¡ç®—æ³¨æ„åŠ›\n    output, attention_weights = scaled_dot_product_attention(Q, K, V)\n    print(f\"æ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶: {output.shape}\")  # (2, 10, 64)\n    print(f\"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}\")  # (2, 10, 10)\n    \n    # ç”Ÿæˆä½ç½®ç¼–ç \n    pe = positional_encoding(max_len=100, d_model=512)\n    print(f\"ä½ç½®ç¼–ç å½¢çŠ¶: {pe.shape}\")  # (100, 512)"
        }
      ]
    }
  ]
}