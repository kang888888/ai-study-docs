{
  "title": "Titans ç¥ç»ç½‘ç»œæ¶æ„",
  "subtitle": "ä»¿ç”Ÿè®°å¿†æ¶æ„ï¼ŒèåˆçŸ­æœŸè®°å¿†ã€é•¿æœŸè®°å¿†å’Œæ³¨æ„åŠ›æœºåˆ¶",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "Titans æ˜¯ç”± Google Research åœ¨ 2025 å¹´ 1 æœˆå‘å¸ƒçš„æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ã€‚è¯¥æ¶æ„é‡‡ç”¨ä»¿ç”Ÿè®¾è®¡ï¼Œèåˆäº†çŸ­æœŸè®°å¿†ã€é•¿æœŸè®°å¿†å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿå¤„ç†è¶…è¿‡ 200 ä¸‡ä¸ª Token çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ ä¸‰ç§æ¶æ„å˜ä½“",
      "content": [
        {
          "type": "tech-box",
          "content": "MACï¼ˆMemory as a Contextï¼‰\n                    å°†é•¿æœŸè®°å¿†ä½œä¸ºä¸Šä¸‹æ–‡çš„ä¸€éƒ¨åˆ†ï¼Œå…è®¸æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€ç»“åˆå†å²ä¿¡æ¯ä¸å½“å‰æ•°æ®ã€‚ç®€å•ç›´æ¥ï¼Œæ˜“äºå®ç°ï¼Œé€‚åˆéœ€è¦é¢‘ç¹è®¿é—®å†å²ä¿¡æ¯çš„ä»»åŠ¡ã€‚"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "é•¿æ–‡æœ¬ç†è§£ï¼šæ–‡æ¡£åˆ†æã€ä¹¦ç±ç†è§£ã€æ³•å¾‹æ–‡æ¡£ã€æŠ€æœ¯æ–‡æ¡£å¤„ç†ï¼ˆ200ä¸‡+ Token ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼‰\n                    å¤šè½®å¯¹è¯ï¼šæ™ºèƒ½åŠ©æ‰‹ã€å®¢æœç³»ç»Ÿã€éœ€è¦é•¿æœŸè®°å¿†çš„å¯¹è¯ç³»ç»Ÿï¼ˆå†…ç½®é•¿æœŸè®°å¿†ï¼Œæ— éœ€å¤–éƒ¨è®°å¿†æ¨¡å—ï¼‰\n                    ä»£ç åˆ†æï¼šå¤§å‹ä»£ç åº“ç†è§£ã€è·¨æ–‡ä»¶çš„ä»£ç ä¾èµ–åˆ†æï¼ˆè¶…é•¿ä¸Šä¸‹æ–‡ï¼Œç†è§£ä»£ç ä¾èµ–å…³ç³»ï¼‰\n                    ç§‘å­¦è®¡ç®—ï¼šåŸºå› ç»„åºåˆ—åˆ†æã€æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆé•¿æœŸè®°å¿†ï¼Œè¯†åˆ«å†å²æ¨¡å¼ï¼‰"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "TitansDiagram",
              "caption": "Titansæ¶æ„å›¾",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Titansæ¶æ„å›¾",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "TitansDiagram",
              "caption": "Titansç»„ä»¶è¯¦è§£",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Titansç»„ä»¶è¯¦è§£",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ§  ä»¿ç”Ÿè®°å¿†ç³»ç»Ÿ",
      "content": [
        {
          "type": "tech-box",
          "content": "çŸ­æœŸè®°å¿†ï¼ˆShort-Term Memoryï¼‰\n                    å¿«é€Ÿååº”ï¼Œå¯¹å½“å‰è¾“å…¥å¿«é€Ÿå¤„ç†ï¼Œä¿å­˜æœ€è¿‘çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç±»ä¼¼ Transformer çš„æ³¨æ„åŠ›æœºåˆ¶"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ”¬ è®°å¿†æ¨¡å—è®¾è®¡",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "1. è®°å¿†ç¼–ç å™¨ï¼ˆMemory Encoderï¼‰ï¼šå°†å†å²ä¿¡æ¯ç¼–ç ä¸ºå‹ç¼©è¡¨ç¤ºï¼Œæ”¯æŒå¢é‡æ›´æ–°ï¼Œé«˜æ•ˆå­˜å‚¨å¤§é‡å†å²æ•°æ®\n                    2. è®°å¿†æ£€ç´¢å™¨ï¼ˆMemory Retrieverï¼‰ï¼šæ ¹æ®å½“å‰ä¸Šä¸‹æ–‡æ£€ç´¢ç›¸å…³è®°å¿†ï¼Œä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œæ£€ç´¢ï¼Œé€‰æ‹©æ€§æ£€ç´¢ç›¸å…³ä¿¡æ¯\n                    3. è®°å¿†æ›´æ–°å™¨ï¼ˆMemory Updaterï¼‰ï¼šé€‰æ‹©æ€§æ›´æ–°é•¿æœŸè®°å¿†ï¼Œé—å¿˜ä¸é‡è¦çš„ä¿¡æ¯ï¼Œä¿æŒè®°å¿†çš„æ—¶æ•ˆæ€§å’Œç›¸å…³æ€§"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’¡ æ€§èƒ½è¡¨ç°",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "è¯­è¨€å»ºæ¨¡ï¼šè¶…è¶Šä¼ ç»Ÿ Transformerï¼Œåœ¨é•¿åºåˆ—ä»»åŠ¡ä¸­è¡¨ç°å“è¶Š\n                    å¸¸è¯†æ¨ç†ï¼šåˆ©ç”¨é•¿æœŸè®°å¿†è¿›è¡Œå¤æ‚æ¨ç†ï¼Œä¿æŒæ¨ç†çš„è¿è´¯æ€§\n                    åŸºå› ç»„åˆ†æï¼šå¤„ç†è¶…é•¿ç”Ÿç‰©åºåˆ—ï¼Œè¯†åˆ«é•¿è·ç¦»ä¾èµ–å…³ç³»\n                    æ—¶é—´åºåˆ—é¢„æµ‹ï¼šåˆ©ç”¨å†å²æ¨¡å¼è¿›è¡Œé¢„æµ‹ï¼Œå¤„ç†é•¿æœŸä¾èµ–"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "é•¿æœŸè®°å¿†æ¨¡å—çš„ç®€åŒ–å®ç°",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LongTermMemory(nn.Module):\n    \"\"\"é•¿æœŸè®°å¿†æ¨¡å—\"\"\"\n    def __init__(self, d_model, memory_size):\n        super(LongTermMemory, self).__init__()\n        self.d_model = d_model\n        self.memory_size = memory_size\n        \n        # è®°å¿†ç¼–ç å™¨\n        self.memory_encoder = nn.Linear(d_model, d_model)\n        \n        # è®°å¿†å­˜å‚¨ï¼ˆå¯å­¦ä¹ çš„ï¼‰\n        self.memory = nn.Parameter(torch.randn(memory_size, d_model))\n        \n        # è®°å¿†æ£€ç´¢å™¨ï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰\n        self.query_proj = nn.Linear(d_model, d_model)\n        self.key_proj = nn.Linear(d_model, d_model)\n        self.value_proj = nn.Linear(d_model, d_model)\n        \n        # è®°å¿†æ›´æ–°å™¨ï¼ˆé—¨æ§æœºåˆ¶ï¼‰\n        self.update_gate = nn.Linear(d_model * 2, d_model)\n    \n    def encode(self, x):\n        \"\"\"ç¼–ç è¾“å…¥ä¸ºè®°å¿†è¡¨ç¤º\"\"\"\n        return self.memory_encoder(x)\n    \n    def retrieve(self, query):\n        \"\"\"æ£€ç´¢ç›¸å…³è®°å¿†\"\"\"\n        batch_size = query.shape[0]\n        \n        # è®¡ç®—æŸ¥è¯¢ã€é”®ã€å€¼\n        q = self.query_proj(query)  # [batch_size, d_model]\n        k = self.key_proj(self.memory)  # [memory_size, d_model]\n        v = self.value_proj(self.memory)  # [memory_size, d_model]\n        \n        # è®¡ç®—æ³¨æ„åŠ›æƒé‡\n        scores = torch.matmul(q, k.t()) / (self.d_model ** 0.5)\n        attention = F.softmax(scores, dim=-1)  # [batch_size, memory_size]\n        \n        # åŠ æƒæ±‚å’Œ\n        retrieved = torch.matmul(attention, v)  # [batch_size, d_model]\n        \n        return retrieved, attention\n    \n    def update(self, new_info, retrieved_memory):\n        \"\"\"æ›´æ–°è®°å¿†\"\"\"\n        # åˆå¹¶æ–°ä¿¡æ¯å’Œæ£€ç´¢åˆ°çš„è®°å¿†\n        combined = torch.cat([new_info, retrieved_memory], dim=-1)\n        \n        # é—¨æ§æ›´æ–°\n        gate = torch.sigmoid(self.update_gate(combined))\n        updated = gate * new_info + (1 - gate) * retrieved_memory\n        \n        return updated\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    memory = LongTermMemory(d_model=512, memory_size=1000)\n    query = torch.randn(2, 512)\n    new_info = torch.randn(2, 512)\n    \n    # æ£€ç´¢è®°å¿†\n    retrieved, attention = memory.retrieve(query)\n    print(f\"æ£€ç´¢åˆ°çš„è®°å¿†å½¢çŠ¶: {retrieved.shape}\")  # [2, 512]\n    \n    # æ›´æ–°è®°å¿†\n    updated = memory.update(new_info, retrieved)\n    print(f\"æ›´æ–°åçš„è®°å¿†å½¢çŠ¶: {updated.shape}\")  # [2, 512]"
        }
      ]
    }
  ]
}