{
  "title": "PEFTï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•æ—",
  "subtitle": "é€šè¿‡ Adapterã€Prefix-Tuningã€LoRAã€IA3ã€BitFit ç­‰æ–¹æ³•å†»ç»“å¤§éƒ¨åˆ†å‚æ•°ï¼Œä»…è®­ç»ƒå°å‹é™„åŠ æ¨¡å—ï¼Œå®ç°â€œä½èµ„æºå¯æ‰©å±•â€ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“Š å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æ–¹æ³•å®¶æ—",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "æ–¹æ³•å®¶æ—"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "Adapter ç»“æ„",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Adapter ç»“æ„"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "Prompt Tuning",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Prompt Tuning"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "Adapter",
          "formulas": [
            {
              "text": "Adapter åœ¨å±‚å†…æ·»åŠ ç“¶é¢ˆç»“æ„ï¼š"
            },
            {
              "display": "h' = h + W_{up} \\sigma(W_{down} h)"
            },
            {
              "text": "$W_{down} \\in \\mathbb{R}^{d \\times r}, W_{up} \\in \\mathbb{R}^{r \\times d}$ï¼Œä»…è®­ç»ƒè¿™ä¸¤å±‚ã€‚",
              "inline": "W_{down} \\in \\mathbb{R}^{d \\times r}, W_{up} \\in \\mathbb{R}^{r \\times d}"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Prefix/Prompt Tuning",
          "formulas": [
            {
              "text": "åœ¨å¤šå¤´æ³¨æ„åŠ›å‰æ³¨å…¥è™šæ‹Ÿ tokenï¼š"
            },
            {
              "display": "\\text{Attention}(Q, K, V) \\Rightarrow \\text{Attention}([Q; Q_p], [K; K_p], [V; V_p])"
            },
            {
              "text": "$Q_p,K_p,V_p$ ä¸ºå¯è®­ç»ƒå‰ç¼€å‘é‡ã€‚",
              "inline": "Q_p,K_p,V_p"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "PEFT ç»Ÿä¸€æ¥å£",
          "language": "python",
          "code": "from peft import (LoraConfig, PrefixTuningConfig, PromptTuningConfig,\n                   get_peft_model, TaskType)\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nbase_model = \"google/flan-t5-large\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(base_model)\n\nlora_cfg = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=[\"q\", \"v\"]\n)\nmodel = get_peft_model(model, lora_cfg)\n\n# ä¹Ÿå¯åˆ‡æ¢ä¸º Prefix Tuning\n# prefix_cfg = PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, num_virtual_tokens=30)\n# model = get_peft_model(model, prefix_cfg)"
        }
      ]
    }
  ]
}