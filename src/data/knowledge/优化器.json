{
  "title": "ä¼˜åŒ–å™¨ (Optimizer)",
  "subtitle": "æ›´æ–°æ¨¡å‹å‚æ•°çš„ç®—æ³•ï¼Œå¦‚SGDã€Adamã€AdamWç­‰",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "ä¼˜åŒ–å™¨æ˜¯ç”¨äºæ›´æ–°ç¥ç»ç½‘ç»œå‚æ•°çš„ç®—æ³•ï¼Œæ ¹æ®æ¢¯åº¦ä¿¡æ¯è°ƒæ•´å‚æ•°ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚ä¸åŒçš„ä¼˜åŒ–å™¨é‡‡ç”¨ä¸åŒçš„ç­–ç•¥æ¥åŠ é€Ÿæ”¶æ•›ã€é¿å…å±€éƒ¨æœ€ä¼˜ã€æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ ä¸»è¦ç±»å‹",
      "content": [
        {
          "type": "features",
          "items": [
            "SGDï¼šéšæœºæ¢¯åº¦ä¸‹é™ï¼Œæœ€åŸºç¡€çš„ä¼˜åŒ–å™¨",
            "Momentumï¼šå¼•å…¥åŠ¨é‡ï¼ŒåŠ é€Ÿæ”¶æ•›å¹¶å‡å°‘éœ‡è¡",
            "RMSpropï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œé€‚åˆRNNè®­ç»ƒ",
            "Adamï¼šç»“åˆMomentumå’ŒRMSpropï¼Œå¹¿æ³›ä½¿ç”¨",
            "AdamWï¼šAdamçš„æ”¹è¿›ç‰ˆï¼Œä¿®æ­£æƒé‡è¡°å‡",
            "AdaGradï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œé€‚åˆç¨€ç–æ•°æ®",
            "Adafactorï¼šå†…å­˜é«˜æ•ˆçš„Adamå˜ä½“ï¼Œé€‚åˆå¤§æ¨¡å‹"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "æ¢¯åº¦ä¸‹é™ã€åŠ¨é‡ã€è‡ªé€‚åº”å­¦ä¹ ç‡ã€æƒé‡è¡°å‡ã€å­¦ä¹ ç‡è°ƒåº¦"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "ç¥ç»ç½‘ç»œè®­ç»ƒã€æ·±åº¦å­¦ä¹ ã€å‚æ•°ä¼˜åŒ–ã€æ¨¡å‹å¾®è°ƒ"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "ä¼˜åŒ–å™¨å¯¹æ¯”",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "ä¼˜åŒ–å™¨å¯¹æ¯”",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "ä¼˜åŒ–å™¨æ”¶æ•›æ›²çº¿",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "ä¼˜åŒ–å™¨æ”¶æ•›æ›²çº¿",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰",
          "formulas": [
            {
              "text": "åŸºç¡€çš„å‚æ•°æ›´æ–°å…¬å¼ï¼š"
            },
            {
              "display": "\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)"
            },
            {
              "text": "å…¶ä¸­ $\\eta$ æ˜¯å­¦ä¹ ç‡ï¼Œ$\\nabla L$ æ˜¯æ¢¯åº¦",
              "inline": "\\eta"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Momentum",
          "formulas": [
            {
              "text": "å¼•å…¥åŠ¨é‡é¡¹ï¼Œç´¯ç§¯å†å²æ¢¯åº¦ï¼š"
            },
            {
              "display": "v_t = \\beta v_{t-1} + \\nabla L(\\theta_t)"
            },
            {
              "display": "\\theta_{t+1} = \\theta_t - \\eta v_t"
            },
            {
              "text": "å…¶ä¸­ $\\beta$ æ˜¯åŠ¨é‡ç³»æ•°ï¼Œé€šå¸¸å–0.9",
              "inline": "\\beta"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Adam",
          "formulas": [
            {
              "text": "ç»“åˆMomentumå’ŒRMSpropï¼Œç»´æŠ¤ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡ï¼š"
            },
            {
              "display": "m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla L(\\theta_t)"
            },
            {
              "display": "v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla L(\\theta_t))^2"
            },
            {
              "text": "åå·®ä¿®æ­£ï¼š"
            },
            {
              "display": "\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}"
            },
            {
              "text": "å‚æ•°æ›´æ–°ï¼š"
            },
            {
              "display": "\\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}"
            },
            {
              "text": "é€šå¸¸ $\\beta_1=0.9$ï¼Œ$\\beta_2=0.999$ï¼Œ$\\epsilon=10^{-8}$"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "AdamW",
          "formulas": [
            {
              "text": "AdamWä¿®æ­£äº†æƒé‡è¡°å‡çš„å®ç°ï¼š"
            },
            {
              "display": "\\theta_{t+1} = \\theta_t - \\eta \\left(\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda \\theta_t\\right)"
            },
            {
              "text": "å…¶ä¸­ $\\lambda$ æ˜¯æƒé‡è¡°å‡ç³»æ•°ï¼Œç›´æ¥ä½œç”¨äºå‚æ•°è€Œéæ¢¯åº¦"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "PyTorch ä¸­çš„ä¼˜åŒ–å™¨ä½¿ç”¨",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# å®šä¹‰æ¨¡å‹\nmodel = nn.Sequential(\n    nn.Linear(10, 50),\n    nn.ReLU(),\n    nn.Linear(50, 1)\n)\n\n# 1. SGDä¼˜åŒ–å™¨\noptimizer_sgd = optim.SGD(model.parameters(), lr=0.01)\n\n# 2. SGD + Momentum\noptimizer_momentum = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n# 3. Adamä¼˜åŒ–å™¨ï¼ˆæœ€å¸¸ç”¨ï¼‰\noptimizer_adam = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n\n# 4. AdamWä¼˜åŒ–å™¨ï¼ˆæ¨èç”¨äºTransformerï¼‰\noptimizer_adamw = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n\n# 5. RMSpropä¼˜åŒ–å™¨\noptimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99)\n\n# 6. è®­ç»ƒå¾ªç¯ç¤ºä¾‹\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# æ¨¡æ‹Ÿæ•°æ®\nx_train = torch.randn(100, 10)\ny_train = torch.randn(100, 1)\n\nfor epoch in range(100):\n    # å‰å‘ä¼ æ’­\n    output = model(x_train)\n    loss = criterion(output, y_train)\n    \n    # åå‘ä¼ æ’­\n    optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦\n    loss.backward()  # è®¡ç®—æ¢¯åº¦\n    optimizer.step()  # æ›´æ–°å‚æ•°\n    \n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n\n# 7. å­¦ä¹ ç‡è°ƒåº¦å™¨\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# å­¦ä¹ ç‡è¡°å‡\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n\nfor epoch in range(100):\n    # è®­ç»ƒä»£ç \n    output = model(x_train)\n    loss = criterion(output, y_train)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    # æ›´æ–°å­¦ä¹ ç‡\n    scheduler.step()\n    \n    if (epoch + 1) % 20 == 0:\n        current_lr = optimizer.param_groups[0]['lr']\n        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, LR: {current_lr:.6f}\")\n\n# 8. ä½™å¼¦é€€ç«å­¦ä¹ ç‡\nscheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n\n# 9. ä¸åŒå‚æ•°ç»„ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡\noptimizer_groups = optim.Adam([\n    {'params': model[0].parameters(), 'lr': 0.001},  # ç¬¬ä¸€å±‚\n    {'params': model[2].parameters(), 'lr': 0.0001}  # ç¬¬äºŒå±‚\n])\n\n# 10. æ¢¯åº¦è£å‰ª + ä¼˜åŒ–å™¨\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nfor epoch in range(100):\n    output = model(x_train)\n    loss = criterion(output, y_train)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    \n    # æ¢¯åº¦è£å‰ª\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n    \n    optimizer.step()\n\n# 11. ä¼˜åŒ–å™¨çŠ¶æ€ä¿å­˜å’ŒåŠ è½½\n# ä¿å­˜\ncheckpoint = {\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'epoch': epoch,\n    'loss': loss\n}\ntorch.save(checkpoint, 'checkpoint.pth')\n\n# åŠ è½½\ncheckpoint = torch.load('checkpoint.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\n# 12. æ··åˆç²¾åº¦è®­ç»ƒï¼ˆä½¿ç”¨Apexæˆ–torch.cuda.ampï¼‰\nfrom torch.cuda.amp import autocast, GradScaler\n\nmodel = model.cuda()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscaler = GradScaler()\n\nfor epoch in range(100):\n    x_train_gpu = x_train.cuda()\n    y_train_gpu = y_train.cuda()\n    \n    optimizer.zero_grad()\n    \n    # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦\n    with autocast():\n        output = model(x_train_gpu)\n        loss = criterion(output, y_train_gpu)\n    \n    # ç¼©æ”¾æŸå¤±å¹¶åå‘ä¼ æ’­\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\n# 13. ä¼˜åŒ–å™¨é€‰æ‹©å»ºè®®\ndef get_optimizer(model, optimizer_name='adam', lr=0.001):\n    \"\"\"æ ¹æ®ä»»åŠ¡é€‰æ‹©ä¼˜åŒ–å™¨\"\"\"\n    if optimizer_name == 'sgd':\n        return optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n    elif optimizer_name == 'adam':\n        return optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n    elif optimizer_name == 'adamw':\n        # Transformeræ¨èä½¿ç”¨AdamW\n        return optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n    elif optimizer_name == 'rmsprop':\n        return optim.RMSprop(model.parameters(), lr=lr, alpha=0.99)\n    else:\n        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n\n# ä½¿ç”¨ç¤ºä¾‹\noptimizer = get_optimizer(model, 'adamw', lr=0.001)"
        }
      ]
    }
  ]
}