{
  "title": "æ¢¯åº¦ä¸‹é™ (Gradient Descent)",
  "subtitle": "é€šè¿‡æ²¿ç€è´Ÿæ¢¯åº¦æ–¹å‘æ›´æ–°å‚æ•°æ¥æœ€å°åŒ–æŸå¤±å‡½æ•°çš„åŸºç¡€ä¼˜åŒ–ç®—æ³•",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "æ¢¯åº¦ä¸‹é™æ˜¯ä¼˜åŒ–ç¥ç»ç½‘ç»œå‚æ•°çš„åŸºç¡€ç®—æ³•ã€‚å®ƒé€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°å¯¹å‚æ•°çš„æ¢¯åº¦ï¼Œæ²¿ç€è´Ÿæ¢¯åº¦æ–¹å‘ï¼ˆæŸå¤±ä¸‹é™æœ€å¿«çš„æ–¹å‘ï¼‰æ›´æ–°å‚æ•°ï¼Œé€æ­¥é€¼è¿‘æœ€ä¼˜è§£ã€‚æ ¹æ®æ¯æ¬¡æ›´æ–°ä½¿ç”¨çš„æ ·æœ¬æ•°é‡ï¼Œåˆ†ä¸ºæ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆBGDï¼‰ã€éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰å’Œå°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆMini-batch GDï¼‰ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆBGDï¼‰ï¼šä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ ·æœ¬ï¼Œæ”¶æ•›ç¨³å®šä½†è®¡ç®—æ…¢",
            "éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ï¼šæ¯æ¬¡ä½¿ç”¨å•ä¸ªæ ·æœ¬ï¼Œè®¡ç®—å¿«ä½†æ³¢åŠ¨å¤§",
            "å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆMini-batch GDï¼‰ï¼šå¹³è¡¡ç¨³å®šæ€§å’Œæ•ˆç‡ï¼Œæœ€å¸¸ç”¨",
            "å­¦ä¹ ç‡é€‰æ‹©ï¼šå­¦ä¹ ç‡è¿‡å¤§å¯èƒ½å‘æ•£ï¼Œè¿‡å°æ”¶æ•›æ…¢",
            "æ”¶æ•›æ€§ï¼šåœ¨å‡¸å‡½æ•°ä¸Šä¿è¯æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜ï¼Œéå‡¸å‡½æ•°å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "æ¢¯åº¦è®¡ç®—ã€å­¦ä¹ ç‡è°ƒåº¦ã€æ‰¹é‡å¤§å°é€‰æ‹©ã€æ”¶æ•›åˆ¤æ–­ã€å­¦ä¹ ç‡è¡°å‡"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "ç¥ç»ç½‘ç»œè®­ç»ƒã€çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€æ‰€æœ‰åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–é—®é¢˜"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æ¢¯åº¦ä¸‹é™ç¤ºæ„å›¾",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "æ¢¯åº¦ä¸‹é™"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "æ¢¯åº¦ä¸‹é™æ›´æ–°è§„åˆ™",
          "formulas": [
            {
              "text": "å‚æ•°æ›´æ–°å…¬å¼ï¼š"
            },
            {
              "display": "\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L(\\theta_t)"
            },
            {
              "text": "å…¶ä¸­ $\\eta$ æ˜¯å­¦ä¹ ç‡ï¼Œ$\\nabla_\\theta L$ æ˜¯æŸå¤±å‡½æ•°å¯¹å‚æ•°çš„æ¢¯åº¦"
            },
            {
              "text": "æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼šä½¿ç”¨å…¨éƒ¨æ ·æœ¬è®¡ç®—æ¢¯åº¦"
            },
            {
              "display": "\\nabla_\\theta L = \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta L_i(\\theta)"
            },
            {
              "text": "éšæœºæ¢¯åº¦ä¸‹é™ï¼šæ¯æ¬¡ä½¿ç”¨å•ä¸ªæ ·æœ¬"
            },
            {
              "display": "\\nabla_\\theta L = \\nabla_\\theta L_i(\\theta), \\quad i \\sim \\text{Uniform}(1, m)"
            },
            {
              "text": "å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼šä½¿ç”¨å°æ‰¹é‡æ ·æœ¬"
            },
            {
              "display": "\\nabla_\\theta L = \\frac{1}{b} \\sum_{i \\in B} \\nabla_\\theta L_i(\\theta)"
            },
            {
              "text": "å…¶ä¸­ $b$ æ˜¯æ‰¹é‡å¤§å°ï¼Œ$B$ æ˜¯å½“å‰æ‰¹é‡"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "å­¦ä¹ ç‡çš„å½±å“",
          "formulas": [
            {
              "text": "å­¦ä¹ ç‡è¿‡å¤§ï¼šå¯èƒ½å¯¼è‡´å‚æ•°æ›´æ–°è¿‡å¤§ï¼ŒæŸå¤±å‡½æ•°å‘æ•£"
            },
            {
              "text": "å­¦ä¹ ç‡è¿‡å°ï¼šæ”¶æ•›é€Ÿåº¦æ…¢ï¼Œéœ€è¦æ›´å¤šè¿­ä»£æ¬¡æ•°"
            },
            {
              "text": "æœ€ä¼˜å­¦ä¹ ç‡ï¼šéœ€è¦æ ¹æ®å…·ä½“é—®é¢˜è°ƒæ•´ï¼Œé€šå¸¸é€šè¿‡å®éªŒç¡®å®š"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "æ¢¯åº¦ä¸‹é™å®ç°",
          "language": "python",
          "code": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# 1. æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆBGDï¼‰\ndef batch_gradient_descent(X, y, learning_rate=0.01, epochs=1000):\n    \"\"\"æ‰¹é‡æ¢¯åº¦ä¸‹é™\"\"\"\n    m = len(y)\n    theta = np.zeros(X.shape[1])\n    cost_history = []\n    \n    for epoch in range(epochs):\n        # è®¡ç®—æ¢¯åº¦ï¼ˆä½¿ç”¨å…¨éƒ¨æ ·æœ¬ï¼‰\n        predictions = X.dot(theta)\n        error = predictions - y\n        gradient = (1/m) * X.T.dot(error)\n        \n        # æ›´æ–°å‚æ•°\n        theta = theta - learning_rate * gradient\n        \n        # è®°å½•æŸå¤±\n        cost = (1/(2*m)) * np.sum(error**2)\n        cost_history.append(cost)\n        \n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}, Cost: {cost:.4f}\")\n    \n    return theta, cost_history\n\n# 2. éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰\ndef stochastic_gradient_descent(X, y, learning_rate=0.01, epochs=100):\n    \"\"\"éšæœºæ¢¯åº¦ä¸‹é™\"\"\"\n    m = len(y)\n    theta = np.zeros(X.shape[1])\n    cost_history = []\n    \n    for epoch in range(epochs):\n        epoch_cost = 0\n        \n        # éšæœºæ‰“ä¹±æ•°æ®\n        indices = np.random.permutation(m)\n        \n        for i in indices:\n            # ä½¿ç”¨å•ä¸ªæ ·æœ¬\n            x_i = X[i:i+1]\n            y_i = y[i]\n            \n            # è®¡ç®—æ¢¯åº¦\n            prediction = x_i.dot(theta)\n            error = prediction - y_i\n            gradient = x_i.T.dot(error)\n            \n            # æ›´æ–°å‚æ•°\n            theta = theta - learning_rate * gradient\n            \n            # è®°å½•æŸå¤±\n            epoch_cost += error**2 / 2\n        \n        cost_history.append(epoch_cost / m)\n        \n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}, Cost: {cost_history[-1]:.4f}\")\n    \n    return theta, cost_history\n\n# 3. å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆMini-batch GDï¼‰\ndef mini_batch_gradient_descent(X, y, learning_rate=0.01, batch_size=32, epochs=100):\n    \"\"\"å°æ‰¹é‡æ¢¯åº¦ä¸‹é™\"\"\"\n    m = len(y)\n    theta = np.zeros(X.shape[1])\n    cost_history = []\n    \n    for epoch in range(epochs):\n        epoch_cost = 0\n        \n        # éšæœºæ‰“ä¹±æ•°æ®\n        indices = np.random.permutation(m)\n        \n        # åˆ†æ‰¹å¤„ç†\n        for i in range(0, m, batch_size):\n            batch_indices = indices[i:i+batch_size]\n            X_batch = X[batch_indices]\n            y_batch = y[batch_indices]\n            \n            # è®¡ç®—æ¢¯åº¦\n            predictions = X_batch.dot(theta)\n            error = predictions - y_batch\n            gradient = (1/len(batch_indices)) * X_batch.T.dot(error)\n            \n            # æ›´æ–°å‚æ•°\n            theta = theta - learning_rate * gradient\n            \n            # è®°å½•æŸå¤±\n            epoch_cost += np.sum(error**2) / (2 * len(batch_indices))\n        \n        cost_history.append(epoch_cost / (m // batch_size))\n        \n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}, Cost: {cost_history[-1]:.4f}\")\n    \n    return theta, cost_history\n\n# 4. ä½¿ç”¨ç¤ºä¾‹\n# ç”Ÿæˆç¤ºä¾‹æ•°æ®\nnp.random.seed(42)\nm = 100\nX = np.random.randn(m, 2)\nX = np.column_stack([np.ones(m), X])  # æ·»åŠ åç½®é¡¹\ntrue_theta = np.array([2, 3, 4])\ny = X.dot(true_theta) + np.random.randn(m) * 0.1\n\n# æ‰¹é‡æ¢¯åº¦ä¸‹é™\nprint(\"=== æ‰¹é‡æ¢¯åº¦ä¸‹é™ ===\")\ntheta_bgd, cost_bgd = batch_gradient_descent(X, y, learning_rate=0.01, epochs=1000)\nprint(f\"å­¦ä¹ åˆ°çš„å‚æ•°: {theta_bgd}\")\nprint(f\"çœŸå®å‚æ•°: {true_theta}\")\n\n# éšæœºæ¢¯åº¦ä¸‹é™\nprint(\"\\n=== éšæœºæ¢¯åº¦ä¸‹é™ ===\")\ntheta_sgd, cost_sgd = stochastic_gradient_descent(X, y, learning_rate=0.01, epochs=100)\nprint(f\"å­¦ä¹ åˆ°çš„å‚æ•°: {theta_sgd}\")\n\n# å°æ‰¹é‡æ¢¯åº¦ä¸‹é™\nprint(\"\\n=== å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ ===\")\ntheta_mbgd, cost_mbgd = mini_batch_gradient_descent(X, y, learning_rate=0.01, batch_size=32, epochs=100)\nprint(f\"å­¦ä¹ åˆ°çš„å‚æ•°: {theta_mbgd}\")\n\n# 5. å¯è§†åŒ–æ”¶æ•›è¿‡ç¨‹\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.plot(cost_bgd)\nplt.title('æ‰¹é‡æ¢¯åº¦ä¸‹é™')\nplt.xlabel('è¿­ä»£æ¬¡æ•°')\nplt.ylabel('æŸå¤±')\n\nplt.subplot(1, 3, 2)\nplt.plot(cost_sgd)\nplt.title('éšæœºæ¢¯åº¦ä¸‹é™')\nplt.xlabel('è¿­ä»£æ¬¡æ•°')\nplt.ylabel('æŸå¤±')\n\nplt.subplot(1, 3, 3)\nplt.plot(cost_mbgd)\nplt.title('å°æ‰¹é‡æ¢¯åº¦ä¸‹é™')\nplt.xlabel('è¿­ä»£æ¬¡æ•°')\nplt.ylabel('æŸå¤±')\n\nplt.tight_layout()\nplt.savefig('gradient_descent_comparison.png')\nprint(\"\\næ”¶æ•›æ›²çº¿å·²ä¿å­˜\")\n\n# 6. PyTorchå®ç°\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# å®šä¹‰æ¨¡å‹\nmodel = nn.Linear(10, 1)\ncriterion = nn.MSELoss()\n\n# ä½¿ç”¨SGDä¼˜åŒ–å™¨\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# æ¨¡æ‹Ÿæ•°æ®\nX_torch = torch.randn(100, 10)\ny_torch = torch.randn(100, 1)\n\n# è®­ç»ƒå¾ªç¯\nfor epoch in range(100):\n    # å‰å‘ä¼ æ’­\n    outputs = model(X_torch)\n    loss = criterion(outputs, y_torch)\n    \n    # åå‘ä¼ æ’­\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
        }
      ]
    }
  ]
}
