{
  "title": "ä¼˜åŒ–å™¨åŸç† (Optimizer Theory)",
  "subtitle": "SGDã€Momentumã€Adamã€AdamWç­‰ä¼˜åŒ–å™¨çš„æ•°å­¦åŸç†ä¸é€‚ç”¨åœºæ™¯",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "ä¼˜åŒ–å™¨æ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒä¸­ç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°çš„ç®—æ³•ã€‚ä¸åŒçš„ä¼˜åŒ–å™¨é‡‡ç”¨ä¸åŒçš„ç­–ç•¥æ¥åŠ é€Ÿæ”¶æ•›ã€é¿å…å±€éƒ¨æœ€ä¼˜ã€æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚ä»åŸºç¡€çš„SGDåˆ°ç°ä»£çš„Adamã€AdamWï¼Œæ¯ç§ä¼˜åŒ–å™¨éƒ½æœ‰å…¶ç‹¬ç‰¹çš„æ•°å­¦åŸç†å’Œé€‚ç”¨åœºæ™¯ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "SGDï¼šéšæœºæ¢¯åº¦ä¸‹é™ï¼Œæœ€åŸºç¡€çš„ä¼˜åŒ–å™¨ï¼Œç®€å•ä½†æ”¶æ•›æ…¢",
            "Momentumï¼šå¼•å…¥åŠ¨é‡é¡¹ï¼Œç´¯ç§¯å†å²æ¢¯åº¦ï¼ŒåŠ é€Ÿæ”¶æ•›å¹¶å‡å°‘éœ‡è¡",
            "RMSpropï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œæ ¹æ®æ¢¯åº¦å¹³æ–¹çš„ç§»åŠ¨å¹³å‡è°ƒæ•´å­¦ä¹ ç‡",
            "Adamï¼šç»“åˆMomentumå’ŒRMSpropï¼Œç»´æŠ¤ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡ï¼Œå¹¿æ³›ä½¿ç”¨",
            "AdamWï¼šAdamçš„æ”¹è¿›ç‰ˆï¼Œä¿®æ­£æƒé‡è¡°å‡çš„å®ç°ï¼ŒTransformeræ¨èä½¿ç”¨",
            "AdaGradï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œé€‚åˆç¨€ç–æ¢¯åº¦åœºæ™¯",
            "Adafactorï¼šå†…å­˜é«˜æ•ˆçš„Adamå˜ä½“ï¼Œé€‚åˆå¤§æ¨¡å‹è®­ç»ƒ"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "åŠ¨é‡æœºåˆ¶ã€è‡ªé€‚åº”å­¦ä¹ ç‡ã€ä¸€é˜¶çŸ©ä¼°è®¡ã€äºŒé˜¶çŸ©ä¼°è®¡ã€åå·®ä¿®æ­£ã€æƒé‡è¡°å‡"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "ç¥ç»ç½‘ç»œè®­ç»ƒï¼ˆAdam/AdamWï¼‰ã€CNNè®­ç»ƒï¼ˆSGD+Momentumï¼‰ã€Transformerè®­ç»ƒï¼ˆAdamWï¼‰ã€RNNè®­ç»ƒï¼ˆRMSpropï¼‰ã€å¤§æ¨¡å‹è®­ç»ƒï¼ˆAdafactorï¼‰"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "ä¼˜åŒ–å™¨åŸç†å¯¹æ¯”",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "ä¼˜åŒ–å™¨åŸç†å¯¹æ¯”"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰",
          "formulas": [
            {
              "text": "åŸºç¡€çš„å‚æ•°æ›´æ–°å…¬å¼ï¼š"
            },
            {
              "display": "\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)"
            },
            {
              "text": "å…¶ä¸­ $\\eta$ æ˜¯å­¦ä¹ ç‡ï¼Œ$\\nabla L$ æ˜¯æ¢¯åº¦"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Momentumï¼ˆåŠ¨é‡ï¼‰",
          "formulas": [
            {
              "text": "å¼•å…¥åŠ¨é‡é¡¹ï¼Œç´¯ç§¯å†å²æ¢¯åº¦ï¼š"
            },
            {
              "display": "v_t = \\beta v_{t-1} + \\nabla L(\\theta_t)"
            },
            {
              "display": "\\theta_{t+1} = \\theta_t - \\eta v_t"
            },
            {
              "text": "å…¶ä¸­ $\\beta$ æ˜¯åŠ¨é‡ç³»æ•°ï¼Œé€šå¸¸å–0.9"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Adamï¼ˆè‡ªé€‚åº”çŸ©ä¼°è®¡ï¼‰",
          "formulas": [
            {
              "text": "ç»“åˆMomentumå’ŒRMSpropï¼Œç»´æŠ¤ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡ï¼š"
            },
            {
              "display": "m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla L(\\theta_t)"
            },
            {
              "display": "v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla L(\\theta_t))^2"
            },
            {
              "text": "åå·®ä¿®æ­£ï¼š"
            },
            {
              "display": "\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}"
            },
            {
              "text": "å‚æ•°æ›´æ–°ï¼š"
            },
            {
              "display": "\\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}"
            },
            {
              "text": "é€šå¸¸ $\\beta_1=0.9$ï¼Œ$\\beta_2=0.999$ï¼Œ$\\epsilon=10^{-8}$"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "AdamWï¼ˆä¿®æ­£æƒé‡è¡°å‡çš„Adamï¼‰",
          "formulas": [
            {
              "text": "AdamWä¿®æ­£äº†æƒé‡è¡°å‡çš„å®ç°ï¼š"
            },
            {
              "display": "\\theta_{t+1} = \\theta_t - \\eta \\left(\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda \\theta_t\\right)"
            },
            {
              "text": "å…¶ä¸­ $\\lambda$ æ˜¯æƒé‡è¡°å‡ç³»æ•°ï¼Œç›´æ¥ä½œç”¨äºå‚æ•°è€Œéæ¢¯åº¦"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä¼˜åŒ–å™¨å®ç°ä¸å¯¹æ¯”",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 1. ä¸åŒä¼˜åŒ–å™¨çš„ä½¿ç”¨\nmodel = nn.Sequential(\n    nn.Linear(10, 50),\n    nn.ReLU(),\n    nn.Linear(50, 1)\n)\n\n# SGD\noptimizer_sgd = optim.SGD(model.parameters(), lr=0.01)\n\n# SGD + Momentum\noptimizer_momentum = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n# Adam\noptimizer_adam = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n\n# AdamWï¼ˆæ¨èç”¨äºTransformerï¼‰\noptimizer_adamw = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n\n# RMSprop\noptimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99)\n\n# 2. æ‰‹åŠ¨å®ç°Adamä¼˜åŒ–å™¨\nclass AdamOptimizer:\n    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8):\n        self.params = list(params)\n        self.lr = lr\n        self.beta1, self.beta2 = betas\n        self.eps = eps\n        self.m = [torch.zeros_like(p) for p in self.params]\n        self.v = [torch.zeros_like(p) for p in self.params]\n        self.t = 0\n    \n    def step(self, grads):\n        self.t += 1\n        for i, (param, grad) in enumerate(zip(self.params, grads)):\n            # æ›´æ–°ä¸€é˜¶çŸ©ä¼°è®¡\n            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n            # æ›´æ–°äºŒé˜¶çŸ©ä¼°è®¡\n            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n            \n            # åå·®ä¿®æ­£\n            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n            \n            # å‚æ•°æ›´æ–°\n            param.data -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)\n    \n    def zero_grad(self):\n        pass  # æ¢¯åº¦éœ€è¦æ‰‹åŠ¨æ¸…é›¶\n\n# 3. ä¼˜åŒ–å™¨å¯¹æ¯”å®éªŒ\ncriterion = nn.MSELoss()\n\n# æ¨¡æ‹Ÿæ•°æ®\nx_train = torch.randn(100, 10)\ny_train = torch.randn(100, 1)\n\noptimizers = {\n    'SGD': optim.SGD(model.parameters(), lr=0.01),\n    'SGD+Momentum': optim.SGD(model.parameters(), lr=0.01, momentum=0.9),\n    'Adam': optim.Adam(model.parameters(), lr=0.001),\n    'AdamW': optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01),\n    'RMSprop': optim.RMSprop(model.parameters(), lr=0.01)\n}\n\nresults = {}\n\nfor name, optimizer in optimizers.items():\n    # é‡æ–°åˆå§‹åŒ–æ¨¡å‹\n    for param in model.parameters():\n        param.data.normal_(0, 0.1)\n    \n    losses = []\n    for epoch in range(100):\n        output = model(x_train)\n        loss = criterion(output, y_train)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        losses.append(loss.item())\n    \n    results[name] = losses\n    print(f\"{name}: æœ€ç»ˆæŸå¤± = {losses[-1]:.4f}\")\n\n# 4. å¯è§†åŒ–ä¼˜åŒ–å™¨æ”¶æ•›å¯¹æ¯”\nplt.figure(figsize=(12, 6))\nfor name, losses in results.items():\n    plt.plot(losses, label=name)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('ä¼˜åŒ–å™¨æ”¶æ•›å¯¹æ¯”')\nplt.legend()\nplt.grid(True)\nplt.savefig('optimizer_comparison.png')\nprint(\"\\nä¼˜åŒ–å™¨å¯¹æ¯”å›¾å·²ä¿å­˜\")\n\n# 5. å­¦ä¹ ç‡è°ƒåº¦å™¨é…åˆä¼˜åŒ–å™¨ä½¿ç”¨\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# StepLRï¼šæ¯30ä¸ªepochå­¦ä¹ ç‡è¡°å‡\nscheduler_step = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n\n# CosineAnnealingLRï¼šä½™å¼¦é€€ç«\nscheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n\n# ReduceLROnPlateauï¼šå½“æŸå¤±ä¸å†ä¸‹é™æ—¶é™ä½å­¦ä¹ ç‡\nscheduler_plateau = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n\n# ä½¿ç”¨ç¤ºä¾‹\nfor epoch in range(100):\n    output = model(x_train)\n    loss = criterion(output, y_train)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    # æ›´æ–°å­¦ä¹ ç‡ï¼ˆæ ¹æ®è°ƒåº¦å™¨ç±»å‹é€‰æ‹©ï¼‰\n    scheduler_step.step()  # æˆ– scheduler_cosine.step()\n    # scheduler_plateau.step(loss)  # éœ€è¦ä¼ å…¥losså€¼\n    \n    if (epoch + 1) % 20 == 0:\n        current_lr = optimizer.param_groups[0]['lr']\n        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, LR: {current_lr:.6f}\")\n\n# 6. ä¼˜åŒ–å™¨é€‰æ‹©å»ºè®®\nprint(\"\\n=== ä¼˜åŒ–å™¨é€‰æ‹©å»ºè®® ===\")\nprint(\"1. å°è§„æ¨¡æ¨¡å‹ã€å‡¸ä¼˜åŒ–ï¼šSGD + Momentum\")\nprint(\"2. æ·±åº¦å­¦ä¹ ã€é€šç”¨åœºæ™¯ï¼šAdam æˆ– AdamW\")\nprint(\"3. Transformeræ¨¡å‹ï¼šAdamWï¼ˆæ¨èï¼‰\")\nprint(\"4. RNN/LSTMï¼šRMSprop\")\nprint(\"5. å¤§æ¨¡å‹è®­ç»ƒï¼šAdamW æˆ– Adafactorï¼ˆå†…å­˜é«˜æ•ˆï¼‰\")"
        }
      ]
    }
  ]
}
