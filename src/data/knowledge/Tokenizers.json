{
  "title": "HuggingFace Tokenizers",
  "subtitle": "å¿«é€Ÿã€é«˜æ•ˆçš„åˆ†è¯å™¨è®­ç»ƒã€ç¼–ç /è§£ç ä¸æ‰¹å¤„ç†åº“",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "HuggingFace Tokenizers æ˜¯ä¸€ä¸ªç”¨Rustç¼–å†™çš„é«˜æ€§èƒ½åˆ†è¯åº“ï¼Œæä¾›äº†å¿«é€Ÿçš„åˆ†è¯ã€ç¼–ç å’Œè§£ç åŠŸèƒ½ã€‚æ”¯æŒå¤šç§åˆ†è¯ç®—æ³•ï¼ˆBPEã€WordPieceã€Unigramã€SentencePieceï¼‰ï¼Œå¯ä»¥è®­ç»ƒè‡ªå®šä¹‰åˆ†è¯å™¨ï¼Œæ˜¯Transformersåº“çš„æ ¸å¿ƒä¾èµ–ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "é«˜æ€§èƒ½ï¼šRustå®ç°ï¼Œåˆ†è¯é€Ÿåº¦æ¯”Pythonå®ç°å¿«10-100å€",
            "å¤šç§ç®—æ³•ï¼šæ”¯æŒBPEã€WordPieceã€Unigramã€SentencePieceç­‰åˆ†è¯ç®—æ³•",
            "è®­ç»ƒåˆ†è¯å™¨ï¼šå¯ä»¥ä»é›¶å¼€å§‹è®­ç»ƒè‡ªå®šä¹‰åˆ†è¯å™¨",
            "æ‰¹å¤„ç†ï¼šé«˜æ•ˆçš„æ‰¹é‡ç¼–ç å’Œè§£ç ",
            "ç‰¹æ®ŠTokenï¼šæ”¯æŒæ·»åŠ ç‰¹æ®Štokenï¼ˆå¦‚[CLS]ã€[SEP]ã€[PAD]ç­‰ï¼‰",
            "åå¤„ç†ï¼šæ”¯æŒæ·»åŠ åå¤„ç†æ­¥éª¤ï¼ˆå¦‚æˆªæ–­ã€å¡«å……ç­‰ï¼‰",
            "å¤šè¯­è¨€ï¼šæ”¯æŒå¤šç§è¯­è¨€çš„åˆ†è¯"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "BPEï¼ˆByte Pair Encodingï¼‰ã€WordPieceã€Unigramã€SentencePieceã€å­è¯åˆ†è¯ã€ç‰¹æ®ŠTokenå¤„ç†ã€æ‰¹å¤„ç†ä¼˜åŒ–"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "æ–‡æœ¬é¢„å¤„ç†ï¼šä¸ºNLPæ¨¡å‹å‡†å¤‡è¾“å…¥æ•°æ®\n                    è‡ªå®šä¹‰åˆ†è¯å™¨ï¼šä¸ºç‰¹å®šé¢†åŸŸæˆ–è¯­è¨€è®­ç»ƒåˆ†è¯å™¨\n                    æ‰¹é‡å¤„ç†ï¼šé«˜æ•ˆå¤„ç†å¤§é‡æ–‡æœ¬æ•°æ®\n                    æ¨¡å‹éƒ¨ç½²ï¼šåœ¨æ¨ç†æ—¶å¿«é€Ÿåˆ†è¯\n                    å¤šè¯­è¨€æ”¯æŒï¼šå¤„ç†ä¸åŒè¯­è¨€çš„æ–‡æœ¬"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "Tokenizersæ¶æ„",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Tokenizersæ¶æ„",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "Tokenizersæµç¨‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "Tokenizersæµç¨‹",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨é¢„è®­ç»ƒåˆ†è¯å™¨",
          "language": "python",
          "code": "from tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\n# åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# ç¼–ç æ–‡æœ¬\ntext = \"Hello, how are you?\"\nencoded = tokenizer(text)\nprint(encoded)\n# {'input_ids': [101, 7592, 1010, 2129, 2024, 2017, 1029, 102], ...}\n\n# è§£ç \ndecoded = tokenizer.decode(encoded['input_ids'])\nprint(decoded)\n# \"hello, how are you?\""
        },
        {
          "type": "code-box",
          "title": "è®­ç»ƒè‡ªå®šä¹‰åˆ†è¯å™¨",
          "language": "python",
          "code": "from tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\n# åˆå§‹åŒ–åˆ†è¯å™¨\ntokenizer = Tokenizer(BPE(unk_token=\"<UNK>\"))\ntokenizer.pre_tokenizer = Whitespace()\n\n# å‡†å¤‡è®­ç»ƒæ•°æ®\nfiles = [\"data.txt\"]  # æ–‡æœ¬æ–‡ä»¶åˆ—è¡¨\n\n# è®­ç»ƒåˆ†è¯å™¨\ntrainer = BpeTrainer(vocab_size=30000, special_tokens=[\"<UNK>\", \"<CLS>\", \"<SEP>\", \"<PAD>\", \"<MASK>\"])\ntokenizer.train(files, trainer)\n\n# ä¿å­˜åˆ†è¯å™¨\ntokenizer.save(\"./my_tokenizer.json\")\n\n# åŠ è½½åˆ†è¯å™¨\nloaded_tokenizer = Tokenizer.from_file(\"./my_tokenizer.json\")"
        },
        {
          "type": "code-box",
          "title": "æ‰¹å¤„ç†ç¼–ç ",
          "language": "python",
          "code": "from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# æ‰¹é‡ç¼–ç \ntexts = [\"Hello world\", \"How are you?\", \"I am fine\"]\n\n# ç¼–ç ï¼ˆè‡ªåŠ¨å¡«å……å’Œæˆªæ–­ï¼‰\nencoded = tokenizer(\n    texts,\n    padding=True,\n    truncation=True,\n    max_length=128,\n    return_tensors=\"pt\"  # è¿”å›PyTorchå¼ é‡\n)\n\nprint(encoded['input_ids'].shape)\n# torch.Size([3, 128])\n\n# ä¸å¡«å……\nencoded_no_pad = tokenizer(texts, padding=False, return_tensors=\"pt\")\nprint([len(ids) for ids in encoded_no_pad['input_ids']])\n# [4, 5, 4]  # æ¯ä¸ªåºåˆ—çš„å®é™…é•¿åº¦"
        },
        {
          "type": "code-box",
          "title": "æ·»åŠ ç‰¹æ®ŠToken",
          "language": "python",
          "code": "from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# æ·»åŠ ç‰¹æ®Štoken\ntokenizer.add_special_tokens({\"additional_special_tokens\": [\"<CUSTOM>\"]})\n\n# è°ƒæ•´æ¨¡å‹åµŒå…¥å±‚å¤§å°ï¼ˆå¦‚æœä½¿ç”¨æ¨¡å‹ï¼‰\n# model.resize_token_embeddings(len(tokenizer))\n\n# ä½¿ç”¨ç‰¹æ®Štoken\ntext = \"Hello <CUSTOM> world\"\nencoded = tokenizer(text)\nprint(encoded['input_ids'])"
        },
        {
          "type": "code-box",
          "title": "åå¤„ç†ï¼ˆæˆªæ–­å’Œå¡«å……ï¼‰",
          "language": "python",
          "code": "from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# ç¼–ç æ—¶è‡ªåŠ¨æˆªæ–­å’Œå¡«å……\ntext = \"This is a very long text that needs to be truncated.\"\n\n# æˆªæ–­åˆ°æœ€å¤§é•¿åº¦\nencoded = tokenizer(\n    text,\n    truncation=True,\n    max_length=10,\n    return_tensors=\"pt\"\n)\nprint(tokenizer.decode(encoded['input_ids'][0]))\n# \"[CLS] this is a very long text that [SEP]\"\n\n# å¡«å……åˆ°æŒ‡å®šé•¿åº¦\nencoded = tokenizer(\n    \"Short text\",\n    padding=\"max_length\",\n    max_length=20,\n    return_tensors=\"pt\"\n)\nprint(encoded['input_ids'].shape)\n# torch.Size([1, 20])"
        }
      ]
    }
  ]
}