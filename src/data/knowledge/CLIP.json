{
  "title": "CLIP (Contrastive Language-Image Pre-training)",
  "subtitle": "å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "OpenAIæå‡ºçš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å°†å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°åŒä¸€ä¸ªå…±äº«çš„ç‰¹å¾ç©ºé—´ã€‚åœ¨4äº¿å¯¹å›¾æ–‡æ•°æ®ä¸Šè®­ç»ƒï¼Œå…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "å¯¹æ¯”å­¦ä¹ ï¼šæœ€å¤§åŒ–åŒ¹é…å›¾æ–‡å¯¹çš„ç›¸ä¼¼åº¦ï¼Œæœ€å°åŒ–ä¸åŒ¹é…å¯¹çš„ç›¸ä¼¼åº¦",
            "åŒå¡”æ¶æ„ï¼šImage Encoderï¼ˆResNet/ViTï¼‰+ Text Encoderï¼ˆTransformerï¼‰",
            "é›¶æ ·æœ¬åˆ†ç±»ï¼šæ— éœ€å¾®è°ƒå³å¯è¿›è¡Œå›¾åƒåˆ†ç±»ï¼ˆåªéœ€æä¾›ç±»åˆ«åç§°ï¼‰",
            "è¯­ä¹‰å¯¹é½ï¼šæ‰“é€šè§†è§‰ä¸è¯­è¨€çš„è¯­ä¹‰ç©ºé—´",
            "å¤šæ¨¡æ€åŸºçŸ³ï¼šæ˜¯DALL-Eã€Stable Diffusionç­‰ç”Ÿæˆæ¨¡å‹çš„Text Encoder"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "å¯¹æ¯”å­¦ä¹ ï¼ˆContrastive Learningï¼‰ã€ä½™å¼¦ç›¸ä¼¼åº¦ã€æ¸©åº¦å‚æ•°ï¼ˆTemperatureï¼‰ã€å¯¹æ¯”æŸå¤±"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "ä»¥æ–‡æœå›¾ã€é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ã€å›¾åƒæè¿°ç”Ÿæˆã€å¤šæ¨¡æ€æ£€ç´¢ã€æ–‡ç”Ÿå›¾å¼•å¯¼"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "CLIPDiagram",
              "caption": "CLIPåŒ¹é…å¯è§†åŒ–",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "CLIPåŒ¹é…å¯è§†åŒ–",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "CLIPDiagram",
              "caption": "CLIPç»„ä»¶è¯¦è§£",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "CLIPç»„ä»¶è¯¦è§£",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "å¯¹æ¯”å­¦ä¹ æŸå¤±",
          "formulas": [
            {
              "text": "CLIP ä½¿ç”¨å¯¹ç§°çš„å¯¹æ¯”æŸå¤±ï¼š"
            },
            {
              "display": "L = -\\frac{1}{N}\\sum_{i=1}^{N}\\left[\\log\\frac{\\exp(\\text{sim}(I_i, T_i) / \\tau)}{\\sum_{j=1}^{N}\\exp(\\text{sim}(I_i, T_j) / \\tau)} + \\log\\frac{\\exp(\\text{sim}(T_i, I_i) / \\tau)}{\\sum_{j=1}^{N}\\exp(\\text{sim}(T_i, I_j) / \\tau)}\\right]"
            },
            {
              "text": "å…¶ä¸­ï¼š"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "ä½™å¼¦ç›¸ä¼¼åº¦",
          "formulas": [
            {
              "text": "è®¡ç®—å›¾åƒå’Œæ–‡æœ¬åµŒå…¥çš„ç›¸ä¼¼åº¦ï¼š"
            },
            {
              "display": "\\text{sim}(I, T) = \\frac{I \\cdot T}{||I|| \\cdot ||T||} = \\cos(\\theta)"
            },
            {
              "text": "å…¶ä¸­ $\\theta$ æ˜¯å‘é‡ä¹‹é—´çš„å¤¹è§’",
              "inline": "\\theta"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ CLIP è¿›è¡Œå›¾åƒ-æ–‡æœ¬åŒ¹é…",
          "language": "python",
          "code": "import torch\nimport clip\nfrom PIL import Image\n\n# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n# å‡†å¤‡å›¾åƒå’Œæ–‡æœ¬\nimage = preprocess(Image.open(\"image.jpg\")).unsqueeze(0).to(device)\ntext = clip.tokenize([\"a photo of a cat\", \"a photo of a dog\"]).to(device)\n\n# ç¼–ç \nwith torch.no_grad():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    \n    # å½’ä¸€åŒ–\n    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n    \n    # è®¡ç®—ç›¸ä¼¼åº¦\n    logits_per_image = (100.0 * image_features @ text_features.T)\n    probs = logits_per_image.softmax(dim=-1)\n\nprint(f\"å›¾åƒä¸æ–‡æœ¬çš„ç›¸ä¼¼åº¦æ¦‚ç‡: {probs}\")\n\n# é›¶æ ·æœ¬åˆ†ç±»\nclass_names = [\"cat\", \"dog\", \"bird\", \"car\", \"tree\"]\ntext_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in class_names]).to(device)\n\nwith torch.no_grad():\n    text_features = model.encode_text(text_inputs)\n    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n    \n    logits_per_image = (100.0 * image_features @ text_features.T)\n    probs = logits_per_image.softmax(dim=-1)\n\npredicted_class = class_names[probs.argmax().item()]\nprint(f\"é¢„æµ‹ç±»åˆ«: {predicted_class}\")"
        },
        {
          "type": "code-box",
          "title": "æ‰‹åŠ¨å®ç° CLIP å¯¹æ¯”æŸå¤±",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CLIPLoss(nn.Module):\n    \"\"\"CLIP å¯¹æ¯”æŸå¤±\"\"\"\n    def __init__(self, temperature=0.07):\n        super(CLIPLoss, self).__init__()\n        self.temperature = temperature\n    \n    def forward(self, image_features, text_features):\n        \"\"\"\n        å‚æ•°:\n            image_features: [batch_size, embed_dim]\n            text_features: [batch_size, embed_dim]\n        \"\"\"\n        # å½’ä¸€åŒ–\n        image_features = F.normalize(image_features, dim=-1)\n        text_features = F.normalize(text_features, dim=-1)\n        \n        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ\n        logits = torch.matmul(image_features, text_features.T) / self.temperature\n        \n        # åˆ›å»ºæ ‡ç­¾ï¼ˆå¯¹è§’çº¿ä¸º1ï¼Œè¡¨ç¤ºåŒ¹é…ï¼‰\n        labels = torch.arange(logits.size(0), device=logits.device)\n        \n        # å¯¹ç§°æŸå¤±\n        loss_i2t = F.cross_entropy(logits, labels)\n        loss_t2i = F.cross_entropy(logits.T, labels)\n        \n        loss = (loss_i2t + loss_t2i) / 2\n        \n        return loss\n\n# ä½¿ç”¨ç¤ºä¾‹\nif __name__ == \"__main__\":\n    batch_size = 32\n    embed_dim = 512\n    \n    # æ¨¡æ‹Ÿå›¾åƒå’Œæ–‡æœ¬ç‰¹å¾\n    image_features = torch.randn(batch_size, embed_dim)\n    text_features = torch.randn(batch_size, embed_dim)\n    \n    # è®¡ç®—æŸå¤±\n    criterion = CLIPLoss(temperature=0.07)\n    loss = criterion(image_features, text_features)\n    \n    print(f\"CLIP æŸå¤±: {loss.item():.4f}\")"
        }
      ]
    }
  ]
}