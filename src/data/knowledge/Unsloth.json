{
  "title": "Unslothï¼šé¢å‘å¼€æºå¤§æ¨¡å‹çš„æè‡´é«˜æ•ˆå¾®è°ƒæ¡†æ¶",
  "subtitle": "é€šè¿‡å®šåˆ¶åŒ– CUDA Kernelã€Flash Attentionã€è‡ªåŠ¨é‡åŒ–ä¸ LoRA é¢„è®¾ï¼Œå°†è®­ç»ƒé€Ÿåº¦æå‡ 2-5 å€ï¼Œæ˜¾å­˜å ç”¨ä¸‹é™ 80%ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“Š å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æ¨¡å—åŒ–æ¶æ„",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "æ¨¡å—åŒ–æ¶æ„"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "æ€§èƒ½åŠ é€Ÿæ•ˆæœ",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "æ€§èƒ½åŠ é€Ÿæ•ˆæœ"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "å·¥ä½œæµ",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "å·¥ä½œæµ"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦/ç®—æ³•è¦ç‚¹",
      "content": [
        {
          "type": "math-box",
          "title": "æ˜¾å­˜åŠ é€Ÿæ¯”ä¼°ç®—",
          "formulas": [
            {
              "text": "Unsloth é€šè¿‡ 4bit é‡åŒ– + LoRA å°†æ˜¾å­˜é™è‡³ï¼š"
            },
            {
              "display": "\\text{VRAM}_{\\text{QLoRA}} \\approx \\frac{n_{\\text{params}} \\times 4}{8} + 2 \\times n_{\\text{LoRA}} \\times bytes_{\\text{fp16}}"
            },
            {
              "text": "å…¶ä¸­ $n_{\\text{LoRA}} = 2 \\cdot d \\cdot r$ï¼Œé€šå¸¸ä»…å åŸæ¨¡å‹ 0.5%~1%ã€‚",
              "inline": "n_{\\text{LoRA}} = 2 \\cdot d \\cdot r"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "ååé‡ä¼°ç®—",
          "formulas": [
            {
              "text": "é…åˆ Flash Attentionï¼Œè®¡ç®—å¤æ‚åº¦è¿‘ä¼¼ï¼š"
            },
            {
              "display": "\\mathcal{O}(n d^2) \\rightarrow \\mathcal{O}\\bigg(\\frac{n d^2}{\\sqrt{B}}\\bigg)"
            },
            {
              "text": "$B$ ä¸ºå¹¶è¡Œ block æ•°ï¼Œä½“ç°å¤šæµæ‰§è¡Œå¸¦æ¥çš„ååæå‡ã€‚",
              "inline": "B"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "Python API ä¸€é”®å¾®è°ƒ",
          "language": "python",
          "code": "from unsloth import FastLanguageModel\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"meta-llama/Llama-3-8b\",\n    max_seq_length=4096,\n    load_in_4bit=True,\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=64,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_alpha=64,\n    lora_dropout=0.05\n)\n\ntrainer = FastLanguageModel.get_trainer(\n    model=model,\n    tokenizer=tokenizer,\n    dataset=\"unsloth/guanaco-bilingual\",\n    logging_steps=10,\n    learning_rate=2e-4,\n    num_train_epochs=3\n)\n\ntrainer.train()"
        }
      ]
    }
  ]
}