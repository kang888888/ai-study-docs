{
  "title": "è®­ç»ƒæµç¨‹",
  "subtitle": "ä»æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°åˆ°éƒ¨ç½²çš„å®Œæ•´è®­ç»ƒæµç¨‹ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "è®­ç»ƒæµç¨‹æ˜¯å¤§æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®å‡†å¤‡ã€æ¨¡å‹é…ç½®ã€è®­ç»ƒæ‰§è¡Œã€è¯„ä¼°ç­‰æ­¥éª¤ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "æµç¨‹è§„èŒƒï¼šæ ‡å‡†åŒ–çš„è®­ç»ƒæµç¨‹",
            "æ­¥éª¤æ¸…æ™°ï¼šæ¸…æ™°çš„è®­ç»ƒæ­¥éª¤",
            "å¯é‡å¤ï¼šå¯é‡å¤çš„è®­ç»ƒè¿‡ç¨‹",
            "è´¨é‡ä¿è¯ï¼šä¿è¯è®­ç»ƒè´¨é‡",
            "æœ€ä½³å®è·µï¼šéµå¾ªæœ€ä½³å®è·µ"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "æ•°æ®å‡†å¤‡ã€æ¨¡å‹é…ç½®ã€è®­ç»ƒæ‰§è¡Œã€è¯„ä¼°ã€éƒ¨ç½²"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "æ¨¡å‹è®­ç»ƒã€æµç¨‹ç®¡ç†ã€è´¨é‡ä¿è¯ã€æ ‡å‡†åŒ–"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "è®­ç»ƒæµç¨‹å›¾",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "flow",
                "title": "è®­ç»ƒæµç¨‹å›¾"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "è®­ç»ƒé˜¶æ®µåˆ’åˆ†",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "è®­ç»ƒé˜¶æ®µåˆ’åˆ†"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "å®Œæ•´è®­ç»ƒæµç¨‹",
          "language": "python",
          "code": "def train_pipeline(config):\n    # 1. æ•°æ®å‡†å¤‡\n    dataset = load_dataset(config.data_path)\n    tokenizer = load_tokenizer(config.tokenizer_path)\n    dataloader = create_dataloader(dataset, tokenizer, config.batch_size)\n    \n    # 2. æ¨¡å‹åˆå§‹åŒ–\n    model = GPT(config.model_config)\n    model = model.to(config.device)\n    \n    # 3. ä¼˜åŒ–å™¨è®¾ç½®\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=config.learning_rate,\n        weight_decay=config.weight_decay\n    )\n    \n    # 4. è®­ç»ƒå¾ªç¯\n    for epoch in range(config.num_epochs):\n        for batch in dataloader:\n            # å‰å‘ä¼ æ’­\n            logits, loss = model(batch['input_ids'], batch['labels'])\n            \n            # åå‘ä¼ æ’­\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # è®°å½•æ—¥å¿—\n            log_metrics(loss.item())\n        \n        # 5. è¯„ä¼°\n        eval_loss = evaluate(model, eval_dataloader)\n        \n        # 6. ä¿å­˜æ£€æŸ¥ç‚¹\n        save_checkpoint(model, optimizer, epoch, eval_loss)\n    \n    # 7. æ¨¡å‹éƒ¨ç½²\n    export_model(model, config.output_path)"
        },
        {
          "type": "code-box",
          "title": "æ•°æ®å‡†å¤‡æµç¨‹",
          "language": "python",
          "code": "def prepare_data(data_path, tokenizer, max_length=512):\n    \"\"\"æ•°æ®å‡†å¤‡æµç¨‹\"\"\"\n    # 1. åŠ è½½åŸå§‹æ•°æ®\n    raw_data = load_raw_data(data_path)\n    \n    # 2. æ•°æ®æ¸…æ´—\n    cleaned_data = clean_data(raw_data)\n    \n    # 3. åˆ†è¯\n    tokenized_data = []\n    for text in cleaned_data:\n        tokens = tokenizer.encode(text, max_length=max_length, truncation=True)\n        tokenized_data.append(tokens)\n    \n    # 4. åˆ›å»ºæ•°æ®é›†\n    dataset = GPTDataset(tokenized_data, max_length)\n    \n    # 5. æ•°æ®åˆ’åˆ†\n    train_dataset, val_dataset = split_dataset(dataset, train_ratio=0.9)\n    \n    return train_dataset, val_dataset"
        }
      ]
    }
  ]
}