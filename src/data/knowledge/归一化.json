{
  "title": "å½’ä¸€åŒ– (Normalization)",
  "subtitle": "ç¨³å®šè®­ç»ƒå’ŒåŠ é€Ÿæ”¶æ•›çš„å…³é”®æŠ€æœ¯",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "å½’ä¸€åŒ–æ˜¯ä¸€ç±»ç”¨äºæ ‡å‡†åŒ–ç¥ç»ç½‘ç»œä¸­é—´å±‚æ¿€æ´»å€¼åˆ†å¸ƒçš„æŠ€æœ¯ï¼Œé€šè¿‡å°†æ¿€æ´»å€¼è½¬æ¢ä¸ºå‡å€¼ä¸º0ã€æ–¹å·®ä¸º1çš„åˆ†å¸ƒï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼ŒåŠ é€Ÿæ”¶æ•›ï¼Œå¹¶å…è®¸ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ ä¸»è¦ç±»å‹",
      "content": [
        {
          "type": "features",
          "items": [
            "Batch Normalization (BN)ï¼šåœ¨batchç»´åº¦ä¸Šå½’ä¸€åŒ–ï¼Œé€‚ç”¨äºCNN",
            "Layer Normalization (LN)ï¼šåœ¨ç‰¹å¾ç»´åº¦ä¸Šå½’ä¸€åŒ–ï¼Œé€‚ç”¨äºRNN/Transformer",
            "Group Normalization (GN)ï¼šå°†é€šé“åˆ†ç»„åå½’ä¸€åŒ–ï¼Œé€‚ç”¨äºå°batch",
            "Instance Normalization (IN)ï¼šå¯¹æ¯ä¸ªæ ·æœ¬çš„æ¯ä¸ªé€šé“ç‹¬ç«‹å½’ä¸€åŒ–",
            "Weight Normalizationï¼šå½’ä¸€åŒ–æƒé‡è€Œéæ¿€æ´»å€¼",
            "RMSNormï¼šç®€åŒ–ç‰ˆLayerNormï¼Œåªå½’ä¸€åŒ–ä¸ä¸­å¿ƒåŒ–"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "BatchNormã€LayerNormã€GroupNormã€InstanceNormã€RMSNormã€æƒé‡å½’ä¸€åŒ–"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "CNNè®­ç»ƒï¼ˆBatchNormï¼‰ã€Transformerï¼ˆLayerNormï¼‰ã€å°batchè®­ç»ƒï¼ˆGroupNormï¼‰ã€é£æ ¼è¿ç§»ï¼ˆInstanceNormï¼‰ã€ç°ä»£LLMï¼ˆRMSNormï¼‰"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "å½’ä¸€åŒ–æ–¹æ³•å¯¹æ¯”",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "concept",
                "title": "å½’ä¸€åŒ–æ–¹æ³•å¯¹æ¯”"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "Batch Normalization",
          "formulas": [
            {
              "text": "BatchNormåœ¨batchç»´åº¦ä¸Šè®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼š"
            },
            {
              "display": "\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i"
            },
            {
              "display": "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2"
            },
            {
              "text": "å½’ä¸€åŒ–å¹¶åŠ å…¥å¯å­¦ä¹ çš„ç¼©æ”¾å’Œåç§»ï¼š"
            },
            {
              "display": "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}"
            },
            {
              "display": "y_i = \\gamma \\hat{x}_i + \\beta"
            },
            {
              "text": "å…¶ä¸­ $\\gamma$ å’Œ $\\beta$ æ˜¯å¯å­¦ä¹ å‚æ•°ï¼Œ$\\epsilon$ æ˜¯é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°",
              "inline": "\\gamma, \\beta, \\epsilon"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "Layer Normalization",
          "formulas": [
            {
              "text": "LayerNormåœ¨ç‰¹å¾ç»´åº¦ä¸Šå½’ä¸€åŒ–ï¼š"
            },
            {
              "display": "\\mu = \\frac{1}{d} \\sum_{i=1}^{d} x_i"
            },
            {
              "display": "\\sigma^2 = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2"
            },
            {
              "display": "y = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta"
            },
            {
              "text": "LayerNormä¸ä¾èµ–batchç»Ÿè®¡ï¼Œé€‚åˆå˜é•¿åºåˆ—å’Œæ¨ç†"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "RMSNormï¼ˆRoot Mean Square Normalizationï¼‰",
          "formulas": [
            {
              "text": "RMSNormåªå½’ä¸€åŒ–ä¸ä¸­å¿ƒåŒ–ï¼Œè®¡ç®—æ›´ç®€å•ï¼š"
            },
            {
              "display": "RMS(x) = \\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2}"
            },
            {
              "display": "y = \\frac{x}{RMS(x) + \\epsilon} \\cdot \\gamma"
            },
            {
              "text": "å»é™¤äº†å‡å€¼è®¡ç®—ï¼Œåœ¨Transformerä¸­æ•ˆæœä¸LayerNormç›¸å½“ä½†æ›´é«˜æ•ˆ"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» Python ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "PyTorch å®ç°å„ç§å½’ä¸€åŒ–",
          "language": "python",
          "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# 1. Batch Normalizationï¼ˆCNNå¸¸ç”¨ï¼‰\nclass CNNWithBN(nn.Module):\n    def __init__(self):\n        super(CNNWithBN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)  # 2D BNç”¨äºå›¾åƒ\n        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        return x\n\n# 2. Layer Normalizationï¼ˆTransformerå¸¸ç”¨ï¼‰\nclass TransformerBlockWithLN(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward):\n        super(TransformerBlockWithLN, self).__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, dim_feedforward),\n            nn.ReLU(),\n            nn.Linear(dim_feedforward, d_model)\n        )\n        self.norm1 = nn.LayerNorm(d_model)  # LayerNorm\n        self.norm2 = nn.LayerNorm(d_model)\n    \n    def forward(self, x):\n        # Pre-normæ¶æ„ï¼ˆæ›´å¸¸ç”¨ï¼‰\n        x = x + self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n        x = x + self.ffn(self.norm2(x))\n        return x\n\n# 3. Group Normalization\nclass CNNWithGN(nn.Module):\n    def __init__(self, num_groups=32):\n        super(CNNWithGN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n        self.gn1 = nn.GroupNorm(num_groups, 64)  # å°†64ä¸ªé€šé“åˆ†æˆnum_groupsç»„\n        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n        self.gn2 = nn.GroupNorm(num_groups, 128)\n    \n    def forward(self, x):\n        x = F.relu(self.gn1(self.conv1(x)))\n        x = F.relu(self.gn2(self.conv2(x)))\n        return x\n\n# 4. RMSNormå®ç°\nclass RMSNorm(nn.Module):\n    \"\"\"Root Mean Square Layer Normalization\"\"\"\n    def __init__(self, dim, eps=1e-8):\n        super(RMSNorm, self).__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))  # å¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: [batch_size, seq_len, dim] æˆ– [batch_size, dim]\n        \"\"\"\n        # è®¡ç®—RMS\n        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n        # å½’ä¸€åŒ–å¹¶ç¼©æ”¾\n        return x / rms * self.weight\n\n# 5. ä½¿ç”¨RMSNormçš„Transformerå—\nclass TransformerBlockWithRMSNorm(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward):\n        super(TransformerBlockWithRMSNorm, self).__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, dim_feedforward),\n            nn.ReLU(),\n            nn.Linear(dim_feedforward, d_model)\n        )\n        self.norm1 = RMSNorm(d_model)  # ä½¿ç”¨RMSNorm\n        self.norm2 = RMSNorm(d_model)\n    \n    def forward(self, x):\n        x = x + self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n        x = x + self.ffn(self.norm2(x))\n        return x\n\n# 6. å¯¹æ¯”ä¸åŒå½’ä¸€åŒ–æ–¹æ³•\nif __name__ == \"__main__\":\n    # BatchNormç¤ºä¾‹ï¼ˆå›¾åƒï¼‰\n    x_img = torch.randn(32, 3, 224, 224)  # [batch, channels, H, W]\n    model_bn = CNNWithBN()\n    out_bn = model_bn(x_img)\n    print(f\"BatchNormè¾“å‡ºå½¢çŠ¶: {out_bn.shape}\")\n    \n    # LayerNormç¤ºä¾‹ï¼ˆåºåˆ—ï¼‰\n    x_seq = torch.randn(32, 100, 512)  # [batch, seq_len, d_model]\n    model_ln = TransformerBlockWithLN(d_model=512, nhead=8, dim_feedforward=2048)\n    out_ln = model_ln(x_seq)\n    print(f\"LayerNormè¾“å‡ºå½¢çŠ¶: {out_ln.shape}\")\n    \n    # GroupNormç¤ºä¾‹\n    model_gn = CNNWithGN(num_groups=32)\n    out_gn = model_gn(x_img)\n    print(f\"GroupNormè¾“å‡ºå½¢çŠ¶: {out_gn.shape}\")\n    \n    # RMSNormç¤ºä¾‹\n    model_rms = TransformerBlockWithRMSNorm(d_model=512, nhead=8, dim_feedforward=2048)\n    out_rms = model_rms(x_seq)\n    print(f\"RMSNormè¾“å‡ºå½¢çŠ¶: {out_rms.shape}\")\n    \n    # æµ‹è¯•RMSNorm\n    rms_norm = RMSNorm(dim=512)\n    x_test = torch.randn(32, 100, 512)\n    out_test = rms_norm(x_test)\n    print(f\"RMSNormå•ç‹¬æµ‹è¯•è¾“å‡ºå½¢çŠ¶: {out_test.shape}\")"
        }
      ]
    }
  ]
}