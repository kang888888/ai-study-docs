{
  "title": "PPOï¼šè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–",
  "subtitle": "é€šè¿‡è£å‰ªæœºåˆ¶é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œæ˜¯RLHFè®­ç»ƒçš„æ ¸å¿ƒç®—æ³•ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆProximal Policy Optimizationï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ä¿è¯è®­ç»ƒç¨³å®šã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "è®­ç»ƒç¨³å®šï¼šé™åˆ¶æ›´æ–°å¹…åº¦ä¿è¯ç¨³å®š",
            "æ˜“äºå®ç°ï¼šç®—æ³•ç®€å•æ˜“å®ç°",
            "æ€§èƒ½ä¼˜å¼‚ï¼šåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€",
            "å¹¿æ³›é‡‡ç”¨ï¼šRLHFæ ‡å‡†ç®—æ³•",
            "å‚æ•°å¯è°ƒï¼šå¯è°ƒæ•´æ›´æ–°å¹…åº¦"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ã€ç­–ç•¥æ¢¯åº¦ã€é‡è¦æ€§é‡‡æ ·ã€è£å‰ª"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "RLHFè®­ç»ƒã€ç­–ç•¥ä¼˜åŒ–ã€æ¨¡å‹å¯¹é½ã€å¼ºåŒ–å­¦ä¹ "
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“ æ•°å­¦åŸç†",
      "content": [
        {
          "type": "math-box",
          "title": "PPO-Clipç›®æ ‡å‡½æ•°",
          "formulas": [
            {
              "display": "L^{CLIP}(\\theta) = \\mathbb{E}[\\min(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t)]"
            },
            {
              "text": "å…¶ä¸­ï¼š"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "ä¼˜åŠ¿å‡½æ•°ä¼°è®¡ï¼ˆGAEï¼‰",
          "formulas": [
            {
              "display": "A_t = \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + (\\gamma\\lambda)^2\\delta_{t+2} + \\cdots"
            },
            {
              "text": "å…¶ä¸­ $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$",
              "inline": "\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)"
            }
          ]
        },
        {
          "type": "math-box",
          "title": "æ€»æŸå¤±å‡½æ•°ï¼ˆRLHFä¸­ï¼‰",
          "formulas": [
            {
              "display": "L_{total} = L_{CLIP} - c_1 L_{VF} + c_2 L_{KL}"
            },
            {
              "text": "åŒ…å«ç­–ç•¥æŸå¤±ã€ä»·å€¼å‡½æ•°æŸå¤±å’ŒKLæ•£åº¦æƒ©ç½šé¡¹ã€‚"
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "PPOæµç¨‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "flow",
                "title": "PPOæµç¨‹"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "PPOå¯¹æ¯”",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "flow",
                "title": "PPOå¯¹æ¯”"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "PPOæ•ˆæœ",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "flow",
                "title": "PPOæ•ˆæœ"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨ TRL è¿›è¡Œ PPO è®­ç»ƒ",
          "language": "python",
          "code": "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\nfrom transformers import AutoTokenizer\n\nconfig = PPOConfig(\n    model_name=\"meta-llama/Llama-2-7b-hf\",\n    learning_rate=1e-5,\n    batch_size=64,\n    ppo_epochs=4,\n    kl_penalty=0.1\n)\n\ntokenizer = AutoTokenizer.from_pretrained(config.model_name)\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\n    config.model_name,\n    load_in_4bit=True,\n    device_map=\"auto\"\n)\n\nppo_trainer = PPOTrainer(\n    config,\n    model,\n    tokenizer,\n    dataset=rlhf_dataset\n)\n\n# è®­ç»ƒå¾ªç¯\nfor epoch in range(config.ppo_epochs):\n    for batch in dataloader:\n        # ç”Ÿæˆå“åº”\n        responses = model.generate(batch['prompt'])\n        \n        # è®¡ç®—å¥–åŠ±\n        rewards = reward_model(responses)\n        \n        # PPOæ›´æ–°\n        ppo_trainer.step(responses, rewards)"
        }
      ]
    }
  ]
}