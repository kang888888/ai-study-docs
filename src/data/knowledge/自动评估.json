{
  "title": "è‡ªåŠ¨è¯„ä¼°",
  "subtitle": "å‚è€ƒç­”æ¡ˆç±»ã€æ¨¡å‹è£åˆ¤ã€è§„åˆ™æ£€æŸ¥ç­‰è‡ªåŠ¨è¯„ä¼°æ–¹æ³•ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "è‡ªåŠ¨è¯„ä¼°æ˜¯ä½¿ç”¨è‡ªåŠ¨åŒ–æ–¹æ³•è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ŒåŒ…æ‹¬åŸºäºè§„åˆ™çš„è¯„ä¼°å’ŒåŸºäºæ¨¡å‹çš„è¯„ä¼°ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "å¿«é€Ÿé«˜æ•ˆï¼šè‡ªåŠ¨åŒ–è¯„ä¼°é€Ÿåº¦å¿«",
            "æˆæœ¬ä½ï¼šæ— éœ€äººå·¥å‚ä¸",
            "å¯é‡å¤ï¼šç»“æœå¯é‡å¤",
            "å±€é™æ€§ï¼šå¯èƒ½ä¸å¦‚äººå·¥å‡†ç¡®",
            "å¹¿æ³›åº”ç”¨ï¼šå¤§è§„æ¨¡è¯„ä¼°"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "è§„åˆ™è¯„ä¼°ã€æ¨¡å‹è¯„ä¼°ã€æŒ‡æ ‡è®¡ç®—ã€è‡ªåŠ¨åŒ–"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "å¤§è§„æ¨¡è¯„ä¼°ã€å¿«é€Ÿè¯„ä¼°ã€è‡ªåŠ¨åŒ–æµ‹è¯•ã€æŒç»­è¯„ä¼°"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "è‡ªåŠ¨è¯„ä¼°æµç¨‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "flow",
                "title": "è‡ªåŠ¨è¯„ä¼°æµç¨‹"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "è‡ªåŠ¨è¯„ä¼°æ–¹æ³•",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "comparison",
                "title": "è‡ªåŠ¨è¯„ä¼°æ–¹æ³•"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "å‚è€ƒç­”æ¡ˆç±»è¯„ä¼°",
          "language": "python",
          "code": "def evaluate_with_reference(predictions, references, metric='bleu'):\n    \"\"\"ä½¿ç”¨å‚è€ƒç­”æ¡ˆè¿›è¡Œè¯„ä¼°\"\"\"\n    from nltk.translate.bleu_score import sentence_bleu\n    from rouge_score import rouge_scorer\n    \n    scores = []\n    \n    if metric == 'bleu':\n        for pred, ref in zip(predictions, references):\n            score = sentence_bleu([ref.split()], pred.split())\n            scores.append(score)\n    elif metric == 'rouge':\n        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n        for pred, ref in zip(predictions, references):\n            score = scorer.score(ref, pred)\n            scores.append(score['rougeL'].fmeasure)\n    \n    return sum(scores) / len(scores) if scores else 0.0\n\n# ä½¿ç”¨ç¤ºä¾‹\npredictions = [\"the cat is on the mat\", \"a dog runs\"]\nreferences = [\"the cat sat on the mat\", \"a dog is running\"]\nprint(f\"BLEU: {evaluate_with_reference(predictions, references, 'bleu'):.4f}\")\nprint(f\"ROUGE: {evaluate_with_reference(predictions, references, 'rouge'):.4f}\")"
        },
        {
          "type": "code-box",
          "title": "æ¨¡å‹è£åˆ¤è¯„ä¼°",
          "language": "python",
          "code": "from transformers import pipeline\n\n# ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹ä½œä¸ºè£åˆ¤\njudge_model = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n\ndef judge_with_model(prediction, reference, question):\n    \"\"\"ä½¿ç”¨æ¨¡å‹è£åˆ¤è¯„ä¼°ç”Ÿæˆè´¨é‡\"\"\"\n    # æ„å»ºè¯„ä¼°æç¤º\n    prompt = f\"Question: {question}\\nReference: {reference}\\nPrediction: {prediction}\\nIs the prediction good?\"\n    \n    # è·å–æ¨¡å‹è¯„åˆ†\n    result = judge_model(prompt)\n    \n    # è¿”å›è¯„åˆ†ï¼ˆ0-1ä¹‹é—´ï¼‰\n    if result[0]['label'] == 'POSITIVE':\n        return result[0]['score']\n    else:\n        return 1 - result[0]['score']\n\n# ä½¿ç”¨ç¤ºä¾‹\nquestion = \"What is the capital of France?\"\nreference = \"Paris\"\nprediction = \"The capital of France is Paris\"\nscore = judge_with_model(prediction, reference, question)\nprint(f\"Judge Score: {score:.4f}\")"
        }
      ]
    }
  ]
}