{
  "title": "è‡ªåŠ¨è¯„ä¼°",
  "subtitle": "å‚è€ƒç­”æ¡ˆç±»ã€æ¨¡å‹è£åˆ¤ã€è§„åˆ™æ£€æŸ¥ç­‰è‡ªåŠ¨è¯„ä¼°æ–¹æ³•ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "è‡ªåŠ¨è¯„ä¼°æµç¨‹",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "flow",
                "title": "è‡ªåŠ¨è¯„ä¼°æµç¨‹",
                "data": null
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "è‡ªåŠ¨è¯„ä¼°æ–¹æ³•",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "comparison",
                "title": "è‡ªåŠ¨è¯„ä¼°æ–¹æ³•",
                "data": null
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "å‚è€ƒç­”æ¡ˆç±»è¯„ä¼°",
          "language": "python",
          "code": "def evaluate_with_reference(predictions, references, metric='bleu'):\n    \"\"\"ä½¿ç”¨å‚è€ƒç­”æ¡ˆè¿›è¡Œè¯„ä¼°\"\"\"\n    from nltk.translate.bleu_score import sentence_bleu\n    from rouge_score import rouge_scorer\n    \n    scores = []\n    \n    if metric == 'bleu':\n        for pred, ref in zip(predictions, references):\n            score = sentence_bleu([ref.split()], pred.split())\n            scores.append(score)\n    elif metric == 'rouge':\n        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n        for pred, ref in zip(predictions, references):\n            score = scorer.score(ref, pred)\n            scores.append(score['rougeL'].fmeasure)\n    \n    return sum(scores) / len(scores) if scores else 0.0\n\n# ä½¿ç”¨ç¤ºä¾‹\npredictions = [\"the cat is on the mat\", \"a dog runs\"]\nreferences = [\"the cat sat on the mat\", \"a dog is running\"]\nprint(f\"BLEU: {evaluate_with_reference(predictions, references, 'bleu'):.4f}\")\nprint(f\"ROUGE: {evaluate_with_reference(predictions, references, 'rouge'):.4f}\")"
        },
        {
          "type": "code-box",
          "title": "æ¨¡å‹è£åˆ¤è¯„ä¼°",
          "language": "python",
          "code": "from transformers import pipeline\n\n# ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹ä½œä¸ºè£åˆ¤\njudge_model = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n\ndef judge_with_model(prediction, reference, question):\n    \"\"\"ä½¿ç”¨æ¨¡å‹è£åˆ¤è¯„ä¼°ç”Ÿæˆè´¨é‡\"\"\"\n    # æ„å»ºè¯„ä¼°æç¤º\n    prompt = f\"Question: {question}\\nReference: {reference}\\nPrediction: {prediction}\\nIs the prediction good?\"\n    \n    # è·å–æ¨¡å‹è¯„åˆ†\n    result = judge_model(prompt)\n    \n    # è¿”å›è¯„åˆ†ï¼ˆ0-1ä¹‹é—´ï¼‰\n    if result[0]['label'] == 'POSITIVE':\n        return result[0]['score']\n    else:\n        return 1 - result[0]['score']\n\n# ä½¿ç”¨ç¤ºä¾‹\nquestion = \"What is the capital of France?\"\nreference = \"Paris\"\nprediction = \"The capital of France is Paris\"\nscore = judge_with_model(prediction, reference, question)\nprint(f\"Judge Score: {score:.4f}\")"
        }
      ]
    }
  ]
}
