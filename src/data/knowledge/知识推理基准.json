{
  "title": "çŸ¥è¯†æ¨ç†åŸºå‡†",
  "subtitle": "MMLUã€HellaSwagç­‰å¤šå­¦ç§‘çŸ¥è¯†ä¸æ¨ç†è¯„ä¼°åŸºå‡†ã€‚",
  "content": [
    {
      "type": "section",
      "title": "ğŸ“– æ ¸å¿ƒæ¦‚å¿µ",
      "content": [
        {
          "type": "desc-box",
          "content": [
            "çŸ¥è¯†æ¨ç†åŸºå‡†æ˜¯è¯„ä¼°æ¨¡å‹çŸ¥è¯†æ¨ç†èƒ½åŠ›çš„æ ‡å‡†æ•°æ®é›†ï¼Œå¦‚CommonsenseQAã€HellaSwagç­‰ã€‚"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹",
      "content": [
        {
          "type": "features",
          "items": [
            "æ¨ç†èƒ½åŠ›ï¼šè¯„ä¼°æ¨ç†èƒ½åŠ›",
            "çŸ¥è¯†åº”ç”¨ï¼šè¯„ä¼°çŸ¥è¯†åº”ç”¨",
            "ä»»åŠ¡å¤šæ ·ï¼šæ¶µç›–å¤šç§æ¨ç†ä»»åŠ¡",
            "æŒç»­å‘å±•ï¼šä¸æ–­æ”¹è¿›",
            "æƒå¨æ€§ï¼šæƒå¨çš„è¯„ä¼°åŸºå‡†"
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "âš™ï¸ å…³é”®æŠ€æœ¯",
      "content": [
        {
          "type": "tech-box",
          "content": "CommonsenseQAã€HellaSwagã€æ¨ç†ä»»åŠ¡ã€è¯„ä¼°æ•°æ®é›†"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸš€ åº”ç”¨åœºæ™¯",
      "content": [
        {
          "type": "app-box",
          "content": "æ¨ç†èƒ½åŠ›è¯„ä¼°ã€çŸ¥è¯†åº”ç”¨è¯„ä¼°ã€æ¨¡å‹æµ‹è¯•"
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ“Š æ¶æ„å›¾è§£",
      "content": [
        {
          "type": "diagram-gallery",
          "images": [
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "çŸ¥è¯†æ¨ç†åŸºå‡†åˆ†ç±»",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "comparison",
                "title": "çŸ¥è¯†æ¨ç†åŸºå‡†åˆ†ç±»"
              }
            },
            {
              "type": "svg-d3",
              "component": "GenericDiagram",
              "caption": "MMLUåŸºå‡†ç»“æ„",
              "width": 1000,
              "height": 800,
              "interactive": true,
              "props": {
                "type": "architecture",
                "title": "MMLUåŸºå‡†ç»“æ„"
              }
            }
          ]
        }
      ]
    },
    {
      "type": "section",
      "title": "ğŸ’» ä»£ç ç¤ºä¾‹",
      "content": [
        {
          "type": "code-box",
          "title": "ä½¿ç”¨MMLUåŸºå‡†è¯„ä¼°",
          "language": "python",
          "code": "from datasets import load_dataset\n\n# MMLU (Massive Multitask Language Understanding)\n# åŒ…å«57ä¸ªå­¦ç§‘é¢†åŸŸçš„å¤šé¡¹é€‰æ‹©é¢˜\nmmlu_dataset = load_dataset(\"cais/mmlu\", \"all\")\n\n# æŸ¥çœ‹æ•°æ®é›†ç»“æ„\nprint(f\"MMLUåŒ…å« {len(mmlu_dataset)} ä¸ªå­¦ç§‘\")\nprint(f\"ç¤ºä¾‹å­¦ç§‘: {list(mmlu_dataset.keys())[:5]}\")\n\n# åŠ è½½ç‰¹å®šå­¦ç§‘çš„æ•°æ®\nastronomy = load_dataset(\"cais/mmlu\", \"astronomy\")\nprint(f\"å¤©æ–‡å­¦æµ‹è¯•é›†: {len(astronomy['test'])} é¢˜\")\n\n# è¯„ä¼°æ¨¡å‹\nfrom transformers import pipeline\n\nqa_pipeline = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n\ndef evaluate_mmlu(model, dataset, subject):\n    \"\"\"è¯„ä¼°æ¨¡å‹åœ¨MMLUç‰¹å®šå­¦ç§‘ä¸Šçš„è¡¨ç°\"\"\"\n    correct = 0\n    total = 0\n    \n    for example in dataset['test']:\n        question = example['question']\n        choices = example['choices']\n        correct_answer = example['answer']\n        \n        # æ¨¡å‹é¢„æµ‹ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰\n        # å®é™…åº”è¯¥ä½¿ç”¨å®Œæ•´çš„æ¨ç†æµç¨‹\n        predicted = model.predict(question, choices)\n        \n        if predicted == correct_answer:\n            correct += 1\n        total += 1\n    \n    return correct / total if total > 0 else 0.0\n\nprint(f\"MMLUè¯„ä¼°å®Œæˆ\")"
        },
        {
          "type": "code-box",
          "title": "ä½¿ç”¨HellaSwagåŸºå‡†",
          "language": "python",
          "code": "from datasets import load_dataset\n\n# HellaSwag: å¸¸è¯†æ¨ç†åŸºå‡†\nhellaswag = load_dataset(\"Rowan/hellaswag\")\n\nprint(f\"HellaSwagè®­ç»ƒé›†: {len(hellaswag['train'])} æ ·æœ¬\")\nprint(f\"HellaSwagéªŒè¯é›†: {len(hellaswag['validation'])} æ ·æœ¬\")\n\n# æŸ¥çœ‹ç¤ºä¾‹\n example = hellaswag['validation'][0]\nprint(f\"ä¸Šä¸‹æ–‡: {example['ctx']}\")\nprint(f\"æ´»åŠ¨: {example['activity_label']}\")\nprint(f\"é€‰é¡¹: {example['endings']}\")\nprint(f\"æ­£ç¡®ç­”æ¡ˆ: {example['label']}\")\n\ndef evaluate_hellaswag(model, dataset):\n    \"\"\"è¯„ä¼°æ¨¡å‹åœ¨HellaSwagä¸Šçš„è¡¨ç°\"\"\"\n    correct = 0\n    total = 0\n    \n    for example in dataset['validation']:\n        context = example['ctx']\n        endings = example['endings']\n        correct_idx = example['label']\n        \n        # æ¨¡å‹é€‰æ‹©æœ€åˆç†çš„ç»“å°¾\n        predicted_idx = model.predict_best_ending(context, endings)\n        \n        if predicted_idx == correct_idx:\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0.0\n    return accuracy\n\nprint(f\"HellaSwagè¯„ä¼°å®Œæˆ\")"
        }
      ]
    }
  ]
}