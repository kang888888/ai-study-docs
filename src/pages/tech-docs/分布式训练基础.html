<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>分布式训练基础</title>
    <script>
        window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']] }, options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] } };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    
    <!-- 公共样式 -->
    <link rel="stylesheet" href="../common/styles.css">
</head>
<body>
    <div class="container">
        <h1>分布式训练基础</h1>
        <p class="tagline">理解大模型分布式训练的核心概念和方法。</p>

        <div class="meta-grid">
            <div class="meta-card"><h3>核心概念</h3><p>数据并行、模型并行、混合并行。</p></div>
            <div class="meta-card"><h3>关键技术</h3><p>梯度同步、内存优化、通信优化。</p></div>
            <div class="meta-card"><h3>应用场景</h3><p>大模型训练、加速训练、突破内存限制。</p></div>
            <div class="meta-card"><h3>常用框架</h3><p>DeepSpeed、Megatron-LM、FairScale。</p></div>
        </div>

        <div class="section">
            <h2>⚙️ 分布式训练方法</h2>
            <ul>
                <li><strong>数据并行</strong>：每个设备保存完整模型副本，处理不同数据批次</li>
                <li><strong>模型并行</strong>：将模型的不同部分放在不同设备上</li>
                <li><strong>混合并行</strong>：结合数据并行和模型并行</li>
            </ul>
        </div>

        <div class="section">
            <h2>🔧 关键技术</h2>
            
            <h3 style="color:#fbbf24; margin-top:20px;">1. 梯度同步</h3>
            <ul>
                <li><strong>AllReduce操作</strong>：收集所有设备的梯度，计算平均梯度，广播到所有设备</li>
                <li><strong>通信优化</strong>：梯度压缩、异步更新、通信与计算重叠</li>
            </ul>

            <h3 style="color:#fbbf24; margin-top:20px;">2. 内存优化</h3>
            <ul>
                <li><strong>梯度检查点</strong>：牺牲计算时间换取内存，只保存部分激活值</li>
                <li><strong>激活重计算</strong>：不保存中间激活值，需要时重新计算</li>
            </ul>
        </div>

        <div class="section">
            <h2>💻 代码示例</h2>
            <div class="code-box">
                <h3>数据并行训练示例</h3>
                <pre><code class="language-python">import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# 初始化分布式环境
dist.init_process_group(backend='nccl')

# 创建模型
model = MyModel()
model = model.to(device)
model = DDP(model, device_ids=[rank])

# 训练循环
for epoch in range(num_epochs):
    for batch in dataloader:
        outputs = model(batch)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()</code></pre>
            </div>
        </div>

        <div class="section">
            <h2>📊 性能考虑</h2>
            <ul>
                <li><strong>通信开销</strong>：网络带宽、模型大小、并行策略</li>
                <li><strong>负载均衡</strong>：合理分配模型部分，动态负载均衡</li>
                <li><strong>资源管理</strong>：合理分配GPU资源，实现容错机制</li>
            </ul>
        </div>
    </div>
    <!-- 公共脚本 -->
    <script src="../common/tech-document.js"></script>
</body>
</html>

