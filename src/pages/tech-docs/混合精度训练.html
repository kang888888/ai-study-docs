<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>混合精度训练</title>
    <script>
        window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']] }, options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] } };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    
    <!-- 公共样式 -->
    <link rel="stylesheet" href="../common/styles.css">
</head>
<body>
    <div class="container">
        <h1>混合精度训练（Mixed Precision Training）</h1>
        <p class="tagline">使用FP16/BF16进行前向和反向传播，使用FP32保存主权重和优化器状态。</p>

        <div class="meta-grid">
            <div class="meta-card"><h3>精度类型</h3><p>FP16、BF16、FP32混合使用。</p></div>
            <div class="meta-card"><h3>优势</h3><p>减少内存占用、加速训练（Tensor Core）。</p></div>
            <div class="meta-card"><h3>关键技术</h3><p>梯度缩放、损失缩放、主权重。</p></div>
            <div class="meta-card"><h3>注意事项</h3><p>梯度下溢、数值稳定性。</p></div>
        </div>

        <div class="section">
            <h2>⚙️ 精度类型</h2>
            
            <h3 style="color:#fbbf24; margin-top:20px;">1. FP16（半精度）</h3>
            <ul>
                <li><strong>位数</strong>：16位（1符号位 + 5指数位 + 10尾数位）</li>
                <li><strong>范围</strong>：约 $6 \times 10^{-5}$ 到 $65504$</li>
                <li><strong>优势</strong>：内存减半，在Tensor Core上加速</li>
                <li><strong>劣势</strong>：数值范围小，容易下溢</li>
            </ul>

            <h3 style="color:#fbbf24; margin-top:20px;">2. BF16（Brain Float）</h3>
            <ul>
                <li><strong>位数</strong>：16位（1符号位 + 8指数位 + 7尾数位）</li>
                <li><strong>范围</strong>：与FP32相同</li>
                <li><strong>优势</strong>：数值范围大，更稳定</li>
                <li><strong>劣势</strong>：精度略低于FP16</li>
            </ul>
        </div>

        <div class="section">
            <h2>🔧 关键技术</h2>
            
            <h3 style="color:#fbbf24; margin-top:20px;">1. 梯度缩放（Gradient Scaling）</h3>
            <ul>
                <li><strong>问题</strong>：FP16梯度可能下溢</li>
                <li><strong>解决</strong>：将损失乘以缩放因子，反向传播后再除以缩放因子</li>
                <li><strong>公式</strong>：$\text{loss} = \text{loss} \times \text{scale}$，$\text{grad} = \text{grad} / \text{scale}$</li>
            </ul>

            <h3 style="color:#fbbf24; margin-top:20px;">2. 主权重（Master Weights）</h3>
            <ul>
                <li><strong>原理</strong>：使用FP32保存模型权重和优化器状态</li>
                <li><strong>更新</strong>：FP16计算，FP32更新</li>
                <li><strong>优势</strong>：保证数值精度和训练稳定性</li>
            </ul>
        </div>

        <div class="section">
            <h2>💻 代码示例</h2>
            <div class="code-box">
                <h3>PyTorch混合精度训练</h3>
                <pre><code class="language-python">from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()
        
        # 前向传播使用FP16
        with autocast():
            outputs = model(batch)
            loss = criterion(outputs, targets)
        
        # 反向传播和梯度缩放
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()</code></pre>
            </div>
        </div>

        <div class="section">
            <h2>📊 性能提升</h2>
            <ul>
                <li><strong>内存节省</strong>：约50%（激活值和梯度）</li>
                <li><strong>训练速度</strong>：在支持Tensor Core的GPU上提升1.5-2倍</li>
                <li><strong>数值稳定性</strong>：BF16比FP16更稳定，推荐使用</li>
            </ul>
        </div>
    </div>
    <!-- 公共脚本 -->
    <script src="../common/tech-document.js"></script>
</body>
</html>

