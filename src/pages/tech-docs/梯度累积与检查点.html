<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¢¯åº¦ç´¯ç§¯ä¸æ£€æŸ¥ç‚¹</title>
    <script>
        window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']] }, options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] } };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    
    <!-- å…¬å…±æ ·å¼ -->
    <link rel="stylesheet" href="../common/styles.css">
</head>
<body>
    <div class="container">
        <h1>æ¢¯åº¦ç´¯ç§¯ä¸æ£€æŸ¥ç‚¹ï¼ˆGradient Accumulation & Checkpointingï¼‰</h1>
        <p class="tagline">æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿæ›´å¤§æ‰¹æ¬¡ï¼Œæ£€æŸ¥ç‚¹æŠ€æœ¯èŠ‚çœå†…å­˜ã€‚</p>

        <div class="meta-grid">
            <div class="meta-card"><h3>æ¢¯åº¦ç´¯ç§¯</h3><p>ç´¯ç§¯å¤šä¸ªå°æ‰¹æ¬¡çš„æ¢¯åº¦ï¼Œæ¨¡æ‹Ÿæ›´å¤§æ‰¹æ¬¡ã€‚</p></div>
            <div class="meta-card"><h3>æ£€æŸ¥ç‚¹æŠ€æœ¯</h3><p>ç‰ºç‰²è®¡ç®—æ—¶é—´æ¢å–å†…å­˜ï¼Œåªä¿å­˜éƒ¨åˆ†æ¿€æ´»å€¼ã€‚</p></div>
            <div class="meta-card"><h3>ä¼˜åŠ¿</h3><p>çªç ´å†…å­˜é™åˆ¶ã€æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚</p></div>
            <div class="meta-card"><h3>ä»£ä»·</h3><p>æ¢¯åº¦ç´¯ç§¯å¢åŠ è®­ç»ƒæ—¶é—´ï¼Œæ£€æŸ¥ç‚¹å¢åŠ è®¡ç®—æ—¶é—´ã€‚</p></div>
        </div>

        <div class="section">
            <h2>âš™ï¸ æ¢¯åº¦ç´¯ç§¯ï¼ˆGradient Accumulationï¼‰</h2>
            
            <h3 style="color:#fbbf24; margin-top:20px;">å·¥ä½œåŸç†</h3>
            <ul>
                <li><strong>æ­¥éª¤1</strong>ï¼šå¤„ç†å°æ‰¹æ¬¡ï¼Œè®¡ç®—æ¢¯åº¦ä½†ä¸æ›´æ–°å‚æ•°</li>
                <li><strong>æ­¥éª¤2</strong>ï¼šç´¯ç§¯æ¢¯åº¦ï¼ˆ$\text{grad} = \text{grad}_1 + \text{grad}_2 + ...$ï¼‰</li>
                <li><strong>æ­¥éª¤3</strong>ï¼šç´¯ç§¯Nä¸ªæ‰¹æ¬¡åï¼Œä½¿ç”¨å¹³å‡æ¢¯åº¦æ›´æ–°å‚æ•°</li>
                <li><strong>æ•ˆæœ</strong>ï¼šç­‰æ•ˆæ‰¹æ¬¡å¤§å° = å®é™…æ‰¹æ¬¡å¤§å° Ã— ç´¯ç§¯æ­¥æ•°</li>
            </ul>

            <h3 style="color:#fbbf24; margin-top:20px;">åº”ç”¨åœºæ™¯</h3>
            <ul>
                <li><strong>å†…å­˜å—é™</strong>ï¼šæ— æ³•ä½¿ç”¨å¤§æ‰¹æ¬¡ï¼Œç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿ</li>
                <li><strong>è®­ç»ƒç¨³å®šæ€§</strong>ï¼šä½¿ç”¨æ›´å¤§çš„æœ‰æ•ˆæ‰¹æ¬¡æé«˜ç¨³å®šæ€§</li>
                <li><strong>åˆ†å¸ƒå¼è®­ç»ƒ</strong>ï¼šåœ¨æµæ°´çº¿å¹¶è¡Œä¸­ç´¯ç§¯æ¢¯åº¦</li>
            </ul>
        </div>

        <div class="section">
            <h2>ğŸ”§ æ£€æŸ¥ç‚¹æŠ€æœ¯ï¼ˆGradient Checkpointingï¼‰</h2>
            
            <h3 style="color:#fbbf24; margin-top:20px;">å·¥ä½œåŸç†</h3>
            <ul>
                <li><strong>å‰å‘ä¼ æ’­</strong>ï¼šåªä¿å­˜éƒ¨åˆ†æ¿€æ´»å€¼ï¼ˆæ£€æŸ¥ç‚¹ï¼‰</li>
                <li><strong>åå‘ä¼ æ’­</strong>ï¼šä»æ£€æŸ¥ç‚¹é‡æ–°è®¡ç®—ä¸­é—´æ¿€æ´»å€¼</li>
                <li><strong>å†…å­˜èŠ‚çœ</strong>ï¼šçº¦50-80%æ¿€æ´»å€¼å†…å­˜</li>
                <li><strong>è®¡ç®—ä»£ä»·</strong>ï¼šå¢åŠ çº¦33%çš„è®¡ç®—æ—¶é—´</li>
            </ul>

            <h3 style="color:#fbbf24; margin-top:20px;">å®ç°æ–¹å¼</h3>
            <ul>
                <li><strong>PyTorch</strong>ï¼š`torch.utils.checkpoint.checkpoint()`</li>
                <li><strong>Transformers</strong>ï¼š`model.gradient_checkpointing_enable()`</li>
                <li><strong>è‡ªå®šä¹‰</strong>ï¼šåœ¨ç‰¹å®šå±‚è®¾ç½®æ£€æŸ¥ç‚¹</li>
            </ul>
        </div>

        <div class="section">
            <h2>ğŸ’» ä»£ç ç¤ºä¾‹</h2>
            <div class="code-box">
                <h3>æ¢¯åº¦ç´¯ç§¯ç¤ºä¾‹</h3>
                <pre><code class="language-python">accumulation_steps = 4
optimizer.zero_grad()

for i, batch in enumerate(dataloader):
    outputs = model(batch)
    loss = criterion(outputs, targets) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()</code></pre>
            </div>

            <div class="code-box">
                <h3>æ£€æŸ¥ç‚¹æŠ€æœ¯ç¤ºä¾‹</h3>
                <pre><code class="language-python">from torch.utils.checkpoint import checkpoint

# å¯ç”¨æ£€æŸ¥ç‚¹
model.gradient_checkpointing_enable()

# æˆ–è‡ªå®šä¹‰æ£€æŸ¥ç‚¹
def forward_with_checkpoint(self, x):
    x = checkpoint(self.layer1, x)
    x = checkpoint(self.layer2, x)
    return x</code></pre>
            </div>
        </div>

        <div class="section">
            <h2>ğŸ“Š æ€§èƒ½æƒè¡¡</h2>
            <ul>
                <li><strong>æ¢¯åº¦ç´¯ç§¯</strong>ï¼šå†…å­˜ä¸å˜ï¼Œè®­ç»ƒæ—¶é—´å¢åŠ ï¼ˆç´¯ç§¯æ­¥æ•°å€ï¼‰</li>
                <li><strong>æ£€æŸ¥ç‚¹æŠ€æœ¯</strong>ï¼šå†…å­˜å‡å°‘50-80%ï¼Œè®¡ç®—æ—¶é—´å¢åŠ çº¦33%</li>
                <li><strong>ç»„åˆä½¿ç”¨</strong>ï¼šä¸¤è€…ç»“åˆå¯ä»¥è®­ç»ƒæ›´å¤§çš„æ¨¡å‹</li>
            </ul>
        </div>
    </div>
    <!-- å…¬å…±è„šæœ¬ -->
    <script src="../common/tech-document.js"></script>
</body>
</html>

