<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>模型评估全景指南</title>
    
    <!-- 公共样式 -->
    <link rel="stylesheet" href="../common/styles.css">
</head>
<body>
<header>
    <h1>模型评估全景指南</h1>
    <p class="lead">结合指标体系、评估方法、公开基准与自动化工具，打造可复现的 LLM 评估闭环。</p>
</header>
<main>
    <section>
        <h2>评估框架一览<span class="badge">Framework</span></h2>
        <table>
            <thead>
                <tr><th>维度</th><th>核心问题</th><th>常用手段</th></tr>
            </thead>
            <tbody>
                <tr><td>指标体系</td><td>如何度量模型表现</td><td>Accuracy/F1、BLEU/ROUGE/BERTScore、EM、CodeBLEU、Faithfulness</td></tr>
                <tr><td>评估方法</td><td>如何执行评估</td><td>自动评估（规则/参考/模型裁判）、人工评估（单点评分、对比、排序）</td></tr>
                <tr><td>基准测试</td><td>如何横向对比</td><td>GLUE、SuperGLUE、MMLU、HellaSwag、HumanEval、MT-Bench</td></tr>
                <tr><td>工具链</td><td>如何提升效率</td><td>LM Evaluation Harness、LLM AutoEval、DeepEval、Ragas、Evalchemy</td></tr>
            </tbody>
        </table>
    </section>

    <section>
        <h2>指标体系</h2>
        <h3>分类指标</h3>
        <ul>
            <li><strong>Accuracy</strong> = (TP + TN) / (TP + TN + FP + FN)</li>
            <li><strong>Precision</strong> = TP / (TP + FP)</li>
            <li><strong>Recall</strong> = TP / (TP + FN)</li>
            <li><strong>F1 Score</strong> = 2 × (P × R)/(P + R)</li>
            <li>AUC-ROC、混淆矩阵用于多阈值分析</li>
        </ul>
        <h3>生成指标</h3>
        <ul>
            <li><strong>BLEU</strong>：n-gram 精确度，适合翻译、代码</li>
            <li><strong>ROUGE</strong>：偏召回率，摘要任务常用</li>
            <li><strong>METEOR</strong>：考虑同义词、词干</li>
            <li><strong>BERTScore</strong>：语义相似度，更适合开放式生成</li>
        </ul>
        <h3>任务特定指标</h3>
        <table>
            <thead><tr><th>任务</th><th>指标</th><th>说明</th></tr></thead>
            <tbody>
                <tr><td>问答</td><td>EM、Token F1</td><td>严格匹配 + 宽松匹配</td></tr>
                <tr><td>代码生成</td><td>CodeBLEU、Pass@k</td><td>兼顾语法、语义、执行正确性</td></tr>
                <tr><td>RAG</td><td>Faithfulness、Context Precision/Recall</td><td>关注事实一致性与检索质量</td></tr>
                <tr><td>数学推理</td><td>最终答案准确率、CoT 评分</td><td>评估推理链稳健性</td></tr>
            </tbody>
        </table>
    </section>

    <section>
        <h2>评估方法与流程</h2>
        <ol>
            <li>定义任务与风险关注点。</li>
            <li>挑选主/辅指标，设计阈值。</li>
            <li>准备评估数据集并版本化。</li>
            <li>运行自动评估脚本，记录日志。</li>
            <li>人工抽检高风险样本，补充主观评分。</li>
            <li>与历史模型或公开基准对比，输出报告。</li>
        </ol>
        <h3>自动评估</h3>
        <ul>
            <li>参考答案类：计算 BLEU/ROUGE/METEOR。</li>
            <li>模型裁判：调用更强模型给出偏好或分数。</li>
            <li>规则检查：长度、格式、正则校验。</li>
        </ul>
        <h3>人工评估</h3>
        <ul>
            <li>单点评估：有用性、准确性、相关性、流畅性、安全性。</li>
            <li>对比评估：A/B 测试或多模型排名。</li>
            <li>多维面板：多人多维度评分，计算一致性。</li>
        </ul>
    </section>

    <section>
        <h2>基准测试</h2>
        <table>
            <thead><tr><th>基准</th><th>场景</th><th>要点</th></tr></thead>
            <tbody>
                <tr><td>GLUE / SuperGLUE</td><td>语言理解</td><td>涵盖蕴含、语义匹配、问答</td></tr>
                <tr><td>MMLU</td><td>多学科知识</td><td>57 门课程，检验知识覆盖</td></tr>
                <tr><td>HellaSwag</td><td>常识推理</td><td>评估对日常场景的推断</td></tr>
                <tr><td>HumanEval / MBPP</td><td>代码生成</td><td>以 Pass@k 衡量执行正确性</td></tr>
                <tr><td>MT-Bench / Arena</td><td>对话偏好</td><td>人工或模型裁判打分</td></tr>
            </tbody>
        </table>
        <p>选择与业务贴合的基准，记录 Prompt、温度、采样种子，确保可复现。</p>
    </section>

    <section>
        <h2>工具与自动化</h2>
        <pre><code>from lm_eval import evaluator

tasks = ["hellaswag", "mmlu"]
results = evaluator.simple_evaluate(
    model="hf",
    model_args="pretrained=meta-llama/Llama-3-8b",
    tasks=tasks,
    batch_size=4,
)
print(results["results"]["mmlu"]["acc"])
</code></pre>
        <ul>
            <li><strong>LM Evaluation Harness</strong>：覆盖主流基准。</li>
            <li><strong>LLM AutoEval / DeepEval</strong>：对话、RAG、工具链评估。</li>
            <li><strong>Ragas</strong>：Faithfulness、Context Precision 等指标。</li>
            <li><strong>Evalchemy</strong>：快速定义评测用例，裁判模型即插即用。</li>
            <li><strong>CI/CD 集成</strong>：将评估脚本接入流水线，自动生成报告。</li>
        </ul>
    </section>

    <section>
        <h2>最佳实践</h2>
        <ul>
            <li>多指标联合决策，避免单一指标失真。</li>
            <li>将安全与合规维度纳入评估清单。</li>
            <li>输出 error analysis，沉淀高价值样本。</li>
            <li>自动化 + 人工抽检形成闭环。</li>
            <li>对评估数据与配置做版本化管理。</li>
        </ul>
        <p>推荐阅读：HELM、OpenAI Evals、PromptBench、DeepEval。</p>
    </section>
</main>
    <!-- 公共脚本 -->
    <script src="../common/tech-document.js"></script>
</body>
</html>
