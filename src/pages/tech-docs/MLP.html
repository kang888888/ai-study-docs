<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MLP (Multilayer Perceptron) å¤šå±‚æ„ŸçŸ¥æœº - æ¶æ„è¯¦è§£</title>
    <!-- MathJax for mathematical formulas -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <!-- Prism.js for code highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    
    <!-- å…¬å…±æ ·å¼ -->
    <link rel="stylesheet" href="../common/styles.css">
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>MLP (Multilayer Perceptron) å¤šå±‚æ„ŸçŸ¥æœº</h1>
            <p>æœ€åŸºç¡€çš„å‰é¦ˆç¥ç»ç½‘ç»œ</p>
        </div>
        <div class="content">
            
            <div class="section">
                <h2>ğŸ“– æ ¸å¿ƒæ¦‚å¿µ</h2>
                <div class="desc-box">
                    <p>æœ€åŸºç¡€çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œç”±è¾“å…¥å±‚ã€å¤šä¸ªéšè—å±‚å’Œè¾“å‡ºå±‚ç»„æˆã€‚å±‚ä¸å±‚ä¹‹é—´å…¨è¿æ¥ï¼ˆFully Connectedï¼‰ï¼Œæ¯ä¸ªç¥ç»å…ƒä¸ä¸‹ä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒç›¸è¿ã€‚</p>
                </div>
            </div>
            <div class="section features">
                <h2>ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹</h2>
                <ul>
                    <li>ç»“æ„ç®€å•ï¼šæ˜“äºç†è§£å’Œå®ç°ï¼Œæ˜¯æ·±åº¦å­¦ä¹ å…¥é—¨çš„ç¬¬ä¸€æ­¥</li>
                    <li>å…¨è¿æ¥ï¼šæ¯å±‚ç¥ç»å…ƒä¸ä¸‹ä¸€å±‚æ‰€æœ‰ç¥ç»å…ƒè¿æ¥</li>
                    <li>éçº¿æ€§æ¿€æ´»ï¼šé€šè¿‡æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ReLUã€Sigmoidï¼‰å¼•å…¥éçº¿æ€§</li>
                    <li>å‚æ•°é‡å¤§ï¼šå¯¹äºé«˜ç»´è¾“å…¥ï¼ˆå¦‚å›¾åƒï¼‰ï¼Œå‚æ•°é‡ä¼šçˆ†ç‚¸å¼å¢é•¿</li>
                    <li>æ— ç©ºé—´ç»“æ„ï¼šä¸è€ƒè™‘è¾“å…¥æ•°æ®çš„ç©ºé—´å…³ç³»ï¼ˆå¦‚å›¾åƒçš„åƒç´ é‚»è¿‘æ€§ï¼‰</li>
                </ul>
            </div>
            <div class="section">
                <h2>âš™ï¸ å…³é”®æŠ€æœ¯</h2>
                <div class="tech-box"><p><strong>åå‘ä¼ æ’­ç®—æ³•ï¼ˆBackpropagationï¼‰ã€æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ã€æ¿€æ´»å‡½æ•°ï¼ˆReLU/Sigmoid/Tanhï¼‰</strong></p></div>
            </div>
            <div class="section">
                <h2>ğŸš€ åº”ç”¨åœºæ™¯</h2>
                <div class="app-box"><p>åˆ†ç±»ä»»åŠ¡ã€å›å½’é¢„æµ‹ã€ç‰¹å¾å­¦ä¹ ã€ç®€å•çš„è¡¨æ ¼æ•°æ®å¤„ç†</p></div>
            </div>

            <div class="section">
                <h2>ğŸ“ æ•°å­¦åŸç†</h2>
                <div class="math-box">
                    <h3>å‰å‘ä¼ æ’­</h3>
                    <div class="math-formula">
                        <p>å¯¹äºç¬¬ $l$ å±‚ï¼Œå‰å‘ä¼ æ’­å…¬å¼ä¸ºï¼š</p>
                        <p>$$z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$$</p>
                        <p>$$a^{(l)} = \sigma(z^{(l)})$$</p>
                        <p>å…¶ä¸­ï¼š</p>
                        <ul style="list-style: none; padding-left: 20px;">
                            <li>$W^{(l)}$ æ˜¯ç¬¬ $l$ å±‚çš„æƒé‡çŸ©é˜µ</li>
                            <li>$b^{(l)}$ æ˜¯ç¬¬ $l$ å±‚çš„åç½®å‘é‡</li>
                            <li>$a^{(l-1)}$ æ˜¯ç¬¬ $l-1$ å±‚çš„æ¿€æ´»å€¼</li>
                            <li>$\sigma$ æ˜¯æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ ReLUã€Sigmoidï¼‰</li>
                        </ul>
                    </div>
                </div>
                <div class="math-box">
                    <h3>åå‘ä¼ æ’­</h3>
                    <div class="math-formula">
                        <p>è¾“å‡ºå±‚è¯¯å·®ï¼š</p>
                        <p>$$\delta^{(L)} = \nabla_a J \odot \sigma'(z^{(L)})$$</p>
                        <p>éšè—å±‚è¯¯å·®ï¼ˆä»åå‘å‰ä¼ æ’­ï¼‰ï¼š</p>
                        <p>$$\delta^{(l)} = ((W^{(l+1)})^T \delta^{(l+1)}) \odot \sigma'(z^{(l)})$$</p>
                        <p>æ¢¯åº¦è®¡ç®—ï¼š</p>
                        <p>$$\frac{\partial J}{\partial W^{(l)}} = \delta^{(l)} (a^{(l-1)})^T$$</p>
                        <p>$$\frac{\partial J}{\partial b^{(l)}} = \delta^{(l)}$$</p>
                    </div>
                </div>
                <div class="math-box">
                    <h3>æ¿€æ´»å‡½æ•°</h3>
                    <div class="math-formula">
                        <p><strong>ReLU:</strong> $f(x) = \max(0, x)$</p>
                        <p><strong>Sigmoid:</strong> $f(x) = \frac{1}{1 + e^{-x}}$</p>
                        <p><strong>Tanh:</strong> $f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>ğŸ’» Python ä»£ç ç¤ºä¾‹</h2>
                <div class="code-box">
                    <h3>ä½¿ç”¨ PyTorch å®ç° MLP</h3>
                    <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class MLP(nn.Module):
    """å¤šå±‚æ„ŸçŸ¥æœºå®ç°"""
    def __init__(self, input_size, hidden_sizes, output_size, activation='relu'):
        super(MLP, self).__init__()
        
        # æ„å»ºå±‚
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(prev_size, hidden_size))
            if activation == 'relu':
                layers.append(nn.ReLU())
            elif activation == 'sigmoid':
                layers.append(nn.Sigmoid())
            elif activation == 'tanh':
                layers.append(nn.Tanh())
            layers.append(nn.Dropout(0.2))  # é˜²æ­¢è¿‡æ‹Ÿåˆ
            prev_size = hidden_size
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_size, output_size))
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ¨¡å‹ï¼šè¾“å…¥784ç»´ï¼Œä¸¤ä¸ªéšè—å±‚[128, 64]ï¼Œè¾“å‡º10ç±»
    model = MLP(input_size=784, hidden_sizes=[128, 64], output_size=10)
    
    # å‰å‘ä¼ æ’­
    x = torch.randn(32, 784)  # batch_size=32
    output = model(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # [32, 10]
    
    # è®¡ç®—æŸå¤±
    criterion = nn.CrossEntropyLoss()
    target = torch.randint(0, 10, (32,))
    loss = criterion(output, target)
    print(f"æŸå¤±å€¼: {loss.item():.4f}")
    
    # åå‘ä¼ æ’­
    loss.backward()
    print("æ¢¯åº¦å·²è®¡ç®—å®Œæˆ")</code></pre>
                </div>
                <div class="code-box">
                    <h3>ä½¿ç”¨ NumPy æ‰‹åŠ¨å®ç°å‰å‘å’Œåå‘ä¼ æ’­</h3>
                    <pre><code class="language-python">import numpy as np

class MLP_Numpy:
    """ä½¿ç”¨NumPyæ‰‹åŠ¨å®ç°MLP"""
    def __init__(self, layer_sizes, learning_rate=0.01):
        self.layer_sizes = layer_sizes
        self.learning_rate = learning_rate
        self.weights = []
        self.biases = []
        
        # åˆå§‹åŒ–æƒé‡å’Œåç½®
        for i in range(len(layer_sizes) - 1):
            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.1
            b = np.zeros((1, layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)
    
    def relu(self, x):
        """ReLUæ¿€æ´»å‡½æ•°"""
        return np.maximum(0, x)
    
    def relu_derivative(self, x):
        """ReLUçš„å¯¼æ•°"""
        return (x > 0).astype(float)
    
    def sigmoid(self, x):
        """Sigmoidæ¿€æ´»å‡½æ•°"""
        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))
    
    def forward(self, X):
        """å‰å‘ä¼ æ’­"""
        self.activations = [X]
        self.z_values = []
        
        for i in range(len(self.weights)):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            self.z_values.append(z)
            if i < len(self.weights) - 1:  # éšè—å±‚ä½¿ç”¨ReLU
                a = self.relu(z)
            else:  # è¾“å‡ºå±‚ä½¿ç”¨Sigmoid
                a = self.sigmoid(z)
            self.activations.append(a)
        
        return self.activations[-1]
    
    def backward(self, X, y, output):
        """åå‘ä¼ æ’­"""
        m = X.shape[0]
        
        # è¾“å‡ºå±‚è¯¯å·®
        delta = output - y
        
        # ä»åå‘å‰æ›´æ–°æƒé‡å’Œåç½®
        for i in range(len(self.weights) - 1, -1, -1):
            # è®¡ç®—æ¢¯åº¦
            dW = np.dot(self.activations[i].T, delta) / m
            db = np.sum(delta, axis=0, keepdims=True) / m
            
            # æ›´æ–°æƒé‡å’Œåç½®
            self.weights[i] -= self.learning_rate * dW
            self.biases[i] -= self.learning_rate * db
            
            # è®¡ç®—å‰ä¸€å±‚è¯¯å·®ï¼ˆå¦‚æœä¸æ˜¯ç¬¬ä¸€å±‚ï¼‰
            if i > 0:
                delta = np.dot(delta, self.weights[i].T) * self.relu_derivative(self.z_values[i-1])
    
    def train(self, X, y, epochs=1000):
        """è®­ç»ƒæ¨¡å‹"""
        for epoch in range(epochs):
            output = self.forward(X)
            self.backward(X, y, output)
            
            if epoch % 100 == 0:
                loss = np.mean((output - y) ** 2)
                print(f"Epoch {epoch}, Loss: {loss:.4f}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºç®€å•çš„æ•°æ®é›†ï¼ˆXORé—®é¢˜ï¼‰
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([[0], [1], [1], [0]])
    
    # åˆ›å»ºæ¨¡å‹ï¼š2è¾“å…¥ -> 4éšè— -> 1è¾“å‡º
    model = MLP_Numpy([2, 4, 1], learning_rate=0.1)
    
    # è®­ç»ƒ
    model.train(X, y, epochs=1000)
    
    # æµ‹è¯•
    predictions = model.forward(X)
    print("\né¢„æµ‹ç»“æœ:")
    print(predictions)</code></pre>
                </div>
            </div>
        </div>
    </div>
    <!-- å…¬å…±è„šæœ¬ -->
    <script src="../common/tech-document.js"></script>
</body>
</html>

