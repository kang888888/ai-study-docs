<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BERT (Bidirectional Encoder Representations from Transformers) - æ¶æ„è¯¦è§£</title>
    <!-- MathJax for mathematical formulas -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <!-- Prism.js for code highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    
    <!-- å…¬å…±æ ·å¼ -->
    <link rel="stylesheet" href="../common/styles.css">
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>BERT (Bidirectional Encoder Representations from Transformers)</h1>
            <p>Googleçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹</p>
        </div>
        <div class="content">
            
            <div class="section">
                <h2>ğŸ“– æ ¸å¿ƒæ¦‚å¿µ</h2>
                <div class="desc-box">
                    <p>Googleåœ¨2018å¹´æå‡ºçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œåªä½¿ç”¨Transformerçš„Encoderéƒ¨åˆ†ã€‚é€šè¿‡æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰å’Œä¸‹ä¸€å¥é¢„æµ‹ï¼ˆNSPï¼‰ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ åŒå‘ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚</p>
                </div>
            </div>
            <div class="section features">
                <h2>ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹</h2>
                <ul>
                    <li>åŒå‘ç†è§£ï¼šåŒæ—¶åˆ©ç”¨å·¦ä¾§å’Œå³ä¾§çš„ä¸Šä¸‹æ–‡ä¿¡æ¯</li>
                    <li>æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰ï¼šéšæœºé®ç›–15%çš„è¯ï¼Œé¢„æµ‹è¢«é®ç›–çš„è¯</li>
                    <li>é¢„è®­ç»ƒ+å¾®è°ƒï¼šåœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šé¢„è®­ç»ƒï¼Œç„¶ååœ¨ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒ</li>
                    <li>åªæœ‰Encoderï¼šä¸åŒ…å«Decoderï¼Œä¸é€‚åˆç”Ÿæˆä»»åŠ¡</li>
                    <li>SOTAæ€§èƒ½ï¼šåœ¨å¤šä¸ªNLPç†è§£ä»»åŠ¡ä¸Šåˆ·æ–°è®°å½•</li>
                </ul>
            </div>
            <div class="section">
                <h2>âš™ï¸ å…³é”®æŠ€æœ¯</h2>
                <div class="tech-box"><p><strong>Masked Language Modelã€Next Sentence Predictionã€WordPieceåˆ†è¯ã€[CLS]å’Œ[SEP]ç‰¹æ®ŠToken</strong></p></div>
            </div>
            <div class="section">
                <h2>ğŸš€ åº”ç”¨åœºæ™¯</h2>
                <div class="app-box"><p>æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€é—®ç­”ç³»ç»Ÿï¼ˆQAï¼‰ã€è¯­ä¹‰ç›¸ä¼¼åº¦ã€æƒ…æ„Ÿåˆ†æ</p></div>
            </div>
            <div class="section">
                <h2>ğŸ“Š æ¶æ„å›¾è§£</h2>
                <div class="diagram-gallery">
                    <div class="diagram-item"><img src="BERT_MLM_Visualization.png" alt="BERT MLMå¯è§†åŒ–" onclick="openImageModal(this)" onerror="this.parentElement.style.display='none'"><div class="caption">BERT MLMå¯è§†åŒ–</div></div>
                </div>
            </div>

            <div class="section">
                <h2>ğŸ“ æ•°å­¦åŸç†</h2>
                <div class="math-box">
                    <h3>æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰æŸå¤±</h3>
                    <div class="math-formula">
                        <p>å¯¹äºè¢«æ©ç çš„ä½ç½® $m$ï¼Œé¢„æµ‹è¢«æ©ç çš„è¯ï¼š</p>
                        <p>$$L_{MLM} = -\sum_{m \in M} \log P(x_m | x_{\backslash m})$$</p>
                        <p>å…¶ä¸­ $M$ æ˜¯è¢«æ©ç çš„ä½ç½®é›†åˆï¼Œ$x_{\backslash m}$ æ˜¯é™¤ä½ç½® $m$ å¤–çš„æ‰€æœ‰è¯</p>
                    </div>
                </div>
                <div class="math-box">
                    <h3>ä¸‹ä¸€å¥é¢„æµ‹ï¼ˆNSPï¼‰æŸå¤±</h3>
                    <div class="math-formula">
                        <p>é¢„æµ‹å¥å­Bæ˜¯å¦æ˜¯å¥å­Açš„ä¸‹ä¸€å¥ï¼š</p>
                        <p>$$L_{NSP} = -\log P(\text{IsNext} | \text{CLS})$$</p>
                        <p>æ€»æŸå¤±ï¼š$L = L_{MLM} + L_{NSP}$</p>
                    </div>
                </div>
                <div class="math-box">
                    <h3>åŒå‘æ³¨æ„åŠ›</h3>
                    <div class="math-formula">
                        <p>BERTä½¿ç”¨åŒå‘è‡ªæ³¨æ„åŠ›ï¼Œæ¯ä¸ªè¯å¯ä»¥åŒæ—¶çœ‹åˆ°å·¦å³ä¸¤ä¾§çš„ä¸Šä¸‹æ–‡ï¼š</p>
                        <p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$</p>
                        <p>ä¸GPTçš„å•å‘æ³¨æ„åŠ›ä¸åŒï¼ŒBERTå¯ä»¥åŒæ—¶åˆ©ç”¨å‰åæ–‡ä¿¡æ¯</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>ğŸ’» Python ä»£ç ç¤ºä¾‹</h2>
                <div class="code-box">
                    <h3>ä½¿ç”¨ Transformers åº“åŠ è½½ BERT</h3>
                    <pre><code class="language-python">from transformers import BertModel, BertTokenizer, BertForMaskedLM
import torch

# åŠ è½½é¢„è®­ç»ƒçš„BERTæ¨¡å‹å’Œåˆ†è¯å™¨
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# è¾“å…¥æ–‡æœ¬
text = "The cat sat on the [MASK]."

# åˆ†è¯å’Œç¼–ç 
inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

# å‰å‘ä¼ æ’­
with torch.no_grad():
    outputs = model(**inputs)

# è·å–è¯åµŒå…¥
embeddings = outputs.last_hidden_state
print(f"è¯åµŒå…¥å½¢çŠ¶: {embeddings.shape}")  # [batch_size, seq_len, hidden_size]

# ä½¿ç”¨MLMæ¨¡å‹è¿›è¡Œæ©ç é¢„æµ‹
mlm_model = BertForMaskedLM.from_pretrained('bert-base-uncased')
with torch.no_grad():
    mlm_outputs = mlm_model(**inputs)
    predictions = mlm_outputs.logits

# é¢„æµ‹è¢«æ©ç çš„è¯
masked_index = inputs['input_ids'][0].tolist().index(tokenizer.mask_token_id)
predicted_token_id = predictions[0, masked_index].argmax().item()
predicted_token = tokenizer.decode([predicted_token_id])
print(f"é¢„æµ‹çš„è¯: {predicted_token}")</code></pre>
                </div>
                <div class="code-box">
                    <h3>æ‰‹åŠ¨å®ç° BERT çš„æ©ç è¯­è¨€æ¨¡å‹</h3>
                    <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class BertEmbedding(nn.Module):
    """BERTè¯åµŒå…¥å±‚"""
    def __init__(self, vocab_size, hidden_size, max_seq_length, dropout=0.1):
        super(BertEmbedding, self).__init__()
        self.token_embedding = nn.Embedding(vocab_size, hidden_size)
        self.position_embedding = nn.Embedding(max_seq_length, hidden_size)
        self.segment_embedding = nn.Embedding(2, hidden_size)  # å¥å­Aå’ŒB
        self.layer_norm = nn.LayerNorm(hidden_size)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, input_ids, segment_ids=None):
        seq_length = input_ids.size(1)
        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)
        
        if segment_ids is None:
            segment_ids = torch.zeros_like(input_ids)
        
        token_emb = self.token_embedding(input_ids)
        position_emb = self.position_embedding(position_ids)
        segment_emb = self.segment_embedding(segment_ids)
        
        embeddings = token_emb + position_emb + segment_emb
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embeddings)
        
        return embeddings

class BertMLMHead(nn.Module):
    """BERT MLMé¢„æµ‹å¤´"""
    def __init__(self, hidden_size, vocab_size):
        super(BertMLMHead, self).__init__()
        self.dense = nn.Linear(hidden_size, hidden_size)
        self.layer_norm = nn.LayerNorm(hidden_size)
        self.decoder = nn.Linear(hidden_size, vocab_size)
    
    def forward(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = F.gelu(hidden_states)
        hidden_states = self.layer_norm(hidden_states)
        logits = self.decoder(hidden_states)
        return logits

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    vocab_size = 30522  # BERT-baseè¯æ±‡è¡¨å¤§å°
    hidden_size = 768
    max_seq_length = 512
    
    embedding = BertEmbedding(vocab_size, hidden_size, max_seq_length)
    mlm_head = BertMLMHead(hidden_size, vocab_size)
    
    # æ¨¡æ‹Ÿè¾“å…¥
    input_ids = torch.randint(0, vocab_size, (2, 128))  # batch_size=2, seq_len=128
    segment_ids = torch.zeros(2, 128, dtype=torch.long)
    
    # å‰å‘ä¼ æ’­
    embeddings = embedding(input_ids, segment_ids)
    print(f"åµŒå…¥å½¢çŠ¶: {embeddings.shape}")  # [2, 128, 768]
    
    # MLMé¢„æµ‹
    logits = mlm_head(embeddings)
    print(f"MLM logitså½¢çŠ¶: {logits.shape}")  # [2, 128, 30522]</code></pre>
                </div>
            </div>
        </div>
    </div>
    <!-- å›¾ç‰‡æ”¾å¤§å¼¹çª— -->
    <div id="imageModal" class="image-modal" onclick="closeImageModal()">
        <span class="image-modal-close" onclick="event.stopPropagation(); closeImageModal()">&times;</span>
        <img id="modalImage" class="image-modal-content" src="" alt="">
        <div id="modalCaption" class="image-modal-caption"></div>
    </div>
    <script>
        function openImageModal(img) {
            const modal = document.getElementById('imageModal');
            const modalImg = document.getElementById('modalImage');
            const modalCaption = document.getElementById('modalCaption');
            modal.classList.add('active');
            modalImg.src = img.src;
            modalCaption.textContent = img.alt || '';
            modalImg.onclick = function(e) { e.stopPropagation(); };
        }
        function closeImageModal() {
            document.getElementById('imageModal').classList.remove('active');
        }
        document.addEventListener('keydown', function(e) {
            if (e.key === 'Escape') closeImageModal();
        });
    </script>
    <!-- å…¬å…±è„šæœ¬ -->
    <script src="../common/tech-document.js"></script>
</body>
</html>

