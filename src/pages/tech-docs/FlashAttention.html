<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FlashAttention æ¨ç†ä¼˜åŒ–</title>
    <script>
        window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']] }, options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] } };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    
    <!-- å…¬å…±æ ·å¼ -->
    <link rel="stylesheet" href="../common/styles.css">
</head>
<body>
    <div class="container">
        <h1>FlashAttentionï¼šIO æ„ŸçŸ¥çš„æ³¨æ„åŠ›è®¡ç®—</h1>
        <p class="tagline">é€šè¿‡å—çŠ¶ tilingã€å¯„å­˜å™¨å¤ç”¨å’Œèåˆ softmaxï¼Œå°†æ³¨æ„åŠ›å¤æ‚åº¦é™ä½ä¸º IO æœ€ä¼˜ï¼Œå®ç°æ›´å¿«çš„é•¿åºåˆ—æ¨ç†ã€‚</p>

        <div class="meta-grid">
            <div class="meta-card"><h3>å…³é”®åˆ›æ–°</h3><p>Block-sparse tilingã€Online Softmaxã€é¿å…ä¸­é—´å¼ é‡å†™å…¥ã€‚</p></div>
            <div class="meta-card"><h3>ç‰ˆæœ¬</h3><p>FA1ï¼ˆåŸºç¡€ï¼‰ã€FA2ï¼ˆæ›´çµæ´» shapeï¼‰ã€FA3ï¼ˆé›†åˆ Flash Decodingï¼‰ã€‚</p></div>
            <div class="meta-card"><h3>æ”¶ç›Š</h3><p>ååæå‡ 2-3xï¼Œæ˜¾å­˜å³°å€¼é™ä½ 10xï¼Œé€‚åˆ 8K/16K åºåˆ—ã€‚</p></div>
            <div class="meta-card"><h3>é›†æˆ</h3><p>PyTorch 2.xã€xFormersã€TensorRT-LLMã€vLLMã€Unslothã€‚</p></div>
        </div>

        <div class="section">
            <h2>âš¡ æŠ€æœ¯è¦ç‚¹</h2>
            <ul>
                <li><strong>Tilingï¼š</strong>å°† Q/K/V åˆ‡åˆ†ä¸ºå°å—ï¼ŒæŒ‰å—åŠ è½½åˆ° SRAMï¼Œå‡å°‘ HBM è®¿é—®ã€‚</li>
                <li><strong>åœ¨çº¿ Softmaxï¼š</strong>é‡‡ç”¨ `running max + running sum` é¿å…æ•°å€¼æº¢å‡ºã€‚</li>
                <li><strong>Flash Decodingï¼š</strong>FA3 æ”¯æŒ KV Cache è§£ç é˜¶æ®µçš„ O(1) è®¡ç®—ã€‚</li>
                <li><strong>å¯ç»„åˆï¼š</strong>å¯ä¸ Ropeã€Grouped-Query Attentionã€Alibi ç­‰å…±å­˜ã€‚</li>
            </ul>
        </div>

        <div class="section">
            <h2>ğŸ“Š å›¾è§£</h2>
            <div class="diagram-gallery">
                <div class="diagram-item"><img src="./assets/FlashAttentionæµç¨‹.png" alt="æµç¨‹" onclick="openImageModal(this)" onerror="this.parentElement.style.display='none'"><div class="caption">æµç¨‹</div></div>
                <div class="diagram-item"><img src="./assets/FlashAttentiontiling.png" alt="Tiling" onclick="openImageModal(this)" onerror="this.parentElement.style.display='none'"><div class="caption">Tiling</div></div>
                <div class="diagram-item"><img src="./assets/FlashDecoding.png" alt="Flash Decoding" onclick="openImageModal(this)" onerror="this.parentElement.style.display='none'"><div class="caption">Flash Decoding</div></div>
            </div>
        </div>

        <div class="section">
            <h2>ğŸ“ æ•°å­¦åŸç†</h2>
            <div class="math-box">
                <h3>åœ¨çº¿ Softmax</h3>
                <div class="math-formula">
                    <p>$$m_i = \max(m_{i-1}, x_i), \quad l_i = l_{i-1}\, e^{m_{i-1}-m_i} + e^{x_i - m_i}$$</p>
                    <p>$$\text{softmax}(x)_i = \frac{e^{x_i - m_n}}{l_n}$$</p>
                    <p>æ— éœ€å­˜å‚¨å…¨éƒ¨ logitsã€‚</p>
                </div>
            </div>
            <div class="math-box">
                <h3>IO æœ€ä¼˜</h3>
                <div class="math-formula">
                    <p>FlashAttention å°† IO å¤æ‚åº¦é™è‡³ï¼š</p>
                    <p>$$O\Big(\frac{n^2}{B} + n d\Big)$$</p>
                    <p>$B$ ä¸ºå—å¤§å°ï¼Œç†è®ºä¸Šå·²è¾¾ IO ä¸‹ç•Œã€‚</p>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>ğŸ’» ä»£ç ç¤ºä¾‹</h2>
            <div class="code-box">
                <h3>PyTorch 2.x å¯ç”¨ FlashAttention</h3>
<pre><code class="language-python">import torch
from torch.nn.functional import scaled_dot_product_attention

def flash_attention(q, k, v, is_causal=True):
    return scaled_dot_product_attention(
        q, k, v,
        attn_mask=None,
        dropout_p=0.0,
        is_causal=is_causal
    )

# åœ¨æ¨ç†æ¨¡å‹ä¸­æ›¿æ¢åŸå§‹ Attention
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_mem_efficient=True, enable_math=True):
    y = flash_attention(q, k, v)
</code></pre>
            </div>
        </div>
    </div>

    <div id="imageModal" class="image-modal" onclick="closeImageModal()">
        <span class="image-modal-close" onclick="event.stopPropagation(); closeImageModal();">&times;</span>
        <img id="modalImage" class="image-modal-content" src="" alt="">
        <div id="modalCaption" class="image-modal-caption"></div>
    </div>

    <script>
        function openImageModal(img){const modal=document.getElementById('imageModal');modal.classList.add('active');document.getElementById('modalImage').src=img.src;document.getElementById('modalCaption').textContent=img.alt||'';}
        function closeImageModal(){document.getElementById('imageModal').classList.remove('active');}
        document.addEventListener('keydown',e=>{if(e.key==='Escape')closeImageModal();});
    </script>
    <!-- å…¬å…±è„šæœ¬ -->
    <script src="../common/tech-document.js"></script>
</body>
</html>
