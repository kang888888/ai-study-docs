<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Miras 深度学习架构设计框架 - 架构详解</title>
    <!-- MathJax for mathematical formulas -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <!-- Prism.js for code highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    
    <!-- 公共样式 -->
    <link rel="stylesheet" href="../common/styles.css">
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Miras 深度学习架构设计框架</h1>
            <p>通用框架，重新概念化神经架构为关联记忆模块</p>
        </div>
        <div class="content">
            
            <div class="section">
                <h2>📖 核心概念</h2>
                <div class="desc-box">
                    <p><strong>Miras</strong> 是 Google Research 提出的深度学习架构设计<strong>通用框架</strong>，旨在超越现有的 Transformer 模型。该框架受人类认知现象中的<strong>注意力偏差（Attention Bias）</strong>启发，将神经架构（包括 Transformers、Titans 和现代线性递归神经网络）重新概念化为<strong>关联记忆模块（Associative Memory Modules）</strong>。</p>
                </div>
            </div>
            <div class="section features">
                <h2>🌟 核心思想</h2>
                <ul>
                    <li><strong>关联记忆系统</strong>：通过键值映射存储和检索信息</li>
                    <li><strong>注意力偏差驱动</strong>：通过内部目标（注意力偏差）学习键值映射</li>
                    <li><strong>通用设计框架</strong>：可以统一理解多种架构（Transformer、Titans、线性RNN等）</li>
                    <li><strong>四个关键选择</strong>：关联记忆架构、注意力偏差目标、保留门控机制、记忆学习算法</li>
                </ul>
            </div>
            <div class="section">
                <h2>⚙️ 四个关键选择</h2>
                <div class="tech-box">
                    <p><strong>1. 关联记忆架构（Associative Memory Architecture）</strong></p>
                    <p>定义模型如何存储和检索信息，决定记忆的组织方式（扁平/层次/动态）</p>
                </div>
                <div class="tech-box">
                    <p><strong>2. 注意力偏差目标（Attention Bias Objective）</strong></p>
                    <p>确定模型在处理输入时应优先关注哪些信息，指导注意力机制的学习</p>
                </div>
                <div class="tech-box">
                    <p><strong>3. 保留门控机制（Retention Gate）</strong></p>
                    <p>控制模型如何遗忘或保留信息，管理信息的生命周期（硬门控/软门控/自适应）</p>
                </div>
                <div class="tech-box">
                    <p><strong>4. 记忆学习算法（Memory Learning Algorithm）</strong></p>
                    <p>指导模型如何有效地学习和更新记忆，优化记忆的存储和检索（在线/批量/增量）</p>
                </div>
            </div>
            <div class="section">
                <h2>🚀 应用场景</h2>
                <div class="app-box">
                    <p><strong>架构设计</strong>：指导新架构的设计，理解现有架构的原理</p>
                    <p><strong>模型优化</strong>：优化注意力机制，改进记忆管理</p>
                    <p><strong>任务适配</strong>：根据任务设计合适的架构，选择最优的注意力偏差</p>
                    <p><strong>理论研究</strong>：统一理解神经架构，探索记忆和注意力的本质</p>
                </div>
            </div>

            <div class="section">
                <h2>📐 数学原理</h2>
                <div class="math-box">
                    <h3>关联记忆系统</h3>
                    <div class="math-formula">
                        <p>关联记忆系统可以表示为：</p>
                        <p>$$M = \{ (k_i, v_i) \}_{i=1}^{N}$$</p>
                        <p>其中：</p>
                        <ul style="list-style: none; padding-left: 0;">
                            <li>• $k_i$：键（Key），用于检索的索引</li>
                            <li>• $v_i$：值（Value），存储的信息</li>
                            <li>• $N$：记忆容量</li>
                        </ul>
                    </div>
                </div>
                <div class="math-box">
                    <h3>检索过程</h3>
                    <div class="math-formula">
                        <p>检索输出：</p>
                        <p>$$o = \sum_{i=1}^{N} \alpha_i \cdot v_i$$</p>
                        <p>其中注意力权重：</p>
                        <p>$$\alpha_i = \text{softmax}(\text{score}(q, k_i) + \text{bias}_i)$$</p>
                        <p>其中 $\text{bias}_i$ 是<strong>注意力偏差</strong>，可以：</p>
                        <ul style="list-style: none; padding-left: 0;">
                            <li>• <strong>引导检索</strong>：偏向某些记忆</li>
                            <li>• <strong>过滤噪声</strong>：降低不相关记忆的权重</li>
                            <li>• <strong>优化性能</strong>：提高检索效率</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>🔗 与现有架构的关系</h2>
                <table>
                    <thead>
                        <tr>
                            <th>架构</th>
                            <th>关联记忆视角</th>
                            <th>注意力偏差</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Transformer</strong></td>
                            <td>扁平记忆，全局检索</td>
                            <td>位置偏差 + 内容偏差</td>
                        </tr>
                        <tr>
                            <td><strong>Titans</strong></td>
                            <td>层次记忆，长期+短期</td>
                            <td>时间偏差 + 任务偏差</td>
                        </tr>
                        <tr>
                            <td><strong>线性RNN</strong></td>
                            <td>递归记忆，局部检索</td>
                            <td>时间偏差</td>
                        </tr>
                        <tr>
                            <td><strong>Moneta/Yaad/Memora</strong></td>
                            <td>优化的记忆架构</td>
                            <td>学习的偏差</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="section">
                <h2>💡 基于 Miras 的模型</h2>
                <div class="tech-box">
                    <p><strong>Moneta</strong>：高效的关联记忆架构，优势是快速检索和更新，应用于实时推理任务</p>
                </div>
                <div class="tech-box">
                    <p><strong>Yaad</strong>：优化的注意力偏差，优势是更好的信息选择，应用于需要精确检索的任务</p>
                </div>
                <div class="tech-box">
                    <p><strong>Memora</strong>：长期记忆管理，优势是高效存储和检索历史信息，应用于长序列建模任务</p>
                </div>
            </div>

            <div class="section">
                <h2>📊 性能表现</h2>
                <div class="desc-box">
                    <p><strong>语言建模</strong>：超越 Transformers 和现代线性递归模型</p>
                    <p><strong>常识推理</strong>：利用关联记忆进行推理，更好的信息检索能力</p>
                    <p><strong>高召回率任务</strong>：需要精确检索的任务，利用注意力偏差优化检索</p>
                </div>
            </div>

            <div class="section">
                <h2>💻 Python 代码示例</h2>
                <div class="code-box">
                    <h3>关联记忆模块的简化实现</h3>
                    <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class AssociativeMemory(nn.Module):
    """关联记忆模块"""
    def __init__(self, d_model, memory_size):
        super(AssociativeMemory, self).__init__()
        self.d_model = d_model
        self.memory_size = memory_size
        
        # 记忆存储：键值对
        self.register_buffer('keys', torch.randn(memory_size, d_model))
        self.register_buffer('values', torch.randn(memory_size, d_model))
        
        # 注意力偏差（可学习）
        self.bias = nn.Parameter(torch.zeros(memory_size))
    
    def forward(self, query):
        """
        参数:
            query: [batch_size, d_model] 查询向量
        返回:
            output: [batch_size, d_model] 检索结果
        """
        batch_size = query.shape[0]
        
        # 计算查询与键的相似度
        scores = torch.matmul(query, self.keys.t())  # [batch_size, memory_size]
        
        # 添加注意力偏差
        scores = scores + self.bias.unsqueeze(0)
        
        # 计算注意力权重
        attention_weights = F.softmax(scores, dim=-1)  # [batch_size, memory_size]
        
        # 加权求和值
        output = torch.matmul(attention_weights, self.values)  # [batch_size, d_model]
        
        return output

# 使用示例
if __name__ == "__main__":
    memory = AssociativeMemory(d_model=512, memory_size=1000)
    query = torch.randn(2, 512)
    output = memory(query)
    print(f"输出形状: {output.shape}")  # [2, 512]</code></pre>
                </div>
            </div>
        </div>
    </div>
    <!-- 图片放大弹窗 -->
    <div id="imageModal" class="image-modal" onclick="closeImageModal()">
        <span class="image-modal-close" onclick="event.stopPropagation(); closeImageModal()">&times;</span>
        <img id="modalImage" class="image-modal-content" src="" alt="">
        <div id="modalCaption" class="image-modal-caption"></div>
    </div>
    <script>
        function openImageModal(img) {
            const modal = document.getElementById('imageModal');
            const modalImg = document.getElementById('modalImage');
            const modalCaption = document.getElementById('modalCaption');
            modal.classList.add('active');
            modalImg.src = img.src;
            modalCaption.textContent = img.alt || '';
            modalImg.onclick = function(e) { e.stopPropagation(); };
        }
        function closeImageModal() {
            document.getElementById('imageModal').classList.remove('active');
        }
        document.addEventListener('keydown', function(e) {
            if (e.key === 'Escape') closeImageModal();
        });
    </script>
    <!-- 公共脚本 -->
    <script src="../common/tech-document.js"></script>
</body>
</html>

