<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>QWen (é€šä¹‰åƒé—®) é˜¿é‡Œäº‘å¤§æ¨¡å‹ - æ¶æ„è¯¦è§£</title>
    <!-- MathJax for mathematical formulas -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <!-- Prism.js for code highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    
    <!-- å…¬å…±æ ·å¼ -->
    <link rel="stylesheet" href="../common/styles.css">
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>QWen (é€šä¹‰åƒé—®) é˜¿é‡Œäº‘å¤§æ¨¡å‹</h1>
            <p>é˜¿é‡Œäº‘å¼€æºçš„å¤§è¯­è¨€æ¨¡å‹ç³»åˆ—</p>
        </div>
        <div class="content">
            
            <div class="section">
                <h2>ğŸ“– æ ¸å¿ƒæ¦‚å¿µ</h2>
                <div class="desc-box">
                    <p>é˜¿é‡Œäº‘å¼€æºçš„å¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼Œä»0.5Båˆ°72Bå¤šä¸ªè§„æ¨¡ã€‚åœ¨ä¸­æ–‡ã€ä»£ç ã€æ•°å­¦ç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ”¯æŒ32Ké•¿ä¸Šä¸‹æ–‡ï¼Œå¹¶æä¾›å¤šæ¨¡æ€ç‰ˆæœ¬ã€‚</p>
                </div>
            </div>
            <div class="section features">
                <h2>ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹</h2>
                <ul>
                    <li>å¤šè§„æ¨¡ï¼šä»0.5Båˆ°72Bï¼Œè¦†ç›–ä¸åŒåœºæ™¯éœ€æ±‚</li>
                    <li>é•¿ä¸Šä¸‹æ–‡ï¼šæ”¯æŒ32K tokensï¼Œé€‚åˆé•¿æ–‡æ¡£ç†è§£</li>
                    <li>GQAä¼˜åŒ–ï¼šä½¿ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼Œæå‡æ¨ç†æ•ˆç‡</li>
                    <li>å¤šæ¨¡æ€ï¼šQWen-VLæ”¯æŒå›¾åƒï¼ŒQWen-Audioæ”¯æŒéŸ³é¢‘</li>
                    <li>ä»£ç èƒ½åŠ›å¼ºï¼šåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°çªå‡º</li>
                </ul>
            </div>
            <div class="section">
                <h2>âš™ï¸ å…³é”®æŠ€æœ¯</h2>
                <div class="tech-box"><p><strong>Grouped-Query Attentionã€RoPEã€SwiGLUã€Flash Attention</strong></p></div>
            </div>
            <div class="section">
                <h2>ğŸš€ åº”ç”¨åœºæ™¯</h2>
                <div class="app-box"><p>ä¸­æ–‡å¯¹è¯ã€ä»£ç ç”Ÿæˆã€é•¿æ–‡æ¡£ç†è§£ã€å¤šæ¨¡æ€ç†è§£ã€æ•°å­¦æ¨ç†</p></div>
            </div>

            <div class="section">
                <h2>ğŸ“ æ•°å­¦åŸç†</h2>
                <div class="math-box">
                    <h3>Grouped-Query Attention (GQA)</h3>
                    <div class="math-formula">
                        <p>GQA å°†å¤šä¸ªæŸ¥è¯¢å¤´åˆ†ç»„å…±äº«é”®å€¼ï¼š</p>
                        <p>$$\text{GQA}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$</p>
                        <p>$$\text{head}_i = \text{Attention}(Q_i, K_{group}, V_{group})$$</p>
                        <p>ç›¸æ¯”MHAï¼ŒGQAå‡å°‘äº†KV Cacheï¼Œæå‡æ¨ç†æ•ˆç‡</p>
                    </div>
                </div>
                <div class="math-box">
                    <h3>RoPE ä½ç½®ç¼–ç </h3>
                    <div class="math-formula">
                        <p>QWenä½¿ç”¨æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼Œä¸LLaMAç›¸åŒï¼š</p>
                        <p>$$R_{\Theta, m}^d = \text{Rotary}(m, \theta)$$</p>
                        <p>æ”¯æŒé•¿ä¸Šä¸‹æ–‡æ‰©å±•ï¼Œå¯ä»¥å¤„ç†32K tokens</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>ğŸ’» Python ä»£ç ç¤ºä¾‹</h2>
                <div class="code-box">
                    <h3>ä½¿ç”¨ Transformers åº“åŠ è½½ QWen</h3>
                    <pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_path = "Qwen/Qwen-7B-Chat"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    trust_remote_code=True,
    torch_dtype=torch.float16,
    device_map="auto"
)

# å¯¹è¯
messages = [
    {"role": "user", "content": "ä½ å¥½"}
]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer([text], return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=100)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(response)</code></pre>
                </div>
            </div>
        </div>
    </div>
    <!-- å…¬å…±è„šæœ¬ -->
    <script src="../common/tech-document.js"></script>
</body>
</html>

